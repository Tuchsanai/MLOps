{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31d2d384",
   "metadata": {},
   "source": [
    "\n",
    "# Evidently Lab: Binary Classification (v0.7+ API)\n",
    "\n",
    "**Goal:** Build a minimal-yet-practical workflow to evaluate a binary classifier using Evidently **0.7+** with:\n",
    "- clean data schema via `DataDefinition` + `BinaryClassification`\n",
    "- model quality metrics (Accuracy, Precision, Recall, F1, ROC AUC)\n",
    "- simple data quality checks\n",
    "- segment-level breakdowns\n",
    "- reference vs. current comparison & drift checks\n",
    "\n",
    "> **Import rule (per lab request):** only import **what we use** from the list provided.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8498992",
   "metadata": {},
   "source": [
    "## 1) Setup & version check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64865258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evidently.__version__: 0.7.14\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# (Optional) quick version check — should be 0.7+\n",
    "import evidently\n",
    "print(\"evidently.__version__:\", evidently.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5eb78a",
   "metadata": {},
   "source": [
    "## 2) Imports (selected from the given list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7cff7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "from evidently import Dataset, DataDefinition, BinaryClassification, Report\n",
    "\n",
    "# Metrics & utilities used in this lab (picked from the provided list)\n",
    "from evidently.metrics import (\n",
    "    ColumnCount,\n",
    "    RowCount,\n",
    "    DuplicatedRowCount,\n",
    "    DatasetMissingValueCount,\n",
    "    CategoryCount,\n",
    "    Accuracy,\n",
    "    Precision,\n",
    "    Recall,\n",
    "    F1Score,\n",
    "    RocAuc,\n",
    "    ScoreDistribution,\n",
    "    DriftedColumnsCount,\n",
    "    ValueDrift,\n",
    ")\n",
    "from evidently.metrics.group_by import GroupBy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f19622",
   "metadata": {},
   "source": [
    "## 3) Create synthetic **reference** and **current** datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f97c488a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>income</th>\n",
       "      <th>segment</th>\n",
       "      <th>target</th>\n",
       "      <th>pred_proba</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>38</td>\n",
       "      <td>37000</td>\n",
       "      <td>B</td>\n",
       "      <td>1</td>\n",
       "      <td>0.346330</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>41</td>\n",
       "      <td>92000</td>\n",
       "      <td>A</td>\n",
       "      <td>1</td>\n",
       "      <td>0.404463</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>44</td>\n",
       "      <td>26000</td>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "      <td>0.464406</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25</td>\n",
       "      <td>46000</td>\n",
       "      <td>C</td>\n",
       "      <td>0</td>\n",
       "      <td>0.392051</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>43</td>\n",
       "      <td>24000</td>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "      <td>0.300144</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>795</th>\n",
       "      <td>57</td>\n",
       "      <td>83000</td>\n",
       "      <td>B</td>\n",
       "      <td>0</td>\n",
       "      <td>0.519305</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>796</th>\n",
       "      <td>18</td>\n",
       "      <td>27000</td>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "      <td>0.434655</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>797</th>\n",
       "      <td>57</td>\n",
       "      <td>96000</td>\n",
       "      <td>C</td>\n",
       "      <td>0</td>\n",
       "      <td>0.512207</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798</th>\n",
       "      <td>60</td>\n",
       "      <td>87000</td>\n",
       "      <td>C</td>\n",
       "      <td>0</td>\n",
       "      <td>0.267657</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799</th>\n",
       "      <td>65</td>\n",
       "      <td>44000</td>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "      <td>0.474304</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>800 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     age  income segment  target  pred_proba  prediction\n",
       "0     38   37000       B       1    0.346330           0\n",
       "1     41   92000       A       1    0.404463           0\n",
       "2     44   26000       A       0    0.464406           0\n",
       "3     25   46000       C       0    0.392051           0\n",
       "4     43   24000       A       0    0.300144           0\n",
       "..   ...     ...     ...     ...         ...         ...\n",
       "795   57   83000       B       0    0.519305           1\n",
       "796   18   27000       A       0    0.434655           0\n",
       "797   57   96000       C       0    0.512207           1\n",
       "798   60   87000       C       0    0.267657           0\n",
       "799   65   44000       A       0    0.474304           0\n",
       "\n",
       "[800 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# We'll create a small synthetic binary classification dataset using only pandas + stdlib.\n",
    "# Columns:\n",
    "# - 'age' (numeric), 'income' (numeric), 'segment' (categorical)\n",
    "# - 'target' (0/1), 'pred_proba' (probability of class 1), 'prediction' (0/1 via 0.5 threshold)\n",
    "\n",
    "import random\n",
    "random.seed(7)\n",
    "\n",
    "def make_split(n_rows=600, shift=False):\n",
    "    rows = []\n",
    "    for i in range(n_rows):\n",
    "        age = random.randint(18, 70)\n",
    "        base_income = random.randint(18, 120) * 1000\n",
    "        segment = random.choice([\"A\", \"B\", \"C\"])\n",
    "        \n",
    "        # True target: a simple rule with noise\n",
    "        p_true = 0.25\n",
    "        if age < 30: p_true += 0.05\n",
    "        if base_income > 80000: p_true += 0.15\n",
    "        if segment == \"A\": p_true += 0.10\n",
    "        # add small noise\n",
    "        p_true = max(0.01, min(0.99, p_true + random.uniform(-0.05, 0.05)))\n",
    "        target = 1 if random.random() < p_true else 0\n",
    "        \n",
    "        # Model probability (imperfect) — optionally shift to simulate drift\n",
    "        p_pred = p_true * 0.7 + random.uniform(0, 0.3)\n",
    "        if shift:\n",
    "            # Simulate a slight covariate/behavior shift in \"current\"\n",
    "            p_pred = min(0.99, max(0.01, p_pred + 0.05))\n",
    "            if segment == \"C\":\n",
    "                p_pred = min(0.99, p_pred + 0.05)\n",
    "        \n",
    "        pred_label = 1 if p_pred >= 0.5 else 0\n",
    "        \n",
    "        rows.append({\n",
    "            \"age\": age,\n",
    "            \"income\": base_income,\n",
    "            \"segment\": segment,\n",
    "            \"target\": target,\n",
    "            \"pred_proba\": p_pred,\n",
    "            \"prediction\": pred_label,\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "reference_df = make_split(n_rows=800, shift=False)\n",
    "current_df   = make_split(n_rows=800, shift=True)\n",
    "\n",
    "reference_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7460da",
   "metadata": {},
   "source": [
    "## 4) Define the schema with `DataDefinition` + `BinaryClassification`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "885cf69a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK: DataDefinition + Dataset created\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Map column roles so Evidently understands targets, predictions, probabilities, and features.\n",
    "data_def = DataDefinition(\n",
    "    classification=[\n",
    "        BinaryClassification(\n",
    "            # Required roles:\n",
    "            target=\"target\",               # ground-truth labels (0/1)\n",
    "            prediction_labels=\"prediction\",# predicted class labels (0/1)\n",
    "            prediction=\"pred_proba\",       # probability of the positive class (1)\n",
    "            # Optional: you can specify pos_label if your positive class is not 1.\n",
    "            # pos_label=1,\n",
    "        )\n",
    "    ],\n",
    "    # (Optional) you can also mark feature types; here we let Evidently infer.\n",
    ")\n",
    "dataset_ref = Dataset.from_pandas(reference_df)\n",
    "dataset_cur = Dataset.from_pandas(current_df)\n",
    "\n",
    "print(\"OK: DataDefinition + Dataset created\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875ba5ce",
   "metadata": {},
   "source": [
    "## 5) Quick **Data Quality** report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "670ae995",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Report.run() got an unexpected keyword argument 'data_definition'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      1\u001b[39m dq_report = Report(metrics=[\n\u001b[32m      2\u001b[39m     ColumnCount(),\n\u001b[32m      3\u001b[39m     RowCount(),\n\u001b[32m      4\u001b[39m \n\u001b[32m      5\u001b[39m ])\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[43mdq_report\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_definition\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_def\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset_cur\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreference_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset_ref\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m dq_report\n",
      "\u001b[31mTypeError\u001b[39m: Report.run() got an unexpected keyword argument 'data_definition'"
     ]
    }
   ],
   "source": [
    "\n",
    "dq_report = Report(metrics=[\n",
    "    ColumnCount(),\n",
    "    RowCount(),\n",
    "\n",
    "])\n",
    "\n",
    "dq_report.run(\n",
    "    data_definition=data_def,\n",
    "    data=dataset_cur,\n",
    "    reference_data=dataset_ref,\n",
    ")\n",
    "dq_report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6dd7e9b",
   "metadata": {},
   "source": [
    "## 6) **Model Quality** metrics (current-only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63b705aa",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for ScoreDistribution\nk\n  field required (type=value_error.missing)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValidationError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      1\u001b[39m quality_report = Report(metrics=[\n\u001b[32m      2\u001b[39m     Accuracy(),\n\u001b[32m      3\u001b[39m     Precision(),\n\u001b[32m      4\u001b[39m     Recall(),\n\u001b[32m      5\u001b[39m     F1Score(),\n\u001b[32m      6\u001b[39m     RocAuc(),\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     \u001b[43mScoreDistribution\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumn\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpred_proba\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m,  \u001b[38;5;66;03m# visualize score distribution\u001b[39;00m\n\u001b[32m      8\u001b[39m ])\n\u001b[32m     10\u001b[39m quality_report.run(\n\u001b[32m     11\u001b[39m     data_definition=data_def,\n\u001b[32m     12\u001b[39m     data=dataset_cur,\n\u001b[32m     13\u001b[39m )\n\u001b[32m     14\u001b[39m quality_report\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/evidently/pydantic_utils.py:89\u001b[39m, in \u001b[36mFrozenBaseModel.__init__\u001b[39m\u001b[34m(self, **data)\u001b[39m\n\u001b[32m     88\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, **data: Any):\n\u001b[32m---> \u001b[39m\u001b[32m89\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__init_values__\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     90\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m private_attr \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.__private_attributes__:\n\u001b[32m     91\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m private_attr \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.__init_values__:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/pydantic/v1/main.py:347\u001b[39m, in \u001b[36mBaseModel.__init__\u001b[39m\u001b[34m(__pydantic_self__, **data)\u001b[39m\n\u001b[32m    345\u001b[39m values, fields_set, validation_error = validate_model(__pydantic_self__.\u001b[34m__class__\u001b[39m, data)\n\u001b[32m    346\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n\u001b[32m    348\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    349\u001b[39m     object_setattr(__pydantic_self__, \u001b[33m'\u001b[39m\u001b[33m__dict__\u001b[39m\u001b[33m'\u001b[39m, values)\n",
      "\u001b[31mValidationError\u001b[39m: 1 validation error for ScoreDistribution\nk\n  field required (type=value_error.missing)"
     ]
    }
   ],
   "source": [
    "\n",
    "quality_report = Report(metrics=[\n",
    "    Accuracy(),\n",
    "    Precision(),\n",
    "    Recall(),\n",
    "    F1Score(),\n",
    "    RocAuc(),\n",
    "    ScoreDistribution(column=\"pred_proba\"),  # visualize score distribution\n",
    "])\n",
    "\n",
    "quality_report.run(\n",
    "    data_definition=data_def,\n",
    "    data=dataset_cur,\n",
    ")\n",
    "quality_report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539a596c",
   "metadata": {},
   "source": [
    "## 7) Segment-level breakdown with `GroupBy` (by `segment`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b88bf58",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "segmented_report = Report(metrics=[\n",
    "    GroupBy(column=\"segment\", metrics=[Accuracy(), Precision(), Recall(), F1Score()])\n",
    "])\n",
    "\n",
    "segmented_report.run(\n",
    "    data_definition=data_def,\n",
    "    data=dataset_cur,\n",
    ")\n",
    "segmented_report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d513ca",
   "metadata": {},
   "source": [
    "## 8) Reference vs. Current: column drift & performance shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c792f60c",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Report.run() got an unexpected keyword argument 'data_definition'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      1\u001b[39m compare_report = Report(metrics=[\n\u001b[32m      2\u001b[39m     DriftedColumnsCount(),                 \u001b[38;5;66;03m# count of columns detected as drifted\u001b[39;00m\n\u001b[32m      3\u001b[39m     ValueDrift(column=\u001b[33m\"\u001b[39m\u001b[33mage\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m   (...)\u001b[39m\u001b[32m      7\u001b[39m     F1Score(),\n\u001b[32m      8\u001b[39m ])\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[43mcompare_report\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_definition\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_def\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset_cur\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreference_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset_ref\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m compare_report\n",
      "\u001b[31mTypeError\u001b[39m: Report.run() got an unexpected keyword argument 'data_definition'"
     ]
    }
   ],
   "source": [
    "\n",
    "compare_report = Report(metrics=[\n",
    "    DriftedColumnsCount(),                 # count of columns detected as drifted\n",
    "    ValueDrift(column=\"age\"),\n",
    "    ValueDrift(column=\"income\"),\n",
    "    ValueDrift(column=\"pred_proba\"),       # score/probability drift\n",
    "    Accuracy(),\n",
    "    F1Score(),\n",
    "])\n",
    "\n",
    "compare_report.run(\n",
    "    data_definition=data_def,\n",
    "    data=dataset_cur,\n",
    "    reference_data=dataset_ref,\n",
    ")\n",
    "compare_report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8530ba3d",
   "metadata": {},
   "source": [
    "## 9) Extract numbers programmatically (JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a053ff8f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Report' object has no attribute 'as_dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m as_dict = \u001b[43mcompare_report\u001b[49m\u001b[43m.\u001b[49m\u001b[43mas_dict\u001b[49m()\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Walk the structure to find Accuracy and F1 in a generic way\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfind_metric\u001b[39m(result_dict, metric_name):\n",
      "\u001b[31mAttributeError\u001b[39m: 'Report' object has no attribute 'as_dict'"
     ]
    }
   ],
   "source": [
    "\n",
    "as_dict = compare_report.as_dict()\n",
    "# Walk the structure to find Accuracy and F1 in a generic way\n",
    "def find_metric(result_dict, metric_name):\n",
    "    hits = []\n",
    "    for sec in result_dict.get(\"metrics\", []):\n",
    "        if sec.get(\"metric\") == metric_name and \"result\" in sec:\n",
    "            hits.append(sec[\"result\"])\n",
    "    return hits\n",
    "\n",
    "acc_values = find_metric(as_dict, \"Accuracy\")\n",
    "f1_values  = find_metric(as_dict, \"F1Score\")\n",
    "\n",
    "print(\"Extracted Accuracy result objects:\", acc_values[:1])\n",
    "print(\"Extracted F1Score result objects:\", f1_values[:1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb57343",
   "metadata": {},
   "source": [
    "## 10) Save reports to HTML and JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0081491",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m out_dir = \u001b[33m\"\u001b[39m\u001b[33m/mnt/data/evidently_lab_outputs\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mos\u001b[49m.makedirs(out_dir, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# HTML\u001b[39;00m\n\u001b[32m      5\u001b[39m dq_report.save_html(os.path.join(out_dir, \u001b[33m\"\u001b[39m\u001b[33m01_data_quality.html\u001b[39m\u001b[33m\"\u001b[39m))\n",
      "\u001b[31mNameError\u001b[39m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "out_dir = \"/mnt/data/evidently_lab_outputs\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "# HTML\n",
    "dq_report.save_html(os.path.join(out_dir, \"01_data_quality.html\"))\n",
    "quality_report.save_html(os.path.join(out_dir, \"02_model_quality_current.html\"))\n",
    "segmented_report.save_html(os.path.join(out_dir, \"03_segmented_quality.html\"))\n",
    "compare_report.save_html(os.path.join(out_dir, \"04_compare_ref_vs_cur.html\"))\n",
    "\n",
    "# JSON (machine-readable)\n",
    "with open(os.path.join(out_dir, \"04_compare_ref_vs_cur.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(compare_report.as_dict(), f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"Saved to:\", out_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3ba2a3",
   "metadata": {},
   "source": [
    "\n",
    "## 11) Exercises / Variations\n",
    "\n",
    "1. **Change the threshold:** Replace `prediction` with labels derived from a custom threshold (e.g. 0.35, 0.7) and re-run quality metrics.\n",
    "2. **Add more features:** Insert additional numeric/categorical columns and add `ValueDrift` for them.\n",
    "3. **Imbalance scenario:** Modify the data generator to reduce positives to ~10% and see how Precision/Recall/F1 change.\n",
    "4. **Per-segment deep dive:** Add more metrics inside `GroupBy`, e.g., `RocAuc()`.\n",
    "5. **Multiclass extension:** Swap to `MulticlassClassification` (not covered here) and adapt metrics accordingly.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
