# üéì Git Lab 03: ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô Git ‡∏Å‡∏±‡∏ö Machine Learning Project (MLOps)

## üìã Pipeline Overview

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     ML Project Git Workflow Pipeline                             ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                                  ‚îÇ
‚îÇ   LOCAL REPOSITORY                              REMOTE REPOSITORY               ‚îÇ
‚îÇ   ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê                              ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê               ‚îÇ
‚îÇ                                                                                  ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                ‚îÇ
‚îÇ   ‚îÇ   ML Code    ‚îÇ  git add                     ‚îÇ   GitHub/    ‚îÇ                ‚îÇ
‚îÇ   ‚îÇ   + Data     ‚îÇ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                  ‚îÇ   GitLab     ‚îÇ                ‚îÇ
‚îÇ   ‚îÇ   + Models   ‚îÇ           ‚îÇ                  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò           ‚ñº                         ‚ñ≤                        ‚îÇ
‚îÇ          ‚îÇ           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                  ‚îÇ                        ‚îÇ
‚îÇ          ‚îÇ           ‚îÇ   Staging    ‚îÇ                  ‚îÇ                        ‚îÇ
‚îÇ          ‚îÇ           ‚îÇ    Area      ‚îÇ                  ‚îÇ                        ‚îÇ
‚îÇ          ‚îÇ           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                  ‚îÇ                        ‚îÇ
‚îÇ          ‚îÇ                   ‚îÇ                         ‚îÇ                        ‚îÇ
‚îÇ          ‚îÇ                   ‚îÇ git commit              ‚îÇ git push               ‚îÇ
‚îÇ          ‚îÇ                   ‚ñº                         ‚îÇ                        ‚îÇ
‚îÇ          ‚îÇ           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                  ‚îÇ                        ‚îÇ
‚îÇ          ‚îÇ           ‚îÇ    Local     ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                        ‚îÇ
‚îÇ          ‚îÇ           ‚îÇ  Repository  ‚îÇ                                           ‚îÇ
‚îÇ          ‚îÇ           ‚îÇ  (versions)  ‚îÇ                                           ‚îÇ
‚îÇ          ‚îÇ           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                           ‚îÇ
‚îÇ          ‚îÇ                                                                      ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ
‚îÇ   ‚îÇ                    ML Project Components                              ‚îÇ     ‚îÇ
‚îÇ   ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ     ‚îÇ
‚îÇ   ‚îÇ  ‚îÇ  Data   ‚îÇ  ‚îÇ Feature ‚îÇ  ‚îÇ  Model  ‚îÇ  ‚îÇ  Eval   ‚îÇ  ‚îÇ Config  ‚îÇ    ‚îÇ     ‚îÇ
‚îÇ   ‚îÇ  ‚îÇ Loading ‚îÇ‚îÄ‚ñ∂‚îÇEngineer ‚îÇ‚îÄ‚ñ∂‚îÇTraining ‚îÇ‚îÄ‚ñ∂‚îÇ Metrics ‚îÇ‚îÄ‚ñ∂‚îÇ  Files  ‚îÇ    ‚îÇ     ‚îÇ
‚îÇ   ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ     ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ
‚îÇ                                                                                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üìÅ ‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Ñ ML ‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á

```
ml-git-lab03_advance/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ data_loader.py       # ‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏•‡∏∞‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
‚îÇ   ‚îú‚îÄ‚îÄ feature_engineer.py  # ‡∏™‡∏£‡πâ‡∏≤‡∏á features
‚îÇ   ‚îú‚îÄ‚îÄ train.py             # training model
‚îÇ   ‚îî‚îÄ‚îÄ evaluate.py          # ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏• model
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îî‚îÄ‚îÄ model_config.yaml    # ‡∏Ñ‡πà‡∏≤ hyperparameters
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ raw/                 # ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡∏¥‡∏ö
‚îÇ   ‚îî‚îÄ‚îÄ processed/           # ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÅ‡∏•‡πâ‡∏ß
‚îú‚îÄ‚îÄ models/                  # ‡πÄ‡∏Å‡πá‡∏ö trained models
‚îú‚îÄ‚îÄ results/                 # ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏•‡∏≠‡∏á
‚îú‚îÄ‚îÄ notebooks/               # Jupyter notebooks (optional)
‚îú‚îÄ‚îÄ requirements.txt         # dependencies
‚îú‚îÄ‚îÄ .gitignore              # ‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á track
‚îî‚îÄ‚îÄ README.md               # ‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Ñ
```

---

## üéØ ‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ

1. ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Ñ ML ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡∏Å‡∏±‡∏ö Git
2. ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ `.gitignore` ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö ML project
3. ‡∏ù‡∏∂‡∏Å‡πÉ‡∏ä‡πâ Git workflow ‡∏Å‡∏±‡∏ö sklearn ‡πÅ‡∏•‡∏∞ data pipeline
4. ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏Å‡∏≤‡∏£ version control ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö ML experiments

---

## ‚öôÔ∏è Git Configuration (‡∏ó‡∏≥‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏° Lab)

```bash
# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏ä‡∏∑‡πà‡∏≠‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ (‡πÉ‡∏ä‡πâ‡∏ä‡∏∑‡πà‡∏≠‡∏à‡∏£‡∏¥‡∏á‡∏Ç‡∏≠‡∏á‡∏ô‡∏±‡∏Å‡∏®‡∏∂‡∏Å‡∏©‡∏≤)
git config --global user.name "Your Name"

# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏≠‡∏µ‡πÄ‡∏°‡∏• (‡πÉ‡∏ä‡πâ‡∏≠‡∏µ‡πÄ‡∏°‡∏•‡∏ó‡∏µ‡πà‡∏•‡∏á‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô‡∏Å‡∏±‡∏ö GitHub)
git config --global user.email "your.email@example.com"

# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ default branch ‡πÄ‡∏õ‡πá‡∏ô main
git config --global init.defaultBranch main

# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤
git config --list
```

---

## üöÄ Part 1: ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏™‡∏£‡πâ‡∏≤‡∏á ML Project

### Step 1: ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Ñ

```bash
mkdir ml-git-lab03_advance
cd ml-git-lab03_advance
```

**‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö:**
```bash
pwd
```

**‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå:**
```
/home/student/ml-git-lab03_advance
```

---

### Step 2: ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô Git Repository

```bash
git init
```

**‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå:**
```
Initialized empty Git repository in /home/student/ml-git-lab03_advance/.git/
```

---

### Step 3: ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå ML Project

```bash
mkdir -p src config data/raw data/processed models results notebooks
```

**‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö:**
```bash
ls -la
```

**‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå:**
```
total 36
drwxr-xr-x 10 student student 4096 Dec 16 10:00 .
drwxr-xr-x  3 student student 4096 Dec 16 10:00 ..
drwxr-xr-x  2 student student 4096 Dec 16 10:00 config
drwxr-xr-x  4 student student 4096 Dec 16 10:00 data
drwxr-xr-x  7 student student 4096 Dec 16 10:00 .git
drwxr-xr-x  2 student student 4096 Dec 16 10:00 models
drwxr-xr-x  2 student student 4096 Dec 16 10:00 notebooks
drwxr-xr-x  2 student student 4096 Dec 16 10:00 results
drwxr-xr-x  2 student student 4096 Dec 16 10:00 src
```

---

### Step 4: ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå .gitignore ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö ML Project

> üìù **‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏°‡∏≤‡∏Å:** ‡πÑ‡∏ü‡∏•‡πå `.gitignore` ‡∏ä‡πà‡∏ß‡∏¢‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà (data, models) ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß‡∏ñ‡∏π‡∏Å track

```bash
cat > .gitignore << 'EOF'
# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
env/
venv/
.venv/

# Jupyter Notebooks
.ipynb_checkpoints/
*.ipynb_checkpoints

# Data files (‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà)
data/raw/*.csv
data/raw/*.json
data/raw/*.xlsx
data/processed/*.csv
data/processed/*.pkl
data/processed/*.parquet
!data/raw/.gitkeep
!data/processed/.gitkeep

# Model files (‡πÑ‡∏ü‡∏•‡πå model ‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà)
models/*.pkl
models/*.joblib
models/*.h5
models/*.pt
models/*.pth
!models/.gitkeep

# Results (‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏µ‡πà generate ‡πÉ‡∏´‡∏°‡πà‡πÑ‡∏î‡πâ)
results/*.png
results/*.csv
results/*.json
!results/.gitkeep

# IDE
.vscode/
.idea/
*.swp
*.swo

# OS
.DS_Store
Thumbs.db

# Secrets
.env
*.key
credentials.json
EOF
```

**‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö:**
```bash
cat .gitignore
```

---

### Step 5: ‡∏™‡∏£‡πâ‡∏≤‡∏á .gitkeep ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ß‡πà‡∏≤‡∏á

> üìù **‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢:** Git ‡πÑ‡∏°‡πà track ‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ß‡πà‡∏≤‡∏á ‡∏î‡∏±‡∏á‡∏ô‡∏±‡πâ‡∏ô‡πÉ‡∏ä‡πâ `.gitkeep` ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏±‡∏Å‡∏©‡∏≤‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á

```bash
touch data/raw/.gitkeep
touch data/processed/.gitkeep
touch models/.gitkeep
touch results/.gitkeep
touch notebooks/.gitkeep
```

**‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö:**
```bash
ls -la data/raw/
```

**‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå:**
```
total 8
drwxr-xr-x 2 student student 4096 Dec 16 10:05 .
drwxr-xr-x 4 student student 4096 Dec 16 10:00 ..
-rw-r--r-- 1 student student    0 Dec 16 10:05 .gitkeep
```

---

### Step 6: ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå requirements.txt

```bash
cat > requirements.txt << 'EOF'
# ML Libraries
scikit-learn>=1.3.0
pandas>=2.0.0
numpy>=1.24.0

# Data Visualization
matplotlib>=3.7.0
seaborn>=0.12.0

# Configuration
pyyaml>=6.0

# Model Persistence
joblib>=1.3.0
EOF
```

**‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö:**
```bash
cat requirements.txt
```

---

### Step 7: ‡∏™‡∏£‡πâ‡∏≤‡∏á README.md

```bash
cat > README.md << 'EOF'
# ML Git Lab 03 - Scikit-learn Classification Project

## üìã Project Overview
‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Ñ‡∏ô‡∏µ‡πâ‡∏™‡∏≤‡∏ò‡∏¥‡∏ï‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ Git workflow ‡∏Å‡∏±‡∏ö Machine Learning project ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ Iris dataset ‡πÅ‡∏•‡∏∞ scikit-learn

## üèóÔ∏è Project Structure
```
ml-git-lab03_advance/
‚îú‚îÄ‚îÄ src/              # Source code
‚îú‚îÄ‚îÄ config/           # Configuration files
‚îú‚îÄ‚îÄ data/             # Data files
‚îú‚îÄ‚îÄ models/           # Trained models
‚îú‚îÄ‚îÄ results/          # Evaluation results
‚îî‚îÄ‚îÄ notebooks/        # Jupyter notebooks
```

## üöÄ Quick Start
```bash
# Install dependencies
pip install -r requirements.txt

# Train model
python src/train.py

# Evaluate model
python src/evaluate.py
```

## üìä Dataset
- **Name:** Iris Dataset
- **Source:** scikit-learn built-in
- **Features:** 4 (sepal/petal length/width)
- **Classes:** 3 (setosa, versicolor, virginica)

## üë• Contributors
- Student Name
EOF
```

---

### Step 8: ‡∏™‡∏£‡πâ‡∏≤‡∏á src/__init__.py

```bash
cat > src/__init__.py << 'EOF'
"""
ML Git Lab 03 - Scikit-learn Classification Project
====================================================
‡πÇ‡∏°‡∏î‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö training ‡πÅ‡∏•‡∏∞ evaluation ‡∏Ç‡∏≠‡∏á ML models
"""

__version__ = "1.0.0"
__author__ = "MLOps Student"
EOF
```

---

### Step 9: Commit ‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÅ‡∏£‡∏Å - Project Structure

```bash
git status
```

**‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå:**
```
On branch main

No commits yet

Untracked files:
  (use "git add <file>..." to include in what will be committed)
        .gitignore
        README.md
        config/
        data/
        models/
        notebooks/
        requirements.txt
        results/
        src/
```

```bash
git add .
git commit -m "Initial commit: ML project structure with .gitignore"
```

**‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå:**
```
[main (root-commit) a1b2c3d] Initial commit: ML project structure with .gitignore
 8 files changed, 95 insertions(+)
 create mode 100644 .gitignore
 create mode 100644 README.md
 create mode 100644 data/processed/.gitkeep
 create mode 100644 data/raw/.gitkeep
 create mode 100644 models/.gitkeep
 create mode 100644 notebooks/.gitkeep
 create mode 100644 requirements.txt
 create mode 100644 results/.gitkeep
 create mode 100644 src/__init__.py
```

**‡∏î‡∏π log:**
```bash
git log --oneline
```

**‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå:**
```
a1b2c3d (HEAD -> main) Initial commit: ML project structure with .gitignore
```

---

## üìä Part 2: ‡∏™‡∏£‡πâ‡∏≤‡∏á Data Loading Module

### Step 10: ‡∏™‡∏£‡πâ‡∏≤‡∏á data_loader.py

```bash
cat > src/data_loader.py << 'EOF'
"""
Data Loader Module
==================
‡πÇ‡∏°‡∏î‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏•‡∏∞‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Iris dataset
"""

import pandas as pd
import numpy as np
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split


def load_iris_data():
    """
    ‡πÇ‡∏´‡∏•‡∏î Iris dataset ‡∏à‡∏≤‡∏Å scikit-learn
    
    Returns:
        tuple: (X, y, feature_names, target_names)
    """
    iris = load_iris()
    X = iris.data
    y = iris.target
    feature_names = iris.feature_names
    target_names = iris.target_names
    
    print(f"‚úÖ Loaded Iris dataset")
    print(f"   - Samples: {X.shape[0]}")
    print(f"   - Features: {X.shape[1]}")
    print(f"   - Classes: {len(target_names)}")
    
    return X, y, feature_names, target_names


def create_dataframe(X, y, feature_names, target_names):
    """
    ‡∏™‡∏£‡πâ‡∏≤‡∏á DataFrame ‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
    
    Args:
        X: Feature matrix
        y: Target vector
        feature_names: ‡∏ä‡∏∑‡πà‡∏≠ features
        target_names: ‡∏ä‡∏∑‡πà‡∏≠ classes
    
    Returns:
        pd.DataFrame: DataFrame ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
    """
    df = pd.DataFrame(X, columns=feature_names)
    df['target'] = y
    df['target_name'] = df['target'].map(
        lambda x: target_names[x]
    )
    return df


def split_data(X, y, test_size=0.2, random_state=42):
    """
    ‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏õ‡πá‡∏ô train ‡πÅ‡∏•‡∏∞ test sets
    
    Args:
        X: Feature matrix
        y: Target vector
        test_size: ‡∏™‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô test set (default: 0.2)
        random_state: random seed (default: 42)
    
    Returns:
        tuple: (X_train, X_test, y_train, y_test)
    """
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, 
        test_size=test_size, 
        random_state=random_state,
        stratify=y
    )
    
    print(f"‚úÖ Data split completed")
    print(f"   - Train samples: {X_train.shape[0]}")
    print(f"   - Test samples: {X_test.shape[0]}")
    
    return X_train, X_test, y_train, y_test


def get_data_summary(df):
    """
    ‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô
    
    Args:
        df: DataFrame
    
    Returns:
        dict: ‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
    """
    summary = {
        'n_samples': len(df),
        'n_features': len(df.columns) - 2,  # ‡πÑ‡∏°‡πà‡∏ô‡∏±‡∏ö target columns
        'class_distribution': df['target_name'].value_counts().to_dict(),
        'missing_values': df.isnull().sum().sum()
    }
    return summary


if __name__ == "__main__":
    # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö module
    print("=" * 50)
    print("Testing Data Loader Module")
    print("=" * 50)
    
    # ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
    X, y, feature_names, target_names = load_iris_data()
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á DataFrame
    df = create_dataframe(X, y, feature_names, target_names)
    print(f"\nüìä DataFrame shape: {df.shape}")
    print(f"\nüìã First 5 rows:")
    print(df.head())
    
    # ‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
    X_train, X_test, y_train, y_test = split_data(X, y)
    
    # ‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
    summary = get_data_summary(df)
    print(f"\nüìà Data Summary:")
    for key, value in summary.items():
        print(f"   - {key}: {value}")
EOF
```

**‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÑ‡∏ü‡∏•‡πå:**
```bash
cat src/data_loader.py
```

---

### Step 11: ‡∏ó‡∏î‡∏™‡∏≠‡∏ö data_loader.py

```bash
python src/data_loader.py
```

**‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå:**
```
==================================================
Testing Data Loader Module
==================================================
‚úÖ Loaded Iris dataset
   - Samples: 150
   - Features: 4
   - Classes: 3

üìä DataFrame shape: (150, 6)

üìã First 5 rows:
   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  target target_name
0                5.1               3.5                1.4               0.2       0      setosa
1                4.9               3.0                1.4               0.2       0      setosa
2                4.7               3.2                1.3               0.2       0      setosa
3                4.6               3.1                1.5               0.2       0      setosa
4                5.0               3.6                1.4               0.2       0      setosa
‚úÖ Data split completed
   - Train samples: 120
   - Test samples: 30

üìà Data Summary:
   - n_samples: 150
   - n_features: 4
   - class_distribution: {'setosa': 50, 'versicolor': 50, 'virginica': 50}
   - missing_values: 0
```

---

### Step 12: Commit Data Loader

```bash
git status
```

**‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå:**
```
On branch main
Untracked files:
  (use "git add <file>..." to include in what will be committed)
        src/data_loader.py

nothing added to commit but untracked files present (use "git add" to track)
```

```bash
git add src/data_loader.py
git commit -m "Add data_loader module with Iris dataset support"
```

**‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå:**
```
[main b2c3d4e] Add data_loader module with Iris dataset support
 1 file changed, 107 insertions(+)
 create mode 100644 src/data_loader.py
```

---

## üîß Part 3: ‡∏™‡∏£‡πâ‡∏≤‡∏á Feature Engineering Module

### Step 13: ‡∏™‡∏£‡πâ‡∏≤‡∏á feature_engineer.py

```bash
cat > src/feature_engineer.py << 'EOF'
"""
Feature Engineering Module
==========================
‡πÇ‡∏°‡∏î‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏•‡∏∞‡πÅ‡∏õ‡∏•‡∏á features
"""

import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.preprocessing import LabelEncoder


class FeatureEngineer:
    """
    Class ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ feature engineering
    """
    
    def __init__(self, scaling_method='standard'):
        """
        Initialize FeatureEngineer
        
        Args:
            scaling_method: ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£ scale ('standard' ‡∏´‡∏£‡∏∑‡∏≠ 'minmax')
        """
        self.scaling_method = scaling_method
        self.scaler = None
        self.feature_names = None
        
    def fit(self, X, feature_names=None):
        """
        Fit scaler ‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        
        Args:
            X: Feature matrix
            feature_names: ‡∏ä‡∏∑‡πà‡∏≠ features (optional)
        """
        if self.scaling_method == 'standard':
            self.scaler = StandardScaler()
        elif self.scaling_method == 'minmax':
            self.scaler = MinMaxScaler()
        else:
            raise ValueError(f"Unknown scaling method: {self.scaling_method}")
        
        self.scaler.fit(X)
        self.feature_names = feature_names
        print(f"‚úÖ Scaler fitted with {self.scaling_method} method")
        
    def transform(self, X):
        """
        Transform ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡πâ‡∏ß‡∏¢ scaler ‡∏ó‡∏µ‡πà fit ‡πÅ‡∏•‡πâ‡∏ß
        
        Args:
            X: Feature matrix
        
        Returns:
            np.ndarray: Scaled features
        """
        if self.scaler is None:
            raise ValueError("Scaler not fitted. Call fit() first.")
        
        X_scaled = self.scaler.transform(X)
        return X_scaled
    
    def fit_transform(self, X, feature_names=None):
        """
        Fit ‡πÅ‡∏•‡∏∞ transform ‡πÉ‡∏ô‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡πÄ‡∏î‡∏µ‡∏¢‡∏ß
        
        Args:
            X: Feature matrix
            feature_names: ‡∏ä‡∏∑‡πà‡∏≠ features (optional)
        
        Returns:
            np.ndarray: Scaled features
        """
        self.fit(X, feature_names)
        return self.transform(X)
    
    def create_polynomial_features(self, X, degree=2):
        """
        ‡∏™‡∏£‡πâ‡∏≤‡∏á polynomial features
        
        Args:
            X: Feature matrix
            degree: degree ‡∏Ç‡∏≠‡∏á polynomial (default: 2)
        
        Returns:
            np.ndarray: Features ‡∏û‡∏£‡πâ‡∏≠‡∏° polynomial terms
        """
        from sklearn.preprocessing import PolynomialFeatures
        poly = PolynomialFeatures(degree=degree, include_bias=False)
        X_poly = poly.fit_transform(X)
        print(f"‚úÖ Created polynomial features (degree={degree})")
        print(f"   - Original features: {X.shape[1]}")
        print(f"   - New features: {X_poly.shape[1]}")
        return X_poly
    
    def get_feature_statistics(self, X):
        """
        ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏Ç‡∏≠‡∏á features
        
        Args:
            X: Feature matrix
        
        Returns:
            pd.DataFrame: ‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞ feature
        """
        if self.feature_names is not None:
            columns = self.feature_names
        else:
            columns = [f'feature_{i}' for i in range(X.shape[1])]
        
        df = pd.DataFrame(X, columns=columns)
        stats = df.describe().T
        stats['variance'] = df.var()
        return stats


def create_interaction_features(X, feature_names=None):
    """
    ‡∏™‡∏£‡πâ‡∏≤‡∏á interaction features (‡∏Ñ‡∏π‡∏ì‡∏Å‡∏±‡∏ô‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á features)
    
    Args:
        X: Feature matrix
        feature_names: ‡∏ä‡∏∑‡πà‡∏≠ features (optional)
    
    Returns:
        tuple: (X_new, new_feature_names)
    """
    n_features = X.shape[1]
    interactions = []
    new_names = []
    
    if feature_names is None:
        feature_names = [f'f{i}' for i in range(n_features)]
    
    for i in range(n_features):
        for j in range(i+1, n_features):
            interactions.append(X[:, i] * X[:, j])
            new_names.append(f'{feature_names[i]}_x_{feature_names[j]}')
    
    X_interactions = np.column_stack(interactions)
    X_new = np.hstack([X, X_interactions])
    all_names = list(feature_names) + new_names
    
    print(f"‚úÖ Created {len(interactions)} interaction features")
    
    return X_new, all_names


if __name__ == "__main__":
    # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö module
    print("=" * 50)
    print("Testing Feature Engineering Module")
    print("=" * 50)
    
    # Import data_loader
    from data_loader import load_iris_data, split_data
    
    # ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
    X, y, feature_names, target_names = load_iris_data()
    X_train, X_test, y_train, y_test = split_data(X, y)
    
    # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö FeatureEngineer
    print("\nüìä Testing StandardScaler:")
    fe = FeatureEngineer(scaling_method='standard')
    X_train_scaled = fe.fit_transform(X_train, feature_names)
    X_test_scaled = fe.transform(X_test)
    
    print(f"   - Train mean (should be ~0): {X_train_scaled.mean(axis=0).round(2)}")
    print(f"   - Train std (should be ~1): {X_train_scaled.std(axis=0).round(2)}")
    
    # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö interaction features
    print("\nüìä Testing Interaction Features:")
    X_new, new_names = create_interaction_features(X_train[:5], feature_names)
    print(f"   - Original shape: {X_train[:5].shape}")
    print(f"   - New shape: {X_new.shape}")
    print(f"   - New feature names: {new_names[-3:]}")
    
    # ‡∏î‡∏π‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥
    print("\nüìà Feature Statistics (scaled data):")
    stats = fe.get_feature_statistics(X_train_scaled)
    print(stats[['mean', 'std', 'min', 'max']].round(3))
EOF
```

---

### Step 14: ‡∏ó‡∏î‡∏™‡∏≠‡∏ö feature_engineer.py

```bash
cd src
python feature_engineer.py
cd ..
```

**‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå:**
```
==================================================
Testing Feature Engineering Module
==================================================
‚úÖ Loaded Iris dataset
   - Samples: 150
   - Features: 4
   - Classes: 3
‚úÖ Data split completed
   - Train samples: 120
   - Test samples: 30

üìä Testing StandardScaler:
‚úÖ Scaler fitted with standard method
   - Train mean (should be ~0): [-0.  0. -0.  0.]
   - Train std (should be ~1): [1. 1. 1. 1.]

üìä Testing Interaction Features:
‚úÖ Created 6 interaction features
   - Original shape: (5, 4)
   - New shape: (5, 10)
   - New feature names: ['petal length (cm)_x_petal width (cm)', ...]

üìà Feature Statistics (scaled data):
                     mean    std    min    max
sepal length (cm)   -0.000  1.004 -1.870  2.492
sepal width (cm)     0.000  1.004 -2.431  2.791
petal length (cm)    0.000  1.004 -1.567  1.785
petal width (cm)    -0.000  1.004 -1.447  1.712
```

---

### Step 15: Commit Feature Engineering Module

```bash
git add src/feature_engineer.py
git commit -m "Add feature_engineer module with scaling and transformations"
```

**‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå:**
```
[main c3d4e5f] Add feature_engineer module with scaling and transformations
 1 file changed, 168 insertions(+)
 create mode 100644 src/feature_engineer.py
```

---

## ü§ñ Part 4: ‡∏™‡∏£‡πâ‡∏≤‡∏á Training Module

### Step 16: ‡∏™‡∏£‡πâ‡∏≤‡∏á config/model_config.yaml

```bash
cat > config/model_config.yaml << 'EOF'
# Model Configuration
# ===================

# Data settings
data:
  test_size: 0.2
  random_state: 42

# Feature engineering
features:
  scaling_method: standard  # standard or minmax
  create_interactions: false
  polynomial_degree: 1

# Model settings
model:
  type: random_forest  # random_forest, logistic_regression, svm
  
  # Random Forest parameters
  random_forest:
    n_estimators: 100
    max_depth: 5
    min_samples_split: 2
    min_samples_leaf: 1
    random_state: 42
  
  # Logistic Regression parameters
  logistic_regression:
    C: 1.0
    max_iter: 1000
    random_state: 42
  
  # SVM parameters
  svm:
    C: 1.0
    kernel: rbf
    gamma: scale
    random_state: 42

# Training settings
training:
  cross_validation: true
  cv_folds: 5
  verbose: true

# Output settings
output:
  save_model: true
  model_path: models/model.joblib
  save_results: true
  results_path: results/metrics.json
EOF
```

---

### Step 17: ‡∏™‡∏£‡πâ‡∏≤‡∏á train.py

```bash
cat > src/train.py << 'EOF'
"""
Model Training Module
=====================
‡πÇ‡∏°‡∏î‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö training ML models
"""

import os
import sys
import yaml
import json
import joblib
import numpy as np
from datetime import datetime

from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.model_selection import cross_val_score

# Import local modules
from data_loader import load_iris_data, split_data
from feature_engineer import FeatureEngineer


def load_config(config_path='../config/model_config.yaml'):
    """
    ‡πÇ‡∏´‡∏•‡∏î configuration ‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå YAML
    
    Args:
        config_path: path ‡πÑ‡∏õ‡∏¢‡∏±‡∏á‡πÑ‡∏ü‡∏•‡πå config
    
    Returns:
        dict: configuration
    """
    with open(config_path, 'r') as f:
        config = yaml.safe_load(f)
    print(f"‚úÖ Loaded config from {config_path}")
    return config


def create_model(config):
    """
    ‡∏™‡∏£‡πâ‡∏≤‡∏á model ‡∏ï‡∏≤‡∏° configuration
    
    Args:
        config: configuration dict
    
    Returns:
        sklearn model instance
    """
    model_type = config['model']['type']
    
    if model_type == 'random_forest':
        params = config['model']['random_forest']
        model = RandomForestClassifier(**params)
    elif model_type == 'logistic_regression':
        params = config['model']['logistic_regression']
        model = LogisticRegression(**params)
    elif model_type == 'svm':
        params = config['model']['svm']
        model = SVC(**params)
    else:
        raise ValueError(f"Unknown model type: {model_type}")
    
    print(f"‚úÖ Created {model_type} model")
    return model


def train_model(model, X_train, y_train, config):
    """
    Train model
    
    Args:
        model: sklearn model
        X_train: training features
        y_train: training labels
        config: configuration dict
    
    Returns:
        tuple: (trained_model, cv_scores)
    """
    cv_scores = None
    
    # Cross-validation ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡∏¥‡∏î‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô
    if config['training']['cross_validation']:
        n_folds = config['training']['cv_folds']
        cv_scores = cross_val_score(model, X_train, y_train, cv=n_folds)
        print(f"‚úÖ Cross-validation ({n_folds} folds):")
        print(f"   - Scores: {cv_scores.round(4)}")
        print(f"   - Mean: {cv_scores.mean():.4f} (+/- {cv_scores.std()*2:.4f})")
    
    # Train final model
    model.fit(X_train, y_train)
    print(f"‚úÖ Model trained on {X_train.shape[0]} samples")
    
    return model, cv_scores


def save_model(model, scaler, config, output_path):
    """
    ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å model ‡πÅ‡∏•‡∏∞ scaler
    
    Args:
        model: trained model
        scaler: fitted scaler
        config: configuration
        output_path: path ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å
    """
    model_data = {
        'model': model,
        'scaler': scaler,
        'config': config,
        'timestamp': datetime.now().isoformat()
    }
    
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    joblib.dump(model_data, output_path)
    print(f"‚úÖ Model saved to {output_path}")


def save_training_results(results, output_path):
    """
    ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡∏Å‡∏≤‡∏£ training
    
    Args:
        results: dict ‡∏Ç‡∏≠‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
        output_path: path ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å
    """
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    with open(output_path, 'w') as f:
        json.dump(results, f, indent=2)
    print(f"‚úÖ Results saved to {output_path}")


def main():
    """
    Main training pipeline
    """
    print("=" * 60)
    print("üöÄ ML Training Pipeline")
    print("=" * 60)
    
    # 1. Load configuration
    print("\nüìÅ Step 1: Loading configuration...")
    config = load_config()
    
    # 2. Load data
    print("\nüìä Step 2: Loading data...")
    X, y, feature_names, target_names = load_iris_data()
    
    # 3. Split data
    print("\n‚úÇÔ∏è Step 3: Splitting data...")
    test_size = config['data']['test_size']
    random_state = config['data']['random_state']
    X_train, X_test, y_train, y_test = split_data(
        X, y, test_size=test_size, random_state=random_state
    )
    
    # 4. Feature engineering
    print("\nüîß Step 4: Feature engineering...")
    scaling_method = config['features']['scaling_method']
    fe = FeatureEngineer(scaling_method=scaling_method)
    X_train_scaled = fe.fit_transform(X_train, feature_names)
    X_test_scaled = fe.transform(X_test)
    
    # 5. Create model
    print("\nü§ñ Step 5: Creating model...")
    model = create_model(config)
    
    # 6. Train model
    print("\nüéØ Step 6: Training model...")
    model, cv_scores = train_model(model, X_train_scaled, y_train, config)
    
    # 7. Evaluate on test set
    print("\nüìà Step 7: Evaluating on test set...")
    train_score = model.score(X_train_scaled, y_train)
    test_score = model.score(X_test_scaled, y_test)
    print(f"   - Train accuracy: {train_score:.4f}")
    print(f"   - Test accuracy: {test_score:.4f}")
    
    # 8. Save model
    if config['output']['save_model']:
        print("\nüíæ Step 8: Saving model...")
        model_path = config['output']['model_path']
        save_model(model, fe.scaler, config, model_path)
    
    # 9. Save results
    if config['output']['save_results']:
        print("\nüìù Step 9: Saving results...")
        results = {
            'model_type': config['model']['type'],
            'train_accuracy': float(train_score),
            'test_accuracy': float(test_score),
            'cv_scores': cv_scores.tolist() if cv_scores is not None else None,
            'cv_mean': float(cv_scores.mean()) if cv_scores is not None else None,
            'cv_std': float(cv_scores.std()) if cv_scores is not None else None,
            'n_train_samples': int(X_train.shape[0]),
            'n_test_samples': int(X_test.shape[0]),
            'timestamp': datetime.now().isoformat()
        }
        results_path = config['output']['results_path']
        save_training_results(results, results_path)
    
    print("\n" + "=" * 60)
    print("‚úÖ Training pipeline completed!")
    print("=" * 60)
    
    return model, fe.scaler, results


if __name__ == "__main__":
    model, scaler, results = main()
EOF
```

---

### Step 18: ‡∏ó‡∏î‡∏™‡∏≠‡∏ö train.py

```bash
cd src
python train.py
cd ..
```

**‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå:**
```
============================================================
üöÄ ML Training Pipeline
============================================================

üìÅ Step 1: Loading configuration...
‚úÖ Loaded config from ../config/model_config.yaml

üìä Step 2: Loading data...
‚úÖ Loaded Iris dataset
   - Samples: 150
   - Features: 4
   - Classes: 3

‚úÇÔ∏è Step 3: Splitting data...
‚úÖ Data split completed
   - Train samples: 120
   - Test samples: 30

üîß Step 4: Feature engineering...
‚úÖ Scaler fitted with standard method

ü§ñ Step 5: Creating model...
‚úÖ Created random_forest model

üéØ Step 6: Training model...
‚úÖ Cross-validation (5 folds):
   - Scores: [0.9583 0.9167 0.9583 0.9583 0.9167]
   - Mean: 0.9417 (+/- 0.0385)
‚úÖ Model trained on 120 samples

üìà Step 7: Evaluating on test set...
   - Train accuracy: 1.0000
   - Test accuracy: 0.9667

üíæ Step 8: Saving model...
‚úÖ Model saved to ../models/model.joblib

üìù Step 9: Saving results...
‚úÖ Results saved to ../results/metrics.json

============================================================
‚úÖ Training pipeline completed!
============================================================
```

---

### Step 19: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ .gitignore ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô

```bash
git status
```

**‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå:**
```
On branch main
Untracked files:
  (use "git add <file>..." to include in what will be committed)
        config/model_config.yaml
        src/train.py

nothing added to commit but untracked files present (use "git add" to track)
```

> üìù **‡∏™‡∏±‡∏á‡πÄ‡∏Å‡∏ï:** ‡πÑ‡∏ü‡∏•‡πå `models/model.joblib` ‡πÅ‡∏•‡∏∞ `results/metrics.json` ‡πÑ‡∏°‡πà‡πÅ‡∏™‡∏î‡∏á‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏ñ‡∏π‡∏Å ignore ‡πÅ‡∏•‡πâ‡∏ß!

**‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å ignore:**
```bash
git status --ignored
```

**‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå:**
```
On branch main
Untracked files:
  (use "git add <file>..." to include in what will be committed)
        config/model_config.yaml
        src/train.py

Ignored files:
  (use "git add -f <file>..." to include in what will be committed)
        models/model.joblib
        results/metrics.json
        src/__pycache__/
```

---

### Step 20: Commit Training Module

```bash
git add config/model_config.yaml src/train.py
git commit -m "Add training pipeline with config and model saving"
```

**‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå:**
```
[main d4e5f6g] Add training pipeline with config and model saving
 2 files changed, 242 insertions(+)
 create mode 100644 config/model_config.yaml
 create mode 100644 src/train.py
```

---

## üìà Part 5: ‡∏™‡∏£‡πâ‡∏≤‡∏á Evaluation Module

### Step 21: ‡∏™‡∏£‡πâ‡∏≤‡∏á evaluate.py

```bash
cat > src/evaluate.py << 'EOF'
"""
Model Evaluation Module
=======================
‡πÇ‡∏°‡∏î‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏• ML models
"""

import os
import json
import joblib
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime

from sklearn.metrics import (
    accuracy_score,
    precision_score,
    recall_score,
    f1_score,
    confusion_matrix,
    classification_report
)

from data_loader import load_iris_data, split_data


def load_model(model_path):
    """
    ‡πÇ‡∏´‡∏•‡∏î model ‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå
    
    Args:
        model_path: path ‡πÑ‡∏õ‡∏¢‡∏±‡∏á‡πÑ‡∏ü‡∏•‡πå model
    
    Returns:
        dict: model data (model, scaler, config, timestamp)
    """
    model_data = joblib.load(model_path)
    print(f"‚úÖ Loaded model from {model_path}")
    print(f"   - Model type: {type(model_data['model']).__name__}")
    print(f"   - Trained at: {model_data['timestamp']}")
    return model_data


def evaluate_model(model, X_test, y_test, target_names):
    """
    ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏• model
    
    Args:
        model: trained model
        X_test: test features
        y_test: test labels
        target_names: ‡∏ä‡∏∑‡πà‡∏≠ classes
    
    Returns:
        dict: evaluation metrics
    """
    # ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ú‡∏•
    y_pred = model.predict(X_test)
    
    # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì metrics
    metrics = {
        'accuracy': float(accuracy_score(y_test, y_pred)),
        'precision_macro': float(precision_score(y_test, y_pred, average='macro')),
        'recall_macro': float(recall_score(y_test, y_pred, average='macro')),
        'f1_macro': float(f1_score(y_test, y_pred, average='macro')),
        'confusion_matrix': confusion_matrix(y_test, y_pred).tolist(),
        'classification_report': classification_report(
            y_test, y_pred, target_names=target_names, output_dict=True
        )
    }
    
    return metrics, y_pred


def plot_confusion_matrix(cm, target_names, output_path):
    """
    ‡∏™‡∏£‡πâ‡∏≤‡∏á confusion matrix plot
    
    Args:
        cm: confusion matrix
        target_names: ‡∏ä‡∏∑‡πà‡∏≠ classes
        output_path: path ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏£‡∏π‡∏õ
    """
    plt.figure(figsize=(8, 6))
    sns.heatmap(
        cm, 
        annot=True, 
        fmt='d', 
        cmap='Blues',
        xticklabels=target_names,
        yticklabels=target_names
    )
    plt.title('Confusion Matrix')
    plt.ylabel('True Label')
    plt.xlabel('Predicted Label')
    plt.tight_layout()
    
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    plt.savefig(output_path, dpi=150)
    plt.close()
    print(f"‚úÖ Confusion matrix saved to {output_path}")


def plot_feature_importance(model, feature_names, output_path):
    """
    ‡∏™‡∏£‡πâ‡∏≤‡∏á feature importance plot (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö tree-based models)
    
    Args:
        model: trained model
        feature_names: ‡∏ä‡∏∑‡πà‡∏≠ features
        output_path: path ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏£‡∏π‡∏õ
    """
    if not hasattr(model, 'feature_importances_'):
        print("‚ö†Ô∏è Model doesn't have feature_importances_")
        return
    
    importances = model.feature_importances_
    indices = np.argsort(importances)[::-1]
    
    plt.figure(figsize=(10, 6))
    plt.title('Feature Importances')
    plt.bar(range(len(importances)), importances[indices])
    plt.xticks(range(len(importances)), [feature_names[i] for i in indices], rotation=45, ha='right')
    plt.xlabel('Feature')
    plt.ylabel('Importance')
    plt.tight_layout()
    
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    plt.savefig(output_path, dpi=150)
    plt.close()
    print(f"‚úÖ Feature importance plot saved to {output_path}")


def print_evaluation_report(metrics, target_names):
    """
    ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô
    
    Args:
        metrics: dict ‡∏Ç‡∏≠‡∏á metrics
        target_names: ‡∏ä‡∏∑‡πà‡∏≠ classes
    """
    print("\n" + "=" * 60)
    print("üìä Evaluation Report")
    print("=" * 60)
    
    print(f"\nüéØ Overall Metrics:")
    print(f"   - Accuracy:  {metrics['accuracy']:.4f}")
    print(f"   - Precision: {metrics['precision_macro']:.4f}")
    print(f"   - Recall:    {metrics['recall_macro']:.4f}")
    print(f"   - F1-Score:  {metrics['f1_macro']:.4f}")
    
    print(f"\nüìã Per-Class Metrics:")
    report = metrics['classification_report']
    for class_name in target_names:
        class_metrics = report[class_name]
        print(f"   {class_name}:")
        print(f"      - Precision: {class_metrics['precision']:.4f}")
        print(f"      - Recall:    {class_metrics['recall']:.4f}")
        print(f"      - F1-Score:  {class_metrics['f1-score']:.4f}")
    
    print(f"\nüìà Confusion Matrix:")
    cm = np.array(metrics['confusion_matrix'])
    print(f"   {cm}")


def save_evaluation_results(metrics, output_path):
    """
    ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô
    
    Args:
        metrics: dict ‡∏Ç‡∏≠‡∏á metrics
        output_path: path ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å
    """
    results = {
        **metrics,
        'timestamp': datetime.now().isoformat()
    }
    
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    with open(output_path, 'w') as f:
        json.dump(results, f, indent=2)
    print(f"‚úÖ Evaluation results saved to {output_path}")


def main():
    """
    Main evaluation pipeline
    """
    print("=" * 60)
    print("üìä ML Evaluation Pipeline")
    print("=" * 60)
    
    # 1. Load model
    print("\nüìÅ Step 1: Loading model...")
    model_path = '../models/model.joblib'
    model_data = load_model(model_path)
    model = model_data['model']
    scaler = model_data['scaler']
    config = model_data['config']
    
    # 2. Load data
    print("\nüìä Step 2: Loading data...")
    X, y, feature_names, target_names = load_iris_data()
    
    # 3. Split data (‡πÉ‡∏ä‡πâ random_state ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏ï‡∏≠‡∏ô train)
    print("\n‚úÇÔ∏è Step 3: Splitting data...")
    test_size = config['data']['test_size']
    random_state = config['data']['random_state']
    X_train, X_test, y_train, y_test = split_data(
        X, y, test_size=test_size, random_state=random_state
    )
    
    # 4. Scale data
    print("\nüîß Step 4: Scaling test data...")
    X_test_scaled = scaler.transform(X_test)
    
    # 5. Evaluate
    print("\nüìà Step 5: Evaluating model...")
    metrics, y_pred = evaluate_model(model, X_test_scaled, y_test, target_names)
    
    # 6. Print report
    print_evaluation_report(metrics, target_names)
    
    # 7. Create plots
    print("\nüé® Step 6: Creating visualizations...")
    cm = np.array(metrics['confusion_matrix'])
    plot_confusion_matrix(cm, target_names, '../results/confusion_matrix.png')
    plot_feature_importance(model, feature_names, '../results/feature_importance.png')
    
    # 8. Save results
    print("\nüíæ Step 7: Saving results...")
    save_evaluation_results(metrics, '../results/evaluation_metrics.json')
    
    print("\n" + "=" * 60)
    print("‚úÖ Evaluation pipeline completed!")
    print("=" * 60)
    
    return metrics


if __name__ == "__main__":
    metrics = main()
EOF
```

---

### Step 22: ‡∏ó‡∏î‡∏™‡∏≠‡∏ö evaluate.py

```bash
cd src
python evaluate.py
cd ..
```

**‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå:**
```
============================================================
üìä ML Evaluation Pipeline
============================================================

üìÅ Step 1: Loading model...
‚úÖ Loaded model from ../models/model.joblib
   - Model type: RandomForestClassifier
   - Trained at: 2024-12-16T10:30:00

üìä Step 2: Loading data...
‚úÖ Loaded Iris dataset
   - Samples: 150
   - Features: 4
   - Classes: 3

‚úÇÔ∏è Step 3: Splitting data...
‚úÖ Data split completed
   - Train samples: 120
   - Test samples: 30

üîß Step 4: Scaling test data...

üìà Step 5: Evaluating model...

============================================================
üìä Evaluation Report
============================================================

üéØ Overall Metrics:
   - Accuracy:  0.9667
   - Precision: 0.9722
   - Recall:    0.9667
   - F1-Score:  0.9665

üìã Per-Class Metrics:
   setosa:
      - Precision: 1.0000
      - Recall:    1.0000
      - F1-Score:  1.0000
   versicolor:
      - Precision: 0.9167
      - Recall:    1.0000
      - F1-Score:  0.9565
   virginica:
      - Precision: 1.0000
      - Recall:    0.9000
      - F1-Score:  0.9474

üìà Confusion Matrix:
   [[10  0  0]
    [ 0 10  0]
    [ 0  1  9]]

üé® Step 6: Creating visualizations...
‚úÖ Confusion matrix saved to ../results/confusion_matrix.png
‚úÖ Feature importance plot saved to ../results/feature_importance.png

üíæ Step 7: Saving results...
‚úÖ Evaluation results saved to ../results/evaluation_metrics.json

============================================================
‚úÖ Evaluation pipeline completed!
============================================================
```

---

### Step 23: Commit Evaluation Module

```bash
git add src/evaluate.py
git commit -m "Add evaluation module with metrics and visualizations"
```

**‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå:**
```
[main e5f6g7h] Add evaluation module with metrics and visualizations
 1 file changed, 218 insertions(+)
 create mode 100644 src/evaluate.py
```

---

## üåê Part 6: ‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏Å‡∏±‡∏ö Remote Repository

### Step 24: ‡∏î‡∏π Commit History ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î

```bash
git log --oneline --graph --all
```

**‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå:**
```
* e5f6g7h (HEAD -> main) Add evaluation module with metrics and visualizations
* d4e5f6g Add training pipeline with config and model saving
* c3d4e5f Add feature_engineer module with scaling and transformations
* b2c3d4e Add data_loader module with Iris dataset support
* a1b2c3d Initial commit: ML project structure with .gitignore
```

---

### Step 25: ‡πÄ‡∏û‡∏¥‡πà‡∏° Remote Repository

> ‚ö†Ô∏è **‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏:** ‡πÉ‡∏´‡πâ‡∏ô‡∏±‡∏Å‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏™‡∏£‡πâ‡∏≤‡∏á repository ‡∏ö‡∏ô GitHub ‡∏Å‡πà‡∏≠‡∏ô

**‡∏ß‡∏¥‡∏ò‡∏µ‡∏™‡∏£‡πâ‡∏≤‡∏á Repository ‡∏ö‡∏ô GitHub:**
1. ‡πÑ‡∏õ‡∏ó‡∏µ‡πà https://github.com
2. ‡∏Ñ‡∏•‡∏¥‡∏Å "New repository"
3. ‡∏ï‡∏±‡πâ‡∏á‡∏ä‡∏∑‡πà‡∏≠ `ml-git-lab03_advance`
4. **‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á** ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å "Add a README file"
5. ‡∏Ñ‡∏•‡∏¥‡∏Å "Create repository"

```bash
git remote add origin https://github.com/YOUR_USERNAME/ml-git-lab03_advance.git
```

**‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö:**
```bash
git remote -v
```

**‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå:**
```
origin  https://github.com/YOUR_USERNAME/ml-git-lab03_advance.git (fetch)
origin  https://github.com/YOUR_USERNAME/ml-git-lab03_advance.git (push)
```

---

### Step 26: Push ‡πÑ‡∏õ‡∏¢‡∏±‡∏á Remote

```bash
git push -u origin main
```

**‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå:**
```
Enumerating objects: 22, done.
Counting objects: 100% (22/22), done.
Delta compression using up to 8 threads
Compressing objects: 100% (18/18), done.
Writing objects: 100% (22/22), 7.15 KiB | 1.43 MiB/s, done.
Total 22 (delta 3), reused 0 (delta 0), pack-reused 0
To https://github.com/YOUR_USERNAME/ml-git-lab03_advance.git
 * [new branch]      main -> main
branch 'main' set up to track 'origin/main'.
```

---

### Step 27: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Remote Branches

```bash
git branch -a
```

**‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå:**
```
* main
  remotes/origin/main
```

---

## üîÑ Part 7: ‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏•‡∏≠‡∏á‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô Model Configuration

### Step 28: ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á - ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏õ‡πá‡∏ô SVM

```bash
cat > config/model_config.yaml << 'EOF'
# Model Configuration
# ===================
# Updated: Changed to SVM model

# Data settings
data:
  test_size: 0.2
  random_state: 42

# Feature engineering
features:
  scaling_method: standard
  create_interactions: false
  polynomial_degree: 1

# Model settings - CHANGED TO SVM
model:
  type: svm  # Changed from random_forest
  
  # Random Forest parameters
  random_forest:
    n_estimators: 100
    max_depth: 5
    min_samples_split: 2
    min_samples_leaf: 1
    random_state: 42
  
  # Logistic Regression parameters
  logistic_regression:
    C: 1.0
    max_iter: 1000
    random_state: 42
  
  # SVM parameters - USING THESE NOW
  svm:
    C: 1.0
    kernel: rbf
    gamma: scale
    random_state: 42

# Training settings
training:
  cross_validation: true
  cv_folds: 5
  verbose: true

# Output settings
output:
  save_model: true
  model_path: models/model.joblib
  save_results: true
  results_path: results/metrics.json
EOF
```

---

### Step 29: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏î‡πâ‡∏ß‡∏¢ git diff

```bash
git diff config/model_config.yaml
```

**‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå:**
```diff
diff --git a/config/model_config.yaml b/config/model_config.yaml
index abc1234..def5678 100644
--- a/config/model_config.yaml
+++ b/config/model_config.yaml
@@ -1,5 +1,6 @@
 # Model Configuration
 # ===================
+# Updated: Changed to SVM model
 
 # Data settings
 data:
@@ -12,8 +13,8 @@ features:
   create_interactions: false
   polynomial_degree: 1
 
-# Model settings
-model:
-  type: random_forest
+# Model settings - CHANGED TO SVM
+model:
+  type: svm  # Changed from random_forest
```

---

### Step 30: Train ‡πÅ‡∏•‡∏∞ Evaluate ‡πÉ‡∏´‡∏°‡πà‡∏î‡πâ‡∏ß‡∏¢ SVM

```bash
cd src
python train.py
python evaluate.py
cd ..
```

**‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå (‡∏ö‡∏≤‡∏á‡∏™‡πà‡∏ß‡∏ô):**
```
ü§ñ Step 5: Creating model...
‚úÖ Created svm model

üéØ Step 6: Training model...
‚úÖ Cross-validation (5 folds):
   - Scores: [0.9583 0.9583 1.0000 0.9583 0.9167]
   - Mean: 0.9583 (+/- 0.0527)
...
   - Test accuracy: 0.9667
```

---

### Step 31: Commit ‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á config

```bash
git add config/model_config.yaml
git commit -m "Experiment: Switch to SVM model for comparison"
```

**‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå:**
```
[main f6g7h8i] Experiment: Switch to SVM model for comparison
 1 file changed, 5 insertions(+), 3 deletions(-)
```

---

### Step 32: Push ‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡∏∂‡πâ‡∏ô Remote

```bash
git push
```

**‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå:**
```
Enumerating objects: 7, done.
Counting objects: 100% (7/7), done.
Delta compression using up to 8 threads
Compressing objects: 100% (3/3), done.
Writing objects: 100% (4/4), 456 bytes | 456.00 KiB/s, done.
Total 4 (delta 2), reused 0 (delta 0), pack-reused 0
To https://github.com/YOUR_USERNAME/ml-git-lab03_advance.git
   e5f6g7h..f6g7h8i  main -> main
```

---

## üîç Part 8: ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ git checkout ‡∏Å‡∏±‡∏ö ML Project

### Step 33: ‡∏î‡∏π‡πÑ‡∏ü‡∏•‡πå config ‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤

```bash
git log --oneline config/model_config.yaml
```

**‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå:**
```
f6g7h8i (HEAD -> main, origin/main) Experiment: Switch to SVM model for comparison
d4e5f6g Add training pipeline with config and model saving
```

**‡∏î‡∏π‡πÑ‡∏ü‡∏•‡πå config ‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô Random Forest:**
```bash
git show d4e5f6g:config/model_config.yaml | head -20
```

**‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå:**
```yaml
# Model Configuration
# ===================

# Data settings
data:
  test_size: 0.2
  random_state: 42

# Feature engineering
features:
  scaling_method: standard
  create_interactions: false
  polynomial_degree: 1

# Model settings
model:
  type: random_forest  # <-- ‡πÄ‡∏´‡πá‡∏ô‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô random_forest
```

---

### Step 34: ‡∏Å‡∏π‡πâ‡∏Ñ‡∏∑‡∏ô config ‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô Random Forest (‡∏ñ‡πâ‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£)

```bash
# ‡∏î‡∏∂‡∏á‡πÑ‡∏ü‡∏•‡πå config ‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô‡πÄ‡∏Å‡πà‡∏≤‡∏°‡∏≤‡∏î‡∏π
git checkout d4e5f6g -- config/model_config.yaml

# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö
cat config/model_config.yaml | grep "type:"
```

**‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå:**
```
  type: random_forest
```

**‡∏¢‡∏Å‡πÄ‡∏•‡∏¥‡∏Å‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á (‡∏Å‡∏•‡∏±‡∏ö‡πÑ‡∏õ‡πÄ‡∏õ‡πá‡∏ô SVM):**
```bash
git restore config/model_config.yaml
```

---

## üìä Part 9: ‡∏™‡∏£‡∏∏‡∏õ‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ

### ‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á Git ‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô

| ‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á | ‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢ |
|--------|----------|
| `git init` | ‡∏™‡∏£‡πâ‡∏≤‡∏á repository ‡πÉ‡∏´‡∏°‡πà |
| `git add <file>` | ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏Ç‡πâ‡∏≤ staging |
| `git add .` | ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ó‡∏∏‡∏Å‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏Ç‡πâ‡∏≤ staging |
| `git commit -m "msg"` | ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á |
| `git status` | ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞ |
| `git log --oneline` | ‡∏î‡∏π‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥ commit |
| `git diff <file>` | ‡∏î‡∏π‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á |

### ‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á Remote

| ‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á | ‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢ |
|--------|----------|
| `git remote add origin <url>` | ‡πÄ‡∏û‡∏¥‡πà‡∏° remote |
| `git push -u origin main` | push ‡πÅ‡∏•‡∏∞‡∏ï‡∏±‡πâ‡∏á upstream |
| `git push` | push (‡∏´‡∏•‡∏±‡∏á‡∏ï‡∏±‡πâ‡∏á upstream) |
| `git fetch origin` | ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å remote |
| `git pull origin main` | fetch + merge |

### ‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö ML Workflow

| ‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á | ‡πÉ‡∏ä‡πâ‡∏ó‡∏≥‡∏≠‡∏∞‡πÑ‡∏£ |
|--------|----------|
| `git status --ignored` | ‡∏î‡∏π‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å ignore (models, data) |
| `git show <commit>:<file>` | ‡∏î‡∏π‡πÑ‡∏ü‡∏•‡πå config ‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô‡πÄ‡∏Å‡πà‡∏≤ |
| `git checkout <commit> -- <file>` | ‡∏Å‡∏π‡πâ‡∏Ñ‡∏∑‡∏ô‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô‡πÄ‡∏Å‡πà‡∏≤ |
| `git log --oneline <file>` | ‡∏î‡∏π‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡πÑ‡∏ü‡∏•‡πå |

---


---

## üßπ ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î (Optional)

```bash
# ‡∏Å‡∏•‡∏±‡∏ö‡πÑ‡∏õ‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤
cd ..

# ‡∏•‡∏ö‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Ñ (‡∏ñ‡πâ‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£)
rm -rf ml-git-lab03_advance
```

---

## üìö ‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°

- [Git Documentation](https://git-scm.com/doc)
- [Scikit-learn Documentation](https://scikit-learn.org/stable/)
- [MLOps Principles](https://ml-ops.org/)
- [DVC - Data Version Control](https://dvc.org/) (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ data ‡πÅ‡∏•‡∏∞ model files ‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà)