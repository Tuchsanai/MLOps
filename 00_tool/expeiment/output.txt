#%% [markdown]
# üî¨ LAB: Model Monitoring with Scikit-Learn
# ‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°‡πÅ‡∏•‡∏∞‡πÄ‡∏ù‡πâ‡∏≤‡∏£‡∏∞‡∏ß‡∏±‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏• Machine Learning

#%% [markdown]
## üìö ‡∏ö‡∏ó‡∏ô‡∏≥ (Introduction)
# 
# ### Model Monitoring ‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£?
# 
# **Model Monitoring** ‡∏Ñ‡∏∑‡∏≠‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°‡πÅ‡∏•‡∏∞‡πÄ‡∏ù‡πâ‡∏≤‡∏£‡∏∞‡∏ß‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡∏Ç‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏• Machine Learning 
# ‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ deploy ‡πÑ‡∏õ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏à‡∏£‡∏¥‡∏á (Production) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à‡∏ß‡πà‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏¢‡∏±‡∏á‡∏Ñ‡∏á‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
# 
# ### ‡∏ó‡∏≥‡πÑ‡∏°‡∏ï‡πâ‡∏≠‡∏á Monitor ‡πÇ‡∏°‡πÄ‡∏î‡∏•?
# 
# 1. **Data Drift** - ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô‡πÇ‡∏•‡∏Å‡∏à‡∏£‡∏¥‡∏á‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏ï‡∏•‡∏≠‡∏î‡πÄ‡∏ß‡∏•‡∏≤
# 2. **Concept Drift** - ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á features ‡πÅ‡∏•‡∏∞ target ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÑ‡∏õ
# 3. **Model Degradation** - ‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏•‡∏î‡∏•‡∏á‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÄ‡∏ß‡∏•‡∏≤‡∏ú‡πà‡∏≤‡∏ô‡πÑ‡∏õ
# 4. **Data Quality Issues** - ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ‡πÄ‡∏ä‡πà‡∏ô missing values, outliers
# 
# ### ‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÉ‡∏ô LAB ‡∏ô‡∏µ‡πâ
# 
# - ‚úÖ Data Quality Monitoring
# - ‚úÖ Model Performance Tracking
# - ‚úÖ Target Drift Detection
# - ‚úÖ Building Monitoring Dashboard

#%% [markdown]
# ---
# ## üõ†Ô∏è Section 0: Environment Setup
# ### ‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏•‡∏∞‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏™‡∏†‡∏≤‡∏û‡πÅ‡∏ß‡∏î‡∏•‡πâ‡∏≠‡∏°

#%%
# ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á libraries ‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô (‡∏£‡∏±‡∏ô‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß)
# !pip install scikit-learn pandas numpy matplotlib seaborn scipy

#%%
# Import libraries ‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')

# Scikit-learn imports
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from sklearn.linear_model import LogisticRegression, LinearRegression
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    confusion_matrix, classification_report,
    mean_absolute_error, mean_squared_error, r2_score
)

# Statistical tests
from scipy import stats
from scipy.stats import ks_2samp, chi2_contingency, wasserstein_distance

# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ display
pd.set_option('display.max_columns', None)
pd.set_option('display.width', None)
plt.style.use('seaborn-v0_8-whitegrid')
plt.rcParams['figure.figsize'] = [12, 6]
plt.rcParams['font.size'] = 10

print("‚úÖ Import libraries ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!")
print(f"üìÖ ‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏ô: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

#%% [markdown]
# ---
# ## üìä Section 1: Data Quality Monitoring
# ### ‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
# 
# **‡∏ó‡∏§‡∏©‡∏é‡∏µ:**
# 
# Data Quality Monitoring ‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡πÅ‡∏£‡∏Å‡πÅ‡∏•‡∏∞‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£ monitor ‡πÇ‡∏°‡πÄ‡∏î‡∏•
# ‡πÄ‡∏û‡∏£‡∏≤‡∏∞ "Garbage In, Garbage Out" - ‡∏ñ‡πâ‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏°‡πà‡∏î‡∏µ ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏Å‡πá‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÑ‡∏î‡πâ‡∏î‡∏µ
# 
# **‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö:**
# 1. Missing Values - ‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏´‡∏≤‡∏¢‡πÑ‡∏õ
# 2. Duplicates - ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ã‡πâ‡∏≥
# 3. Data Types - ‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
# 4. Value Ranges - ‡∏ä‡πà‡∏ß‡∏á‡∏Ñ‡πà‡∏≤‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
# 5. Outliers - ‡∏Ñ‡πà‡∏≤‡∏ú‡∏¥‡∏î‡∏õ‡∏Å‡∏ï‡∏¥

#%% [markdown]
# ### 1.1 ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á (Synthetic Data)
# 
# ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≥‡∏•‡∏≠‡∏á‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏õ‡∏±‡∏ç‡∏´‡∏≤ Credit Risk Classification
# ‡πÇ‡∏î‡∏¢‡∏°‡∏µ 2 ‡∏ä‡∏∏‡∏î:
# - **Reference Data** - ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ train ‡πÇ‡∏°‡πÄ‡∏î‡∏• (‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô‡∏≠‡∏î‡∏µ‡∏ï)
# - **Current Data** - ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡∏°‡πà‡∏ó‡∏µ‡πà‡πÄ‡∏Ç‡πâ‡∏≤‡∏°‡∏≤ (‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô)

#%%
def create_credit_data(n_samples=1000, seed=42, drift_level=0.0):
    """
    ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Credit Risk ‡∏à‡∏≥‡∏•‡∏≠‡∏á
    
    Parameters:
    -----------
    n_samples : int - ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á
    seed : int - random seed
    drift_level : float - ‡∏£‡∏∞‡∏î‡∏±‡∏ö drift (0.0 = ‡πÑ‡∏°‡πà‡∏°‡∏µ drift, 1.0 = drift ‡∏°‡∏≤‡∏Å)
    
    Returns:
    --------
    DataFrame - ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• credit risk
    """
    np.random.seed(seed)
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á features
    data = {
        'customer_id': range(1, n_samples + 1),
        'age': np.random.normal(35 + drift_level * 5, 10, n_samples).astype(int),
        'income': np.random.normal(50000 + drift_level * 10000, 15000, n_samples),
        'loan_amount': np.random.normal(20000 + drift_level * 5000, 8000, n_samples),
        'credit_score': np.random.normal(650 - drift_level * 30, 80, n_samples).astype(int),
        'employment_years': np.random.exponential(5 + drift_level, n_samples),
        'num_credit_cards': np.random.poisson(3, n_samples),
        'debt_to_income': np.random.uniform(0.1 + drift_level * 0.1, 0.6 + drift_level * 0.1, n_samples),
        'previous_defaults': np.random.binomial(3, 0.1 + drift_level * 0.05, n_samples),
        'education': np.random.choice(['High School', 'Bachelor', 'Master', 'PhD'], 
                                       n_samples, p=[0.3, 0.4, 0.2, 0.1]),
        'employment_type': np.random.choice(['Full-time', 'Part-time', 'Self-employed', 'Unemployed'],
                                            n_samples, p=[0.6, 0.15, 0.2, 0.05])
    }
    
    df = pd.DataFrame(data)
    
    # ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏Ñ‡πà‡∏≤‡πÉ‡∏´‡πâ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏ä‡πà‡∏ß‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°
    df['age'] = df['age'].clip(18, 80)
    df['credit_score'] = df['credit_score'].clip(300, 850)
    df['income'] = df['income'].clip(10000, 200000)
    df['loan_amount'] = df['loan_amount'].clip(1000, 100000)
    df['employment_years'] = df['employment_years'].clip(0, 40)
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á target (default: 1 = ‡∏ú‡∏¥‡∏î‡∏ô‡∏±‡∏î‡∏ä‡∏≥‡∏£‡∏∞, 0 = ‡πÑ‡∏°‡πà‡∏ú‡∏¥‡∏î‡∏ô‡∏±‡∏î)
    default_prob = (
        0.1 +
        0.02 * (df['debt_to_income'] - 0.3) * 10 +
        0.01 * df['previous_defaults'] +
        -0.001 * (df['credit_score'] - 600) +
        -0.0001 * (df['income'] - 40000) +
        drift_level * 0.1
    )
    default_prob = default_prob.clip(0.01, 0.99)
    df['default'] = (np.random.random(n_samples) < default_prob).astype(int)
    
    return df

# ‡∏™‡∏£‡πâ‡∏≤‡∏á Reference Data (‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô‡∏≠‡∏î‡∏µ‡∏ï - ‡πÑ‡∏°‡πà‡∏°‡∏µ drift)
print("üìä ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á...")
reference_data = create_credit_data(n_samples=2000, seed=42, drift_level=0.0)
print(f"‚úÖ Reference Data: {len(reference_data)} rows")

# ‡∏™‡∏£‡πâ‡∏≤‡∏á Current Data (‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô - ‡∏°‡∏µ drift ‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢)
current_data = create_credit_data(n_samples=1000, seed=123, drift_level=0.3)
print(f"‚úÖ Current Data: {len(current_data)} rows")

# ‡πÅ‡∏™‡∏î‡∏á‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
print("\nüìã ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á Reference Data:")
reference_data.head()

#%%
# ‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô
print("=" * 60)
print("üìä REFERENCE DATA INFO")
print("=" * 60)
print(f"Shape: {reference_data.shape}")
print(f"\nData Types:\n{reference_data.dtypes}")
print(f"\nTarget Distribution:\n{reference_data['default'].value_counts(normalize=True)}")

#%% [markdown]
# ### 1.2 ‡∏™‡∏£‡πâ‡∏≤‡∏á Data Quality Report Class
# 
# ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á class ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô

#%%
class DataQualityMonitor:
    """
    Class ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
    
    ‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö:
    - ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö missing values
    - ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö duplicates
    - ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå data integrity
    - ‡∏™‡∏£‡πâ‡∏≤‡∏á quality report
    """
    
    def __init__(self, data, data_name="Data"):
        """
        Parameters:
        -----------
        data : DataFrame - ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö
        data_name : str - ‡∏ä‡∏∑‡πà‡∏≠‡∏Ç‡∏≠‡∏á‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        """
        self.data = data.copy()
        self.data_name = data_name
        self.report = {}
        
    def check_missing_values(self):
        """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö missing values"""
        missing = self.data.isnull().sum()
        missing_pct = (missing / len(self.data) * 100).round(2)
        
        missing_df = pd.DataFrame({
            'column': missing.index,
            'missing_count': missing.values,
            'missing_percentage': missing_pct.values
        })
        missing_df = missing_df[missing_df['missing_count'] > 0].sort_values(
            'missing_percentage', ascending=False
        )
        
        self.report['missing_values'] = {
            'total_missing': int(missing.sum()),
            'columns_with_missing': len(missing_df),
            'details': missing_df.to_dict('records')
        }
        
        return missing_df
    
    def check_duplicates(self):
        """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ã‡πâ‡∏≥"""
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö duplicate rows ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
        duplicate_rows = self.data.duplicated().sum()
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö duplicate ‡∏ï‡∏≤‡∏° ID (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
        id_columns = [col for col in self.data.columns if 'id' in col.lower()]
        duplicate_ids = {}
        
        for id_col in id_columns:
            dup_count = self.data[id_col].duplicated().sum()
            if dup_count > 0:
                duplicate_ids[id_col] = int(dup_count)
        
        self.report['duplicates'] = {
            'duplicate_rows': int(duplicate_rows),
            'duplicate_percentage': round(duplicate_rows / len(self.data) * 100, 2),
            'duplicate_ids': duplicate_ids
        }
        
        return self.report['duplicates']
    
    def check_data_types(self):
        """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•"""
        dtypes_df = pd.DataFrame({
            'column': self.data.columns,
            'dtype': self.data.dtypes.values,
            'unique_values': [self.data[col].nunique() for col in self.data.columns],
            'sample_values': [str(self.data[col].dropna().head(3).tolist()) for col in self.data.columns]
        })
        
        # ‡∏à‡∏≥‡πÅ‡∏ô‡∏Å‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó columns
        numeric_cols = self.data.select_dtypes(include=[np.number]).columns.tolist()
        categorical_cols = self.data.select_dtypes(include=['object', 'category']).columns.tolist()
        
        self.report['data_types'] = {
            'numeric_columns': numeric_cols,
            'categorical_columns': categorical_cols,
            'total_columns': len(self.data.columns)
        }
        
        return dtypes_df
    
    def check_value_ranges(self):
        """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ä‡πà‡∏ß‡∏á‡∏Ñ‡πà‡∏≤‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• numeric"""
        numeric_cols = self.data.select_dtypes(include=[np.number]).columns
        
        range_stats = []
        for col in numeric_cols:
            stats_dict = {
                'column': col,
                'min': self.data[col].min(),
                'max': self.data[col].max(),
                'mean': round(self.data[col].mean(), 2),
                'median': round(self.data[col].median(), 2),
                'std': round(self.data[col].std(), 2),
                'q1': round(self.data[col].quantile(0.25), 2),
                'q3': round(self.data[col].quantile(0.75), 2)
            }
            range_stats.append(stats_dict)
        
        self.report['value_ranges'] = range_stats
        return pd.DataFrame(range_stats)
    
    def detect_outliers(self, method='iqr', threshold=1.5):
        """
        ‡∏ï‡∏£‡∏ß‡∏à‡∏´‡∏≤ outliers ‡∏î‡πâ‡∏ß‡∏¢‡∏ß‡∏¥‡∏ò‡∏µ IQR
        
        Parameters:
        -----------
        method : str - ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏´‡∏≤ ('iqr' ‡∏´‡∏£‡∏∑‡∏≠ 'zscore')
        threshold : float - ‡∏Ñ‡πà‡∏≤ threshold (1.5 ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö IQR, 3 ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö z-score)
        """
        numeric_cols = self.data.select_dtypes(include=[np.number]).columns
        outlier_info = []
        
        for col in numeric_cols:
            if method == 'iqr':
                Q1 = self.data[col].quantile(0.25)
                Q3 = self.data[col].quantile(0.75)
                IQR = Q3 - Q1
                lower_bound = Q1 - threshold * IQR
                upper_bound = Q3 + threshold * IQR
                outliers = self.data[(self.data[col] < lower_bound) | (self.data[col] > upper_bound)]
            else:  # z-score
                z_scores = np.abs(stats.zscore(self.data[col].dropna()))
                outliers = self.data[z_scores > threshold]
            
            outlier_count = len(outliers)
            outlier_pct = round(outlier_count / len(self.data) * 100, 2)
            
            outlier_info.append({
                'column': col,
                'outlier_count': outlier_count,
                'outlier_percentage': outlier_pct,
                'method': method,
                'threshold': threshold
            })
        
        self.report['outliers'] = outlier_info
        return pd.DataFrame(outlier_info)
    
    def generate_full_report(self):
        """‡∏™‡∏£‡πâ‡∏≤‡∏á report ‡∏â‡∏ö‡∏±‡∏ö‡πÄ‡∏ï‡πá‡∏°"""
        print("=" * 70)
        print(f"üìä DATA QUALITY REPORT: {self.data_name}")
        print("=" * 70)
        print(f"üìÖ Generated at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"üìÅ Total Records: {len(self.data):,}")
        print(f"üìã Total Columns: {len(self.data.columns)}")
        print()
        
        # 1. Missing Values
        print("-" * 50)
        print("1Ô∏è‚É£ MISSING VALUES CHECK")
        print("-" * 50)
        missing_df = self.check_missing_values()
        if len(missing_df) > 0:
            print(f"‚ö†Ô∏è ‡∏û‡∏ö {self.report['missing_values']['total_missing']} missing values")
            print(f"   ‡πÉ‡∏ô {self.report['missing_values']['columns_with_missing']} columns")
            print(missing_df.to_string(index=False))
        else:
            print("‚úÖ ‡πÑ‡∏°‡πà‡∏û‡∏ö missing values")
        print()
        
        # 2. Duplicates
        print("-" * 50)
        print("2Ô∏è‚É£ DUPLICATES CHECK")
        print("-" * 50)
        dup_report = self.check_duplicates()
        if dup_report['duplicate_rows'] > 0:
            print(f"‚ö†Ô∏è ‡∏û‡∏ö {dup_report['duplicate_rows']} duplicate rows ({dup_report['duplicate_percentage']}%)")
        else:
            print("‚úÖ ‡πÑ‡∏°‡πà‡∏û‡∏ö duplicate rows")
        
        if dup_report['duplicate_ids']:
            print(f"‚ö†Ô∏è Duplicate IDs: {dup_report['duplicate_ids']}")
        print()
        
        # 3. Data Types
        print("-" * 50)
        print("3Ô∏è‚É£ DATA TYPES SUMMARY")
        print("-" * 50)
        dtypes_df = self.check_data_types()
        print(f"üìä Numeric Columns: {len(self.report['data_types']['numeric_columns'])}")
        print(f"üìù Categorical Columns: {len(self.report['data_types']['categorical_columns'])}")
        print()
        
        # 4. Value Ranges
        print("-" * 50)
        print("4Ô∏è‚É£ VALUE RANGES (Numeric Columns)")
        print("-" * 50)
        ranges_df = self.check_value_ranges()
        print(ranges_df.to_string(index=False))
        print()
        
        # 5. Outliers
        print("-" * 50)
        print("5Ô∏è‚É£ OUTLIERS DETECTION (IQR Method)")
        print("-" * 50)
        outliers_df = self.detect_outliers()
        outliers_with_issues = outliers_df[outliers_df['outlier_count'] > 0]
        if len(outliers_with_issues) > 0:
            print(f"‚ö†Ô∏è ‡∏û‡∏ö outliers ‡πÉ‡∏ô {len(outliers_with_issues)} columns:")
            print(outliers_with_issues.to_string(index=False))
        else:
            print("‚úÖ ‡πÑ‡∏°‡πà‡∏û‡∏ö outliers ‡∏ó‡∏µ‡πà‡∏ú‡∏¥‡∏î‡∏õ‡∏Å‡∏ï‡∏¥")
        
        print()
        print("=" * 70)
        print("üìä END OF DATA QUALITY REPORT")
        print("=" * 70)
        
        return self.report

#%%
# ‡∏ó‡∏î‡∏™‡∏≠‡∏ö Data Quality Monitor ‡∏Å‡∏±‡∏ö Reference Data
print("üîç ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û Reference Data")
print()

ref_monitor = DataQualityMonitor(reference_data, "Reference Data")
ref_report = ref_monitor.generate_full_report()

#%%
# ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏±‡∏ö Current Data
print("\nüîç ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û Current Data")
print()

curr_monitor = DataQualityMonitor(current_data, "Current Data")
curr_report = curr_monitor.generate_full_report()

#%% [markdown]
# ### 1.3 ‡∏à‡∏≥‡∏•‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û
# 
# ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏£‡∏∞‡∏ö‡∏ö monitoring ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û

#%%
def introduce_data_quality_issues(data, missing_rate=0.05, duplicate_rate=0.02):
    """
    ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏£‡∏∞‡∏ö‡∏ö
    
    Parameters:
    -----------
    data : DataFrame - ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö
    missing_rate : float - ‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏™‡πà‡∏ß‡∏ô missing values
    duplicate_rate : float - ‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏™‡πà‡∏ß‡∏ô duplicates
    """
    df = data.copy()
    n_rows = len(df)
    
    # ‡πÄ‡∏û‡∏¥‡πà‡∏° missing values
    missing_cols = ['income', 'credit_score', 'employment_years']
    for col in missing_cols:
        missing_idx = np.random.choice(n_rows, int(n_rows * missing_rate), replace=False)
        df.loc[missing_idx, col] = np.nan
    
    # ‡πÄ‡∏û‡∏¥‡πà‡∏° duplicates
    n_duplicates = int(n_rows * duplicate_rate)
    duplicate_rows = df.sample(n=n_duplicates, random_state=42)
    df = pd.concat([df, duplicate_rows], ignore_index=True)
    
    # ‡πÄ‡∏û‡∏¥‡πà‡∏° outliers
    outlier_idx = np.random.choice(len(df), 20, replace=False)
    df.loc[outlier_idx, 'income'] = df.loc[outlier_idx, 'income'] * 10  # ‡∏£‡∏≤‡∏¢‡πÑ‡∏î‡πâ‡∏™‡∏π‡∏á‡∏ú‡∏¥‡∏î‡∏õ‡∏Å‡∏ï‡∏¥
    
    return df

# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤
problematic_data = introduce_data_quality_issues(current_data.copy())
print(f"üìä ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤: {len(problematic_data)} rows")

# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö
print("\nüîç ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤:")
problem_monitor = DataQualityMonitor(problematic_data, "Problematic Data")
problem_report = problem_monitor.generate_full_report()

#%% [markdown]
# ### 1.4 Data Quality Alert System
# 
# ‡∏£‡∏∞‡∏ö‡∏ö‡πÅ‡∏à‡πâ‡∏á‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡πà‡∏≥‡∏Å‡∏ß‡πà‡∏≤‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î

#%%
class DataQualityAlert:
    """
    ‡∏£‡∏∞‡∏ö‡∏ö‡πÅ‡∏à‡πâ‡∏á‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
    
    ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ threshold ‡πÅ‡∏•‡∏∞‡πÅ‡∏à‡πâ‡∏á‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÄ‡∏Å‡∏¥‡∏ô‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î
    """
    
    def __init__(self):
        # Default thresholds
        self.thresholds = {
            'missing_rate': 0.05,      # ‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏ô 5% missing
            'duplicate_rate': 0.01,    # ‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏ô 1% duplicates
            'outlier_rate': 0.05       # ‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏ô 5% outliers
        }
        self.alerts = []
    
    def set_threshold(self, metric, value):
        """‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ threshold"""
        if metric in self.thresholds:
            self.thresholds[metric] = value
            print(f"‚úÖ ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ {metric} threshold = {value}")
    
    def check_alerts(self, data, data_name="Data"):
        """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á alerts"""
        self.alerts = []
        n_rows = len(data)
        
        # Check missing values
        missing_rate = data.isnull().sum().sum() / (n_rows * len(data.columns))
        if missing_rate > self.thresholds['missing_rate']:
            self.alerts.append({
                'type': 'CRITICAL' if missing_rate > 0.1 else 'WARNING',
                'metric': 'Missing Values',
                'current_value': f"{missing_rate:.2%}",
                'threshold': f"{self.thresholds['missing_rate']:.2%}",
                'message': f"Missing value rate ({missing_rate:.2%}) ‡πÄ‡∏Å‡∏¥‡∏ô threshold"
            })
        
        # Check duplicates
        duplicate_rate = data.duplicated().sum() / n_rows
        if duplicate_rate > self.thresholds['duplicate_rate']:
            self.alerts.append({
                'type': 'WARNING',
                'metric': 'Duplicates',
                'current_value': f"{duplicate_rate:.2%}",
                'threshold': f"{self.thresholds['duplicate_rate']:.2%}",
                'message': f"Duplicate rate ({duplicate_rate:.2%}) ‡πÄ‡∏Å‡∏¥‡∏ô threshold"
            })
        
        # Check outliers (‡πÉ‡∏ä‡πâ IQR method)
        numeric_cols = data.select_dtypes(include=[np.number]).columns
        total_outliers = 0
        for col in numeric_cols:
            Q1 = data[col].quantile(0.25)
            Q3 = data[col].quantile(0.75)
            IQR = Q3 - Q1
            outliers = data[(data[col] < Q1 - 1.5*IQR) | (data[col] > Q3 + 1.5*IQR)]
            total_outliers += len(outliers)
        
        outlier_rate = total_outliers / (n_rows * len(numeric_cols))
        if outlier_rate > self.thresholds['outlier_rate']:
            self.alerts.append({
                'type': 'WARNING',
                'metric': 'Outliers',
                'current_value': f"{outlier_rate:.2%}",
                'threshold': f"{self.thresholds['outlier_rate']:.2%}",
                'message': f"Outlier rate ({outlier_rate:.2%}) ‡πÄ‡∏Å‡∏¥‡∏ô threshold"
            })
        
        # ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏• alerts
        self._display_alerts(data_name)
        
        return self.alerts
    
    def _display_alerts(self, data_name):
        """‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏• alerts"""
        print("=" * 70)
        print(f"üö® DATA QUALITY ALERTS: {data_name}")
        print("=" * 70)
        
        if not self.alerts:
            print("‚úÖ ‡πÑ‡∏°‡πà‡∏°‡∏µ alerts - ‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î")
        else:
            print(f"‚ö†Ô∏è ‡∏û‡∏ö {len(self.alerts)} alerts:")
            print()
            for i, alert in enumerate(self.alerts, 1):
                icon = "üî¥" if alert['type'] == 'CRITICAL' else "üü°"
                print(f"{icon} Alert #{i}: [{alert['type']}] {alert['metric']}")
                print(f"   Current: {alert['current_value']} | Threshold: {alert['threshold']}")
                print(f"   Message: {alert['message']}")
                print()
        
        print("=" * 70)

#%%
# ‡∏ó‡∏î‡∏™‡∏≠‡∏ö Alert System
alert_system = DataQualityAlert()

# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏õ‡∏Å‡∏ï‡∏¥
print("üîç ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Reference Data:")
alerts_ref = alert_system.check_alerts(reference_data, "Reference Data")

print("\n")

# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤
print("üîç ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Problematic Data:")
alerts_problem = alert_system.check_alerts(problematic_data, "Problematic Data")

#%% [markdown]
# ### 1.5 Visualization: Data Quality Dashboard

#%%
def plot_data_quality_summary(data, title="Data Quality Summary"):
    """
    ‡∏™‡∏£‡πâ‡∏≤‡∏á visualization ‡∏™‡∏£‡∏∏‡∏õ‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
    """
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    fig.suptitle(f'üìä {title}', fontsize=14, fontweight='bold')
    
    # 1. Missing Values by Column
    ax1 = axes[0, 0]
    missing = data.isnull().sum()
    missing = missing[missing > 0].sort_values(ascending=True)
    if len(missing) > 0:
        colors = ['red' if v > len(data)*0.05 else 'orange' for v in missing.values]
        missing.plot(kind='barh', ax=ax1, color=colors)
        ax1.set_title('Missing Values by Column')
        ax1.set_xlabel('Count')
    else:
        ax1.text(0.5, 0.5, '‚úÖ No Missing Values', ha='center', va='center', fontsize=12)
        ax1.set_title('Missing Values by Column')
    
    # 2. Data Types Distribution
    ax2 = axes[0, 1]
    dtype_counts = data.dtypes.astype(str).value_counts()
    colors = plt.cm.Set3(range(len(dtype_counts)))
    dtype_counts.plot(kind='pie', ax=ax2, autopct='%1.1f%%', colors=colors)
    ax2.set_title('Data Types Distribution')
    ax2.set_ylabel('')
    
    # 3. Numeric Columns Distribution (Box plots)
    ax3 = axes[1, 0]
    numeric_cols = data.select_dtypes(include=[np.number]).columns[:6]  # ‡πÅ‡∏™‡∏î‡∏á‡πÅ‡∏Ñ‡πà 6 columns
    if len(numeric_cols) > 0:
        # Normalize data for comparison
        normalized = (data[numeric_cols] - data[numeric_cols].mean()) / data[numeric_cols].std()
        normalized.boxplot(ax=ax3)
        ax3.set_title('Numeric Columns Distribution (Normalized)')
        ax3.tick_params(axis='x', rotation=45)
    
    # 4. Quality Score Gauge
    ax4 = axes[1, 1]
    
    # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì quality score
    missing_score = max(0, 100 - (data.isnull().sum().sum() / (len(data) * len(data.columns)) * 100 * 10))
    duplicate_score = max(0, 100 - (data.duplicated().sum() / len(data) * 100 * 10))
    completeness_score = (1 - data.isnull().any(axis=1).sum() / len(data)) * 100
    
    overall_score = (missing_score + duplicate_score + completeness_score) / 3
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á gauge chart ‡∏î‡πâ‡∏ß‡∏¢ pie chart
    sizes = [overall_score, 100 - overall_score]
    colors_gauge = ['green' if overall_score >= 80 else 'orange' if overall_score >= 60 else 'red', 'lightgray']
    ax4.pie(sizes, colors=colors_gauge, startangle=90, counterclock=False)
    
    # ‡∏ß‡∏≤‡∏î‡∏ß‡∏á‡∏Å‡∏•‡∏°‡∏ï‡∏£‡∏á‡∏Å‡∏•‡∏≤‡∏á
    centre_circle = plt.Circle((0, 0), 0.70, fc='white')
    ax4.add_patch(centre_circle)
    ax4.text(0, 0, f'{overall_score:.1f}%', ha='center', va='center', fontsize=20, fontweight='bold')
    ax4.text(0, -0.2, 'Quality Score', ha='center', va='center', fontsize=10)
    ax4.set_title('Overall Data Quality Score')
    
    plt.tight_layout()
    plt.show()
    
    return {
        'missing_score': missing_score,
        'duplicate_score': duplicate_score,
        'completeness_score': completeness_score,
        'overall_score': overall_score
    }

#%%
# ‡πÅ‡∏™‡∏î‡∏á visualization ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Reference Data
print("üìä Reference Data Quality Summary:")
ref_scores = plot_data_quality_summary(reference_data, "Reference Data Quality Summary")

#%%
# ‡πÅ‡∏™‡∏î‡∏á visualization ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Problematic Data
print("üìä Problematic Data Quality Summary:")
problem_scores = plot_data_quality_summary(problematic_data, "Problematic Data Quality Summary")

#%% [markdown]
# ---
# ## üìà Section 2: Model Performance Tracking
# ### ‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡πÇ‡∏°‡πÄ‡∏î‡∏•
# 
# **‡∏ó‡∏§‡∏©‡∏é‡∏µ:**
# 
# Model Performance Tracking ‡∏Ñ‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°‡∏ß‡πà‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÑ‡∏î‡πâ‡∏î‡∏µ‡πÅ‡∏Ñ‡πà‡πÑ‡∏´‡∏ô‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÄ‡∏ß‡∏•‡∏≤‡∏ú‡πà‡∏≤‡∏ô‡πÑ‡∏õ
# 
# **Classification Metrics:**
# - **Accuracy** - ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡πÇ‡∏î‡∏¢‡∏£‡∏ß‡∏°
# - **Precision** - ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥ (‡∏à‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢ positive ‡∏°‡∏µ‡∏Å‡∏µ‡πà % ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å)
# - **Recall** - ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏° (‡∏à‡∏≤‡∏Å positive ‡∏à‡∏£‡∏¥‡∏á ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ñ‡∏π‡∏Å‡∏Å‡∏µ‡πà %)
# - **F1-Score** - ‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏Æ‡∏≤‡∏£‡πå‡πÇ‡∏°‡∏ô‡∏¥‡∏Å‡∏Ç‡∏≠‡∏á Precision ‡πÅ‡∏•‡∏∞ Recall
# 
# **Regression Metrics:**
# - **MAE (Mean Absolute Error)** - ‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏•‡∏≤‡∏î‡πÄ‡∏Ñ‡∏•‡∏∑‡πà‡∏≠‡∏ô‡∏™‡∏±‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå
# - **RMSE (Root Mean Squared Error)** - ‡∏£‡∏≤‡∏Å‡∏Ç‡∏≠‡∏á‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏•‡∏≤‡∏î‡πÄ‡∏Ñ‡∏•‡∏∑‡πà‡∏≠‡∏ô‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏≠‡∏á
# - **R¬≤ (R-squared)** - ‡∏™‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏õ‡∏£‡∏õ‡∏£‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡πÑ‡∏î‡πâ

#%% [markdown]
# ### 2.1 ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏•‡∏∞ Train ‡πÇ‡∏°‡πÄ‡∏î‡∏•

#%%
def prepare_features(data, target_col='default'):
    """
    ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° features ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö modeling
    
    Parameters:
    -----------
    data : DataFrame - ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡∏¥‡∏ö
    target_col : str - ‡∏ä‡∏∑‡πà‡∏≠ column ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô target
    
    Returns:
    --------
    X : DataFrame - features
    y : Series - target
    """
    df = data.copy()
    
    # ‡∏•‡∏ö columns ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ
    drop_cols = ['customer_id']
    df = df.drop(columns=[col for col in drop_cols if col in df.columns])
    
    # ‡πÅ‡∏¢‡∏Å X ‡πÅ‡∏•‡∏∞ y
    y = df[target_col]
    X = df.drop(columns=[target_col])
    
    # Encode categorical columns
    categorical_cols = X.select_dtypes(include=['object']).columns
    for col in categorical_cols:
        le = LabelEncoder()
        X[col] = le.fit_transform(X[col].astype(str))
    
    # Handle missing values
    X = X.fillna(X.median())
    
    return X, y

#%%
# ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Reference
X_ref, y_ref = prepare_features(reference_data)

# ‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö train ‡πÅ‡∏•‡∏∞ test
X_train, X_test, y_train, y_test = train_test_split(
    X_ref, y_ref, test_size=0.3, random_state=42, stratify=y_ref
)

print(f"üìä Training set: {len(X_train)} samples")
print(f"üìä Test set: {len(X_test)} samples")
print(f"\nüìã Features: {list(X_train.columns)}")
print(f"\nüìä Target distribution (train):")
print(y_train.value_counts(normalize=True))

#%%
# Train ‡πÇ‡∏°‡πÄ‡∏î‡∏•
print("üîß ‡∏Å‡∏≥‡∏•‡∏±‡∏á train ‡πÇ‡∏°‡πÄ‡∏î‡∏• Random Forest...")

# Standardize features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Train Random Forest
rf_model = RandomForestClassifier(
    n_estimators=100,
    max_depth=10,
    random_state=42,
    n_jobs=-1
)
rf_model.fit(X_train_scaled, y_train)

print("‚úÖ Train ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô!")

# ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢
y_pred_train = rf_model.predict(X_train_scaled)
y_pred_test = rf_model.predict(X_test_scaled)
y_prob_test = rf_model.predict_proba(X_test_scaled)[:, 1]

print(f"\nüìä Training Accuracy: {accuracy_score(y_train, y_pred_train):.4f}")
print(f"üìä Test Accuracy: {accuracy_score(y_test, y_pred_test):.4f}")

#%% [markdown]
# ### 2.2 ‡∏™‡∏£‡πâ‡∏≤‡∏á Model Performance Monitor Class

#%%
class ModelPerformanceMonitor:
    """
    Class ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡πÇ‡∏°‡πÄ‡∏î‡∏•
    
    ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á Classification ‡πÅ‡∏•‡∏∞ Regression
    """
    
    def __init__(self, model, model_name="Model"):
        """
        Parameters:
        -----------
        model : sklearn model - ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà train ‡πÅ‡∏•‡πâ‡∏ß
        model_name : str - ‡∏ä‡∏∑‡πà‡∏≠‡πÇ‡∏°‡πÄ‡∏î‡∏•
        """
        self.model = model
        self.model_name = model_name
        self.performance_history = []
        self.baseline_metrics = None
        
    def calculate_classification_metrics(self, y_true, y_pred, y_prob=None):
        """‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì metrics ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö classification"""
        metrics = {
            'accuracy': accuracy_score(y_true, y_pred),
            'precision': precision_score(y_true, y_pred, average='weighted', zero_division=0),
            'recall': recall_score(y_true, y_pred, average='weighted', zero_division=0),
            'f1': f1_score(y_true, y_pred, average='weighted', zero_division=0),
        }
        
        # Confusion matrix values
        tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()
        metrics['true_positive'] = int(tp)
        metrics['true_negative'] = int(tn)
        metrics['false_positive'] = int(fp)
        metrics['false_negative'] = int(fn)
        
        # Specificity
        metrics['specificity'] = tn / (tn + fp) if (tn + fp) > 0 else 0
        
        return metrics
    
    def calculate_regression_metrics(self, y_true, y_pred):
        """‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì metrics ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö regression"""
        metrics = {
            'mae': mean_absolute_error(y_true, y_pred),
            'mse': mean_squared_error(y_true, y_pred),
            'rmse': np.sqrt(mean_squared_error(y_true, y_pred)),
            'r2': r2_score(y_true, y_pred),
            'mape': np.mean(np.abs((y_true - y_pred) / (y_true + 1e-10))) * 100
        }
        return metrics
    
    def set_baseline(self, y_true, y_pred, y_prob=None, task='classification'):
        """‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ baseline metrics"""
        if task == 'classification':
            self.baseline_metrics = self.calculate_classification_metrics(y_true, y_pred, y_prob)
        else:
            self.baseline_metrics = self.calculate_regression_metrics(y_true, y_pred)
        
        self.baseline_metrics['timestamp'] = datetime.now()
        self.baseline_metrics['task'] = task
        
        print("‚úÖ ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Baseline Metrics ‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢:")
        self._print_metrics(self.baseline_metrics)
        
        return self.baseline_metrics
    
    def evaluate(self, X, y_true, data_name="Current", task='classification'):
        """
        ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡∏°‡πà
        """
        # ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢
        y_pred = self.model.predict(X)
        y_prob = None
        if task == 'classification' and hasattr(self.model, 'predict_proba'):
            y_prob = self.model.predict_proba(X)[:, 1]
        
        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì metrics
        if task == 'classification':
            current_metrics = self.calculate_classification_metrics(y_true, y_pred, y_prob)
        else:
            current_metrics = self.calculate_regression_metrics(y_true, y_pred)
        
        current_metrics['timestamp'] = datetime.now()
        current_metrics['data_name'] = data_name
        current_metrics['n_samples'] = len(y_true)
        
        # ‡πÄ‡∏Å‡πá‡∏ö‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥
        self.performance_history.append(current_metrics)
        
        # ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Å‡∏±‡∏ö baseline
        comparison = self._compare_with_baseline(current_metrics, task)
        
        # ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•
        self._display_evaluation(current_metrics, comparison, data_name, task)
        
        return current_metrics, comparison
    
    def _compare_with_baseline(self, current_metrics, task):
        """‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Å‡∏±‡∏ö baseline"""
        if self.baseline_metrics is None:
            return None
        
        comparison = {}
        if task == 'classification':
            key_metrics = ['accuracy', 'precision', 'recall', 'f1']
        else:
            key_metrics = ['mae', 'rmse', 'r2']
        
        for metric in key_metrics:
            baseline_val = self.baseline_metrics.get(metric, 0)
            current_val = current_metrics.get(metric, 0)
            
            if task == 'classification':
                # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö classification ‡∏Ñ‡πà‡∏≤‡∏™‡∏π‡∏á‡∏Å‡∏ß‡πà‡∏≤‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤
                change = current_val - baseline_val
                change_pct = (change / baseline_val * 100) if baseline_val != 0 else 0
            else:
                # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö regression (MAE, RMSE) ‡∏Ñ‡πà‡∏≤‡∏ï‡πà‡∏≥‡∏Å‡∏ß‡πà‡∏≤‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤ (‡∏¢‡∏Å‡πÄ‡∏ß‡πâ‡∏ô R2)
                if metric == 'r2':
                    change = current_val - baseline_val
                    change_pct = (change / baseline_val * 100) if baseline_val != 0 else 0
                else:
                    change = baseline_val - current_val
                    change_pct = (change / baseline_val * 100) if baseline_val != 0 else 0
            
            comparison[metric] = {
                'baseline': baseline_val,
                'current': current_val,
                'change': change,
                'change_pct': change_pct,
                'status': 'improved' if change > 0 else 'degraded' if change < 0 else 'stable'
            }
        
        return comparison
    
    def _print_metrics(self, metrics):
        """‡πÅ‡∏™‡∏î‡∏á metrics"""
        for key, value in metrics.items():
            if key not in ['timestamp', 'task', 'data_name', 'n_samples']:
                if isinstance(value, float):
                    print(f"   {key}: {value:.4f}")
                else:
                    print(f"   {key}: {value}")
    
    def _display_evaluation(self, metrics, comparison, data_name, task):
        """‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô"""
        print("=" * 70)
        print(f"üìä MODEL PERFORMANCE EVALUATION: {data_name}")
        print(f"   Model: {self.model_name}")
        print(f"   Samples: {metrics['n_samples']:,}")
        print(f"   Time: {metrics['timestamp'].strftime('%Y-%m-%d %H:%M:%S')}")
        print("=" * 70)
        
        if task == 'classification':
            print("\nüìà Classification Metrics:")
            print("-" * 40)
            print(f"   Accuracy:  {metrics['accuracy']:.4f}")
            print(f"   Precision: {metrics['precision']:.4f}")
            print(f"   Recall:    {metrics['recall']:.4f}")
            print(f"   F1-Score:  {metrics['f1']:.4f}")
            print(f"\nüìä Confusion Matrix:")
            print(f"   TP: {metrics['true_positive']} | FP: {metrics['false_positive']}")
            print(f"   FN: {metrics['false_negative']} | TN: {metrics['true_negative']}")
        else:
            print("\nüìà Regression Metrics:")
            print("-" * 40)
            print(f"   MAE:  {metrics['mae']:.4f}")
            print(f"   RMSE: {metrics['rmse']:.4f}")
            print(f"   R¬≤:   {metrics['r2']:.4f}")
        
        if comparison:
            print("\nüìä Comparison with Baseline:")
            print("-" * 40)
            for metric, values in comparison.items():
                icon = "üìà" if values['status'] == 'improved' else "üìâ" if values['status'] == 'degraded' else "‚û°Ô∏è"
                print(f"   {icon} {metric}: {values['baseline']:.4f} ‚Üí {values['current']:.4f} ({values['change_pct']:+.2f}%)")
        
        print("=" * 70)

#%%
# ‡∏™‡∏£‡πâ‡∏≤‡∏á Performance Monitor
perf_monitor = ModelPerformanceMonitor(rf_model, "Random Forest Credit Risk")

# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Baseline ‡∏î‡πâ‡∏ß‡∏¢ Test Data
print("üìä ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Baseline Metrics:")
baseline = perf_monitor.set_baseline(y_test, y_pred_test, y_prob_test, task='classification')

#%%
# ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏Å‡∏±‡∏ö Current Data (‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡∏°‡πà‡∏ó‡∏µ‡πà‡∏°‡∏µ drift)
X_curr, y_curr = prepare_features(current_data)
X_curr_scaled = scaler.transform(X_curr)

print("\nüìä ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏Å‡∏±‡∏ö Current Data (‡∏°‡∏µ Drift):")
curr_metrics, comparison = perf_monitor.evaluate(
    X_curr_scaled, y_curr, 
    data_name="Current Data (with drift)", 
    task='classification'
)

#%% [markdown]
# ### 2.3 ‡∏à‡∏≥‡∏•‡∏≠‡∏á‡∏Å‡∏≤‡∏£ Monitor ‡∏´‡∏•‡∏≤‡∏¢‡∏ä‡πà‡∏ß‡∏á‡πÄ‡∏ß‡∏•‡∏≤

#%%
def simulate_time_series_monitoring(model, scaler, n_periods=10):
    """
    ‡∏à‡∏≥‡∏•‡∏≠‡∏á‡∏Å‡∏≤‡∏£ monitor ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏Ç‡πâ‡∏≤‡∏°‡∏´‡∏•‡∏≤‡∏¢‡∏ä‡πà‡∏ß‡∏á‡πÄ‡∏ß‡∏•‡∏≤
    """
    performance_over_time = []
    
    for period in range(n_periods):
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≥‡∏•‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ä‡πà‡∏ß‡∏á‡πÄ‡∏ß‡∏•‡∏≤ ‡πÇ‡∏î‡∏¢ drift ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡∏∂‡πâ‡∏ô‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏¢‡πÜ
        drift_level = period * 0.05  # drift ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡∏∂‡πâ‡∏ô 5% ‡∏ï‡πà‡∏≠ period
        
        period_data = create_credit_data(
            n_samples=500, 
            seed=42 + period,
            drift_level=drift_level
        )
        
        X_period, y_period = prepare_features(period_data)
        X_period_scaled = scaler.transform(X_period)
        
        # ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢
        y_pred_period = model.predict(X_period_scaled)
        
        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì metrics
        metrics = {
            'period': period + 1,
            'drift_level': drift_level,
            'accuracy': accuracy_score(y_period, y_pred_period),
            'precision': precision_score(y_period, y_pred_period, average='weighted', zero_division=0),
            'recall': recall_score(y_period, y_pred_period, average='weighted', zero_division=0),
            'f1': f1_score(y_period, y_pred_period, average='weighted', zero_division=0),
            'n_samples': len(y_period),
            'default_rate': y_period.mean()
        }
        
        performance_over_time.append(metrics)
    
    return pd.DataFrame(performance_over_time)

#%%
# ‡∏à‡∏≥‡∏•‡∏≠‡∏á‡∏Å‡∏≤‡∏£ monitor 10 ‡∏ä‡πà‡∏ß‡∏á‡πÄ‡∏ß‡∏•‡∏≤
print("üîÑ ‡∏à‡∏≥‡∏•‡∏≠‡∏á‡∏Å‡∏≤‡∏£ Monitor ‡∏Ç‡πâ‡∏≤‡∏°‡∏´‡∏•‡∏≤‡∏¢‡∏ä‡πà‡∏ß‡∏á‡πÄ‡∏ß‡∏•‡∏≤...")
time_series_perf = simulate_time_series_monitoring(rf_model, scaler, n_periods=10)
print("\nüìä Performance Over Time:")
print(time_series_perf.to_string(index=False))

#%% [markdown]
# ### 2.4 Visualization: Performance Over Time

#%%
def plot_performance_over_time(perf_df):
    """
    ‡πÅ‡∏™‡∏î‡∏á‡∏Å‡∏£‡∏≤‡∏ü‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ï‡∏≤‡∏°‡πÄ‡∏ß‡∏•‡∏≤
    """
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    fig.suptitle('üìà Model Performance Over Time', fontsize=14, fontweight='bold')
    
    # 1. Main Metrics Over Time
    ax1 = axes[0, 0]
    metrics_to_plot = ['accuracy', 'precision', 'recall', 'f1']
    for metric in metrics_to_plot:
        ax1.plot(perf_df['period'], perf_df[metric], marker='o', label=metric.capitalize())
    ax1.set_xlabel('Time Period')
    ax1.set_ylabel('Score')
    ax1.set_title('Classification Metrics Over Time')
    ax1.legend()
    ax1.set_ylim([0.5, 1.0])
    ax1.axhline(y=0.8, color='red', linestyle='--', alpha=0.5, label='Threshold')
    ax1.grid(True, alpha=0.3)
    
    # 2. Accuracy vs Drift Level
    ax2 = axes[0, 1]
    ax2.scatter(perf_df['drift_level'], perf_df['accuracy'], s=100, c=perf_df['period'], cmap='viridis')
    ax2.plot(perf_df['drift_level'], perf_df['accuracy'], 'r--', alpha=0.5)
    ax2.set_xlabel('Drift Level')
    ax2.set_ylabel('Accuracy')
    ax2.set_title('Accuracy vs Data Drift Level')
    
    # ‡πÄ‡∏û‡∏¥‡πà‡∏° colorbar
    sm = plt.cm.ScalarMappable(cmap='viridis', norm=plt.Normalize(vmin=1, vmax=len(perf_df)))
    sm.set_array([])
    cbar = plt.colorbar(sm, ax=ax2)
    cbar.set_label('Period')
    
    # 3. Default Rate Over Time
    ax3 = axes[1, 0]
    ax3.bar(perf_df['period'], perf_df['default_rate'], color='coral', alpha=0.7)
    ax3.set_xlabel('Time Period')
    ax3.set_ylabel('Default Rate')
    ax3.set_title('Target (Default Rate) Over Time')
    ax3.axhline(y=perf_df['default_rate'].iloc[0], color='green', linestyle='--', 
                label=f"Baseline: {perf_df['default_rate'].iloc[0]:.2%}")
    ax3.legend()
    
    # 4. Performance Degradation Alert
    ax4 = axes[1, 1]
    
    # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì degradation ‡∏à‡∏≤‡∏Å baseline
    baseline_accuracy = perf_df['accuracy'].iloc[0]
    degradation = (baseline_accuracy - perf_df['accuracy']) / baseline_accuracy * 100
    
    colors = ['green' if d < 5 else 'orange' if d < 10 else 'red' for d in degradation]
    ax4.bar(perf_df['period'], degradation, color=colors, alpha=0.7)
    ax4.set_xlabel('Time Period')
    ax4.set_ylabel('Degradation (%)')
    ax4.set_title('Accuracy Degradation from Baseline')
    ax4.axhline(y=5, color='orange', linestyle='--', alpha=0.7, label='Warning (5%)')
    ax4.axhline(y=10, color='red', linestyle='--', alpha=0.7, label='Critical (10%)')
    ax4.legend()
    
    plt.tight_layout()
    plt.show()
    
    # ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•
    print("\nüìä Performance Summary:")
    print(f"   Baseline Accuracy: {baseline_accuracy:.4f}")
    print(f"   Final Accuracy: {perf_df['accuracy'].iloc[-1]:.4f}")
    print(f"   Total Degradation: {degradation.iloc[-1]:.2f}%")
    
    if degradation.iloc[-1] > 10:
        print("   üî¥ Status: CRITICAL - ‡∏ï‡πâ‡∏≠‡∏á retrain ‡πÇ‡∏°‡πÄ‡∏î‡∏•!")
    elif degradation.iloc[-1] > 5:
        print("   üü° Status: WARNING - ‡∏Ñ‡∏ß‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° retrain")
    else:
        print("   üü¢ Status: HEALTHY - ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏õ‡∏Å‡∏ï‡∏¥")

#%%
# ‡πÅ‡∏™‡∏î‡∏á‡∏Å‡∏£‡∏≤‡∏ü
plot_performance_over_time(time_series_perf)

#%% [markdown]
# ### 2.5 Confusion Matrix Visualization

#%%
def plot_confusion_matrix_comparison(y_true_baseline, y_pred_baseline, 
                                      y_true_current, y_pred_current):
    """
    ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö Confusion Matrix ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á Baseline ‡πÅ‡∏•‡∏∞ Current
    """
    fig, axes = plt.subplots(1, 2, figsize=(12, 5))
    fig.suptitle('üìä Confusion Matrix Comparison', fontsize=14, fontweight='bold')
    
    # Baseline
    cm_baseline = confusion_matrix(y_true_baseline, y_pred_baseline)
    sns.heatmap(cm_baseline, annot=True, fmt='d', cmap='Blues', ax=axes[0],
                xticklabels=['No Default', 'Default'],
                yticklabels=['No Default', 'Default'])
    axes[0].set_title('Baseline (Test Data)')
    axes[0].set_xlabel('Predicted')
    axes[0].set_ylabel('Actual')
    
    # Current
    cm_current = confusion_matrix(y_true_current, y_pred_current)
    sns.heatmap(cm_current, annot=True, fmt='d', cmap='Oranges', ax=axes[1],
                xticklabels=['No Default', 'Default'],
                yticklabels=['No Default', 'Default'])
    axes[1].set_title('Current Data (with Drift)')
    axes[1].set_xlabel('Predicted')
    axes[1].set_ylabel('Actual')
    
    plt.tight_layout()
    plt.show()

#%%
# ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢ current data
y_pred_curr = rf_model.predict(X_curr_scaled)

# ‡πÅ‡∏™‡∏î‡∏á confusion matrix comparison
plot_confusion_matrix_comparison(y_test, y_pred_test, y_curr, y_pred_curr)

#%% [markdown]
# ---
# ## üéØ Section 3: Target Drift Detection
# ### ‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡∏≠‡∏á Target Distribution
# 
# **‡∏ó‡∏§‡∏©‡∏é‡∏µ:**
# 
# **Target Drift** (‡∏´‡∏£‡∏∑‡∏≠ Concept Drift) ‡∏Ñ‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡∏≠‡∏á distribution ‡∏Ç‡∏≠‡∏á target variable
# 
# **‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Ç‡∏≠‡∏á Drift:**
# 1. **Sudden Drift** - ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏∞‡∏ó‡∏±‡∏ô‡∏´‡∏±‡∏ô
# 2. **Gradual Drift** - ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏ó‡∏µ‡∏•‡∏∞‡∏ô‡πâ‡∏≠‡∏¢
# 3. **Recurring Drift** - ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏≠‡∏ö (‡πÄ‡∏ä‡πà‡∏ô seasonal)
# 
# **‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö:**
# 1. **Statistical Tests** - Chi-square, KS test, PSI
# 2. **Distribution Comparison** - ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö histogram
# 3. **Threshold-based Alerts** - ‡πÅ‡∏à‡πâ‡∏á‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÄ‡∏Å‡∏¥‡∏ô‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î

#%% [markdown]
# ### 3.1 ‡∏™‡∏£‡πâ‡∏≤‡∏á Drift Detection Class

#%%
class DriftDetector:
    """
    Class ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö Data Drift ‡πÅ‡∏•‡∏∞ Target Drift
    
    ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏´‡∏•‡∏≤‡∏¢‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡∏ó‡∏≤‡∏á‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥:
    - Kolmogorov-Smirnov Test (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö continuous)
    - Chi-Square Test (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö categorical)
    - Population Stability Index (PSI)
    - Wasserstein Distance
    """
    
    def __init__(self, reference_data, reference_name="Reference"):
        """
        Parameters:
        -----------
        reference_data : DataFrame - ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• reference
        reference_name : str - ‡∏ä‡∏∑‡πà‡∏≠‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• reference
        """
        self.reference_data = reference_data.copy()
        self.reference_name = reference_name
        self.drift_results = {}
        
    def ks_test(self, reference_col, current_col):
        """
        Kolmogorov-Smirnov Test ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö continuous variables
        
        H0: ‡∏ó‡∏±‡πâ‡∏á‡∏™‡∏≠‡∏á distribution ‡∏°‡∏≤‡∏à‡∏≤‡∏Å‡∏õ‡∏£‡∏∞‡∏ä‡∏≤‡∏Å‡∏£‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô
        ‡∏ñ‡πâ‡∏≤ p-value < 0.05 ‡πÅ‡∏™‡∏î‡∏á‡∏ß‡πà‡∏≤‡∏°‡∏µ drift
        """
        statistic, p_value = ks_2samp(reference_col.dropna(), current_col.dropna())
        return {
            'test': 'Kolmogorov-Smirnov',
            'statistic': statistic,
            'p_value': p_value,
            'drift_detected': p_value < 0.05
        }
    
    def chi_square_test(self, reference_col, current_col):
        """
        Chi-Square Test ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö categorical variables
        """
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á contingency table
        ref_counts = reference_col.value_counts()
        curr_counts = current_col.value_counts()
        
        # ‡∏£‡∏ß‡∏° categories
        all_categories = set(ref_counts.index) | set(curr_counts.index)
        
        ref_freq = [ref_counts.get(cat, 0) for cat in all_categories]
        curr_freq = [curr_counts.get(cat, 0) for cat in all_categories]
        
        contingency = np.array([ref_freq, curr_freq])
        
        chi2, p_value, dof, expected = chi2_contingency(contingency)
        
        return {
            'test': 'Chi-Square',
            'statistic': chi2,
            'p_value': p_value,
            'degrees_of_freedom': dof,
            'drift_detected': p_value < 0.05
        }
    
    def calculate_psi(self, reference_col, current_col, bins=10):
        """
        Population Stability Index (PSI)
        
        PSI < 0.1: ‡πÑ‡∏°‡πà‡∏°‡∏µ drift
        0.1 <= PSI < 0.25: drift ‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢
        PSI >= 0.25: drift ‡∏°‡∏≤‡∏Å
        """
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á bins ‡∏à‡∏≤‡∏Å reference
        ref_min, ref_max = reference_col.min(), reference_col.max()
        bin_edges = np.linspace(ref_min, ref_max, bins + 1)
        
        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì % ‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞ bin
        ref_hist, _ = np.histogram(reference_col, bins=bin_edges)
        curr_hist, _ = np.histogram(current_col.clip(ref_min, ref_max), bins=bin_edges)
        
        # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô percentage
        ref_pct = ref_hist / len(reference_col)
        curr_pct = curr_hist / len(current_col)
        
        # ‡∏´‡∏•‡∏µ‡∏Å‡πÄ‡∏•‡∏µ‡πà‡∏¢‡∏á log(0)
        ref_pct = np.where(ref_pct == 0, 0.0001, ref_pct)
        curr_pct = np.where(curr_pct == 0, 0.0001, curr_pct)
        
        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì PSI
        psi = np.sum((curr_pct - ref_pct) * np.log(curr_pct / ref_pct))
        
        return {
            'psi': psi,
            'interpretation': 'No Drift' if psi < 0.1 else 'Slight Drift' if psi < 0.25 else 'Significant Drift',
            'drift_detected': psi >= 0.1
        }
    
    def wasserstein_distance_test(self, reference_col, current_col):
        """
        Wasserstein Distance (Earth Mover's Distance)
        ‡∏ß‡∏±‡∏î‡∏£‡∏∞‡∏¢‡∏∞‡∏´‡πà‡∏≤‡∏á‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏™‡∏≠‡∏á distributions
        """
        distance = wasserstein_distance(reference_col.dropna(), current_col.dropna())
        
        # Normalize by reference std
        ref_std = reference_col.std()
        normalized_distance = distance / ref_std if ref_std > 0 else distance
        
        return {
            'distance': distance,
            'normalized_distance': normalized_distance,
            'drift_detected': normalized_distance > 0.1
        }
    
    def detect_feature_drift(self, current_data, columns=None):
        """
        ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö drift ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏∏‡∏Å features
        """
        if columns is None:
            # ‡πÉ‡∏ä‡πâ‡∏ó‡∏∏‡∏Å columns ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏ó‡∏±‡πâ‡∏á‡∏™‡∏≠‡∏á datasets
            columns = [col for col in self.reference_data.columns if col in current_data.columns]
        
        results = []
        
        for col in columns:
            ref_col = self.reference_data[col]
            curr_col = current_data[col]
            
            # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏ï‡∏≤‡∏°‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
            if ref_col.dtype in ['object', 'category'] or ref_col.nunique() < 10:
                # Categorical
                test_result = self.chi_square_test(ref_col, curr_col)
                test_type = 'categorical'
            else:
                # Continuous
                ks_result = self.ks_test(ref_col, curr_col)
                psi_result = self.calculate_psi(ref_col, curr_col)
                
                test_result = {
                    'test': 'KS + PSI',
                    'ks_statistic': ks_result['statistic'],
                    'ks_p_value': ks_result['p_value'],
                    'psi': psi_result['psi'],
                    'psi_interpretation': psi_result['interpretation'],
                    'drift_detected': ks_result['drift_detected'] or psi_result['drift_detected']
                }
                test_type = 'continuous'
            
            results.append({
                'column': col,
                'type': test_type,
                **test_result
            })
        
        self.drift_results['feature_drift'] = results
        return pd.DataFrame(results)
    
    def detect_target_drift(self, current_data, target_col):
        """
        ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö Target Drift ‡πÇ‡∏î‡∏¢‡πÄ‡∏â‡∏û‡∏≤‡∏∞
        """
        ref_target = self.reference_data[target_col]
        curr_target = current_data[target_col]
        
        result = {
            'column': target_col,
            'reference_mean': ref_target.mean(),
            'current_mean': curr_target.mean(),
            'reference_std': ref_target.std(),
            'current_std': curr_target.std()
        }
        
        # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏ó‡∏≤‡∏á‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥
        if ref_target.nunique() <= 2:  # Binary target
            # Chi-square test
            chi_result = self.chi_square_test(ref_target, curr_target)
            result.update({
                'test': 'Chi-Square',
                'statistic': chi_result['statistic'],
                'p_value': chi_result['p_value'],
                'drift_detected': chi_result['drift_detected']
            })
        else:
            # KS test + PSI
            ks_result = self.ks_test(ref_target, curr_target)
            psi_result = self.calculate_psi(ref_target, curr_target)
            result.update({
                'test': 'KS + PSI',
                'ks_statistic': ks_result['statistic'],
                'ks_p_value': ks_result['p_value'],
                'psi': psi_result['psi'],
                'drift_detected': ks_result['drift_detected'] or psi_result['drift_detected']
            })
        
        self.drift_results['target_drift'] = result
        return result
    
    def generate_drift_report(self, current_data, target_col=None):
        """
        ‡∏™‡∏£‡πâ‡∏≤‡∏á Drift Report ‡∏â‡∏ö‡∏±‡∏ö‡πÄ‡∏ï‡πá‡∏°
        """
        print("=" * 70)
        print("üîç DRIFT DETECTION REPORT")
        print("=" * 70)
        print(f"üìÖ Generated at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"üìä Reference Data: {len(self.reference_data):,} samples")
        print(f"üìä Current Data: {len(current_data):,} samples")
        print()
        
        # Feature Drift
        print("-" * 50)
        print("1Ô∏è‚É£ FEATURE DRIFT DETECTION")
        print("-" * 50)
        
        feature_cols = [col for col in self.reference_data.columns 
                       if col != target_col and col in current_data.columns]
        feature_drift_df = self.detect_feature_drift(current_data, feature_cols)
        
        drifted_features = feature_drift_df[feature_drift_df['drift_detected'] == True]
        
        if len(drifted_features) > 0:
            print(f"‚ö†Ô∏è ‡∏û‡∏ö Drift ‡πÉ‡∏ô {len(drifted_features)}/{len(feature_cols)} features:")
            for _, row in drifted_features.iterrows():
                print(f"   üî¥ {row['column']}: {row['test']}")
        else:
            print("‚úÖ ‡πÑ‡∏°‡πà‡∏û‡∏ö Feature Drift ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏ô‡∏±‡∏¢‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç")
        print()
        
        # Target Drift
        if target_col:
            print("-" * 50)
            print("2Ô∏è‚É£ TARGET DRIFT DETECTION")
            print("-" * 50)
            
            target_result = self.detect_target_drift(current_data, target_col)
            
            print(f"   Target Column: {target_col}")
            print(f"   Reference Mean: {target_result['reference_mean']:.4f}")
            print(f"   Current Mean: {target_result['current_mean']:.4f}")
            print(f"   Change: {(target_result['current_mean'] - target_result['reference_mean']):.4f}")
            print(f"   Test: {target_result['test']}")
            print(f"   P-value: {target_result.get('p_value', 'N/A')}")
            
            if target_result['drift_detected']:
                print("   üî¥ Status: TARGET DRIFT DETECTED!")
            else:
                print("   üü¢ Status: No significant target drift")
        
        print()
        print("=" * 70)
        print("üìä END OF DRIFT REPORT")
        print("=" * 70)
        
        return {
            'feature_drift': feature_drift_df,
            'target_drift': self.drift_results.get('target_drift')
        }

#%%
# ‡∏ó‡∏î‡∏™‡∏≠‡∏ö Drift Detection
print("üîç ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö Drift ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á Reference ‡πÅ‡∏•‡∏∞ Current Data:")
print()

drift_detector = DriftDetector(reference_data, "Reference Data")
drift_report = drift_detector.generate_drift_report(current_data, target_col='default')

#%% [markdown]
# ### 3.2 Visualization: Drift Analysis

#%%
def plot_drift_analysis(reference_data, current_data, columns_to_plot=None, target_col='default'):
    """
    Visualization ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Drift Analysis
    """
    if columns_to_plot is None:
        numeric_cols = reference_data.select_dtypes(include=[np.number]).columns
        columns_to_plot = [col for col in numeric_cols if col != target_col][:6]
    
    n_cols = min(len(columns_to_plot), 3)
    n_rows = (len(columns_to_plot) + n_cols - 1) // n_cols
    
    fig, axes = plt.subplots(n_rows, n_cols, figsize=(5*n_cols, 4*n_rows))
    fig.suptitle('üìä Feature Distribution Comparison: Reference vs Current', 
                 fontsize=14, fontweight='bold')
    
    if n_rows == 1:
        axes = [axes] if n_cols == 1 else axes
    else:
        axes = axes.flatten()
    
    for idx, col in enumerate(columns_to_plot):
        ax = axes[idx] if isinstance(axes, (list, np.ndarray)) else axes
        
        # Plot distributions
        ax.hist(reference_data[col], bins=30, alpha=0.5, label='Reference', color='blue', density=True)
        ax.hist(current_data[col], bins=30, alpha=0.5, label='Current', color='orange', density=True)
        
        # KS test
        ks_stat, ks_pval = ks_2samp(reference_data[col].dropna(), current_data[col].dropna())
        
        ax.set_title(f'{col}\nKS stat: {ks_stat:.3f}, p-value: {ks_pval:.3f}')
        ax.legend()
        ax.set_xlabel(col)
        ax.set_ylabel('Density')
        
        # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏™‡∏µ background ‡∏ï‡∏≤‡∏° drift status
        if ks_pval < 0.05:
            ax.set_facecolor('#ffcccc')  # ‡∏™‡∏µ‡πÅ‡∏î‡∏á‡∏≠‡πà‡∏≠‡∏ô‡∏ñ‡πâ‡∏≤‡∏°‡∏µ drift
    
    # ‡∏ã‡πà‡∏≠‡∏ô axes ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ
    for idx in range(len(columns_to_plot), len(axes)):
        axes[idx].set_visible(False)
    
    plt.tight_layout()
    plt.show()

#%%
# ‡πÅ‡∏™‡∏î‡∏á Feature Distribution Comparison
plot_drift_analysis(reference_data, current_data, target_col='default')

#%%
def plot_target_drift(reference_data, current_data, target_col='default'):
    """
    Visualization ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Target Drift
    """
    fig, axes = plt.subplots(1, 3, figsize=(15, 5))
    fig.suptitle('üéØ Target Drift Analysis', fontsize=14, fontweight='bold')
    
    # 1. Distribution Comparison
    ax1 = axes[0]
    ref_counts = reference_data[target_col].value_counts(normalize=True)
    curr_counts = current_data[target_col].value_counts(normalize=True)
    
    x = np.arange(len(ref_counts))
    width = 0.35
    
    ax1.bar(x - width/2, ref_counts.values, width, label='Reference', color='blue', alpha=0.7)
    ax1.bar(x + width/2, curr_counts.values, width, label='Current', color='orange', alpha=0.7)
    
    ax1.set_xlabel('Target Value')
    ax1.set_ylabel('Proportion')
    ax1.set_title('Target Distribution Comparison')
    ax1.set_xticks(x)
    ax1.set_xticklabels(['No Default (0)', 'Default (1)'])
    ax1.legend()
    
    # ‡πÄ‡∏û‡∏¥‡πà‡∏° % labels
    for i, (ref_v, curr_v) in enumerate(zip(ref_counts.values, curr_counts.values)):
        ax1.text(i - width/2, ref_v + 0.01, f'{ref_v:.1%}', ha='center', fontsize=9)
        ax1.text(i + width/2, curr_v + 0.01, f'{curr_v:.1%}', ha='center', fontsize=9)
    
    # 2. Cumulative Distribution
    ax2 = axes[1]
    ref_sorted = np.sort(reference_data[target_col])
    curr_sorted = np.sort(current_data[target_col])
    
    ref_cdf = np.arange(1, len(ref_sorted) + 1) / len(ref_sorted)
    curr_cdf = np.arange(1, len(curr_sorted) + 1) / len(curr_sorted)
    
    ax2.plot(ref_sorted, ref_cdf, label='Reference', color='blue')
    ax2.plot(curr_sorted, curr_cdf, label='Current', color='orange')
    ax2.set_xlabel('Target Value')
    ax2.set_ylabel('Cumulative Probability')
    ax2.set_title('Cumulative Distribution Function (CDF)')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    # 3. Drift Summary
    ax3 = axes[2]
    ax3.axis('off')
    
    # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì metrics
    ref_mean = reference_data[target_col].mean()
    curr_mean = current_data[target_col].mean()
    change = curr_mean - ref_mean
    change_pct = (change / ref_mean) * 100 if ref_mean != 0 else 0
    
    # Chi-square test
    contingency = np.array([
        [reference_data[target_col].sum(), len(reference_data) - reference_data[target_col].sum()],
        [current_data[target_col].sum(), len(current_data) - current_data[target_col].sum()]
    ])
    chi2, p_value, _, _ = chi2_contingency(contingency)
    
    summary_text = f"""
    üìä TARGET DRIFT SUMMARY
    ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    
    Reference Default Rate: {ref_mean:.2%}
    Current Default Rate:   {curr_mean:.2%}
    
    Absolute Change: {change:+.2%}
    Relative Change: {change_pct:+.1f}%
    
    ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    Statistical Test (Chi-Square):
    Chi¬≤ Statistic: {chi2:.4f}
    P-value: {p_value:.4f}
    
    ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    Status: {'üî¥ DRIFT DETECTED' if p_value < 0.05 else 'üü¢ NO SIGNIFICANT DRIFT'}
    """
    
    ax3.text(0.1, 0.5, summary_text, transform=ax3.transAxes, 
             fontsize=11, verticalalignment='center', fontfamily='monospace',
             bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))
    
    plt.tight_layout()
    plt.show()

#%%
# ‡πÅ‡∏™‡∏î‡∏á Target Drift Analysis
plot_target_drift(reference_data, current_data, target_col='default')

#%% [markdown]
# ### 3.3 Prediction Drift Detection

#%%
def detect_prediction_drift(model, scaler, reference_data, current_data, target_col='default'):
    """
    ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö Prediction Drift
    ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á predictions ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á reference ‡πÅ‡∏•‡∏∞ current
    """
    # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
    X_ref, _ = prepare_features(reference_data, target_col)
    X_curr, _ = prepare_features(current_data, target_col)
    
    X_ref_scaled = scaler.transform(X_ref)
    X_curr_scaled = scaler.transform(X_curr)
    
    # ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢ probability
    ref_proba = model.predict_proba(X_ref_scaled)[:, 1]
    curr_proba = model.predict_proba(X_curr_scaled)[:, 1]
    
    # ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢ class
    ref_pred = model.predict(X_ref_scaled)
    curr_pred = model.predict(X_curr_scaled)
    
    # Statistical tests
    ks_stat, ks_pval = ks_2samp(ref_proba, curr_proba)
    wasserstein = wasserstein_distance(ref_proba, curr_proba)
    
    # Prediction distribution
    ref_positive_rate = ref_pred.mean()
    curr_positive_rate = curr_pred.mean()
    
    print("=" * 70)
    print("üîÆ PREDICTION DRIFT ANALYSIS")
    print("=" * 70)
    print()
    print("üìä Prediction Probability Statistics:")
    print(f"   Reference - Mean: {ref_proba.mean():.4f}, Std: {ref_proba.std():.4f}")
    print(f"   Current   - Mean: {curr_proba.mean():.4f}, Std: {curr_proba.std():.4f}")
    print()
    print("üìä Predicted Positive Rate:")
    print(f"   Reference: {ref_positive_rate:.2%}")
    print(f"   Current:   {curr_positive_rate:.2%}")
    print(f"   Change:    {(curr_positive_rate - ref_positive_rate):.2%}")
    print()
    print("üìä Statistical Tests:")
    print(f"   KS Statistic: {ks_stat:.4f}")
    print(f"   KS P-value: {ks_pval:.4f}")
    print(f"   Wasserstein Distance: {wasserstein:.4f}")
    print()
    
    if ks_pval < 0.05:
        print("   üî¥ Status: PREDICTION DRIFT DETECTED!")
    else:
        print("   üü¢ Status: No significant prediction drift")
    
    print("=" * 70)
    
    # Visualization
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))
    fig.suptitle('üîÆ Prediction Drift Analysis', fontsize=14, fontweight='bold')
    
    # 1. Probability Distribution
    ax1 = axes[0]
    ax1.hist(ref_proba, bins=50, alpha=0.5, label='Reference', color='blue', density=True)
    ax1.hist(curr_proba, bins=50, alpha=0.5, label='Current', color='orange', density=True)
    ax1.set_xlabel('Predicted Probability')
    ax1.set_ylabel('Density')
    ax1.set_title(f'Prediction Probability Distribution\nKS stat: {ks_stat:.3f}, p-value: {ks_pval:.4f}')
    ax1.legend()
    ax1.axvline(x=0.5, color='red', linestyle='--', alpha=0.5, label='Threshold')
    
    # 2. Predicted Class Distribution
    ax2 = axes[1]
    categories = ['No Default', 'Default']
    ref_dist = [(ref_pred == 0).sum(), (ref_pred == 1).sum()]
    curr_dist = [(curr_pred == 0).sum(), (curr_pred == 1).sum()]
    
    x = np.arange(len(categories))
    width = 0.35
    
    ax2.bar(x - width/2, ref_dist, width, label='Reference', color='blue', alpha=0.7)
    ax2.bar(x + width/2, curr_dist, width, label='Current', color='orange', alpha=0.7)
    ax2.set_xlabel('Predicted Class')
    ax2.set_ylabel('Count')
    ax2.set_title('Predicted Class Distribution')
    ax2.set_xticks(x)
    ax2.set_xticklabels(categories)
    ax2.legend()
    
    plt.tight_layout()
    plt.show()
    
    return {
        'ks_statistic': ks_stat,
        'ks_pvalue': ks_pval,
        'wasserstein_distance': wasserstein,
        'ref_positive_rate': ref_positive_rate,
        'curr_positive_rate': curr_positive_rate,
        'drift_detected': ks_pval < 0.05
    }

#%%
# ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö Prediction Drift
prediction_drift = detect_prediction_drift(rf_model, scaler, reference_data, current_data)

#%% [markdown]
# ### 3.4 Drift Alert System

#%%
class DriftAlertSystem:
    """
    ‡∏£‡∏∞‡∏ö‡∏ö‡πÅ‡∏à‡πâ‡∏á‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô Drift
    """
    
    def __init__(self):
        self.thresholds = {
            'psi': 0.1,              # PSI threshold
            'ks_pvalue': 0.05,       # KS test p-value
            'target_change': 0.05,   # Target rate change
            'prediction_change': 0.05  # Prediction rate change
        }
        self.alerts = []
    
    def check_drift_alerts(self, drift_results, data_name="Current Data"):
        """
        ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á alerts ‡∏à‡∏≤‡∏Å‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö drift
        """
        self.alerts = []
        
        # Check feature drift
        if 'feature_drift' in drift_results:
            feature_df = drift_results['feature_drift']
            drifted_features = feature_df[feature_df['drift_detected'] == True]
            
            if len(drifted_features) > 0:
                drift_pct = len(drifted_features) / len(feature_df) * 100
                alert_type = 'CRITICAL' if drift_pct > 30 else 'WARNING'
                
                self.alerts.append({
                    'type': alert_type,
                    'category': 'Feature Drift',
                    'message': f"‡∏û‡∏ö {len(drifted_features)} features ({drift_pct:.1f}%) ‡∏°‡∏µ drift",
                    'details': drifted_features['column'].tolist()
                })
        
        # Check target drift
        if 'target_drift' in drift_results and drift_results['target_drift']:
            target_result = drift_results['target_drift']
            if target_result.get('drift_detected', False):
                change = abs(target_result['current_mean'] - target_result['reference_mean'])
                
                self.alerts.append({
                    'type': 'CRITICAL' if change > 0.1 else 'WARNING',
                    'category': 'Target Drift',
                    'message': f"Target distribution ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á (change: {change:.2%})",
                    'details': target_result
                })
        
        # Display alerts
        self._display_alerts(data_name)
        
        return self.alerts
    
    def _display_alerts(self, data_name):
        """‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏• alerts"""
        print("=" * 70)
        print(f"üö® DRIFT ALERTS: {data_name}")
        print("=" * 70)
        
        if not self.alerts:
            print("‚úÖ ‡πÑ‡∏°‡πà‡∏°‡∏µ drift alerts - ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏¢‡∏±‡∏á‡∏Ñ‡∏á‡∏°‡∏µ‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞‡πÉ‡∏Å‡∏•‡πâ‡πÄ‡∏Ñ‡∏µ‡∏¢‡∏á‡∏Å‡∏±‡∏ö reference")
        else:
            print(f"‚ö†Ô∏è ‡∏û‡∏ö {len(self.alerts)} alerts:")
            print()
            
            for i, alert in enumerate(self.alerts, 1):
                icon = "üî¥" if alert['type'] == 'CRITICAL' else "üü°"
                print(f"{icon} Alert #{i}: [{alert['type']}] {alert['category']}")
                print(f"   Message: {alert['message']}")
                
                if isinstance(alert['details'], list):
                    print(f"   Affected: {', '.join(alert['details'][:5])}")
                    if len(alert['details']) > 5:
                        print(f"            ... and {len(alert['details']) - 5} more")
                print()
        
        print("=" * 70)

#%%
# ‡∏ó‡∏î‡∏™‡∏≠‡∏ö Drift Alert System
drift_alert = DriftAlertSystem()
alerts = drift_alert.check_drift_alerts(drift_report, "Current Data")

#%% [markdown]
# ---
# ## üìä Section 4: Building Monitoring Dashboard
# ### ‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á Dashboard ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Monitoring
# 
# **‡∏ó‡∏§‡∏©‡∏é‡∏µ:**
# 
# Monitoring Dashboard ‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢‡πÅ‡∏´‡∏•‡πà‡∏á‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÄ‡∏´‡πá‡∏ô‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏°‡∏Ç‡∏≠‡∏á‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡πÇ‡∏°‡πÄ‡∏î‡∏•
# 
# **‡∏≠‡∏á‡∏Ñ‡πå‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç:**
# 1. Data Quality Summary
# 2. Model Performance Metrics
# 3. Drift Detection Results
# 4. Alert Summary
# 5. Historical Trends

#%% [markdown]
# ### 4.1 ‡∏™‡∏£‡πâ‡∏≤‡∏á Comprehensive Dashboard Class

#%%
class ModelMonitoringDashboard:
    """
    Dashboard ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Model Monitoring
    ‡∏£‡∏ß‡∏° Data Quality, Performance, ‡πÅ‡∏•‡∏∞ Drift Detection
    """
    
    def __init__(self, model, model_name="ML Model"):
        self.model = model
        self.model_name = model_name
        self.reports = {
            'data_quality': {},
            'performance': {},
            'drift': {},
            'alerts': []
        }
        self.history = []
    
    def run_full_monitoring(self, reference_data, current_data, target_col, scaler=None):
        """
        ‡∏£‡∏±‡∏ô monitoring ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
        """
        print("=" * 80)
        print("üñ•Ô∏è  MODEL MONITORING DASHBOARD")
        print(f"   Model: {self.model_name}")
        print(f"   Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print("=" * 80)
        print()
        
        # 1. Data Quality Monitoring
        print("üìä [1/4] Running Data Quality Check...")
        dq_monitor = DataQualityMonitor(current_data, "Current Data")
        self.reports['data_quality'] = {
            'missing': dq_monitor.check_missing_values(),
            'duplicates': dq_monitor.check_duplicates(),
            'outliers': dq_monitor.detect_outliers()
        }
        
        # 2. Performance Monitoring
        print("üìà [2/4] Running Performance Evaluation...")
        if scaler is not None:
            X_ref, y_ref = prepare_features(reference_data, target_col)
            X_curr, y_curr = prepare_features(current_data, target_col)
            
            X_ref_scaled = scaler.transform(X_ref)
            X_curr_scaled = scaler.transform(X_curr)
            
            y_pred_ref = self.model.predict(X_ref_scaled)
            y_pred_curr = self.model.predict(X_curr_scaled)
            
            self.reports['performance'] = {
                'reference': {
                    'accuracy': accuracy_score(y_ref, y_pred_ref),
                    'precision': precision_score(y_ref, y_pred_ref, average='weighted', zero_division=0),
                    'recall': recall_score(y_ref, y_pred_ref, average='weighted', zero_division=0),
                    'f1': f1_score(y_ref, y_pred_ref, average='weighted', zero_division=0)
                },
                'current': {
                    'accuracy': accuracy_score(y_curr, y_pred_curr),
                    'precision': precision_score(y_curr, y_pred_curr, average='weighted', zero_division=0),
                    'recall': recall_score(y_curr, y_pred_curr, average='weighted', zero_division=0),
                    'f1': f1_score(y_curr, y_pred_curr, average='weighted', zero_division=0)
                }
            }
        
        # 3. Drift Detection
        print("üîç [3/4] Running Drift Detection...")
        drift_detector = DriftDetector(reference_data)
        feature_drift = drift_detector.detect_feature_drift(current_data, 
            [col for col in reference_data.columns if col != target_col and col in current_data.columns])
        target_drift = drift_detector.detect_target_drift(current_data, target_col)
        
        self.reports['drift'] = {
            'feature_drift': feature_drift,
            'target_drift': target_drift
        }
        
        # 4. Generate Alerts
        print("üö® [4/4] Generating Alerts...")
        self._generate_alerts()
        
        print()
        print("‚úÖ Monitoring Complete!")
        print()
        
        # Save to history
        self.history.append({
            'timestamp': datetime.now(),
            'reports': self.reports.copy()
        })
        
        return self.reports
    
    def _generate_alerts(self):
        """‡∏™‡∏£‡πâ‡∏≤‡∏á alerts ‡∏à‡∏≤‡∏Å‡∏ú‡∏•‡∏Å‡∏≤‡∏£ monitoring"""
        self.reports['alerts'] = []
        
        # Data Quality Alerts
        dq = self.reports['data_quality']
        if len(dq.get('missing', pd.DataFrame())) > 0:
            self.reports['alerts'].append({
                'type': 'WARNING',
                'category': 'Data Quality',
                'message': '‡∏û‡∏ö missing values ‡πÉ‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•'
            })
        
        dup = dq.get('duplicates', {})
        if dup.get('duplicate_rows', 0) > 0:
            self.reports['alerts'].append({
                'type': 'WARNING',
                'category': 'Data Quality',
                'message': f"‡∏û‡∏ö {dup['duplicate_rows']} duplicate rows"
            })
        
        # Performance Alerts
        perf = self.reports.get('performance', {})
        if perf:
            ref_acc = perf['reference']['accuracy']
            curr_acc = perf['current']['accuracy']
            degradation = (ref_acc - curr_acc) / ref_acc * 100
            
            if degradation > 10:
                self.reports['alerts'].append({
                    'type': 'CRITICAL',
                    'category': 'Performance',
                    'message': f"Accuracy ‡∏•‡∏î‡∏•‡∏á {degradation:.1f}% ‡∏à‡∏≤‡∏Å baseline"
                })
            elif degradation > 5:
                self.reports['alerts'].append({
                    'type': 'WARNING',
                    'category': 'Performance',
                    'message': f"Accuracy ‡∏•‡∏î‡∏•‡∏á {degradation:.1f}% ‡∏à‡∏≤‡∏Å baseline"
                })
        
        # Drift Alerts
        drift = self.reports.get('drift', {})
        if 'target_drift' in drift and drift['target_drift'].get('drift_detected', False):
            self.reports['alerts'].append({
                'type': 'CRITICAL',
                'category': 'Drift',
                'message': '‡∏ï‡∏£‡∏ß‡∏à‡∏û‡∏ö Target Drift'
            })
        
        if 'feature_drift' in drift:
            drifted = drift['feature_drift'][drift['feature_drift']['drift_detected'] == True]
            if len(drifted) > 0:
                self.reports['alerts'].append({
                    'type': 'WARNING',
                    'category': 'Drift',
                    'message': f"‡∏û‡∏ö Feature Drift ‡πÉ‡∏ô {len(drifted)} features"
                })
    
    def display_dashboard(self):
        """‡πÅ‡∏™‡∏î‡∏á Dashboard"""
        print()
        print("‚ïî" + "‚ïê" * 78 + "‚ïó")
        print("‚ïë" + " " * 25 + "üìä MONITORING DASHBOARD" + " " * 30 + "‚ïë")
        print("‚ï†" + "‚ïê" * 78 + "‚ï£")
        
        # Overall Status
        n_critical = sum(1 for a in self.reports['alerts'] if a['type'] == 'CRITICAL')
        n_warning = sum(1 for a in self.reports['alerts'] if a['type'] == 'WARNING')
        
        if n_critical > 0:
            status = "üî¥ CRITICAL"
            status_color = "Action Required!"
        elif n_warning > 0:
            status = "üü° WARNING"
            status_color = "Attention Needed"
        else:
            status = "üü¢ HEALTHY"
            status_color = "All Systems Normal"
        
        print(f"‚ïë  Overall Status: {status} - {status_color}")
        print(f"‚ïë  Last Updated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print("‚ï†" + "‚ïê" * 78 + "‚ï£")
        
        # Performance Summary
        print("‚ïë  üìà PERFORMANCE SUMMARY")
        print("‚ïë  " + "-" * 40)
        perf = self.reports.get('performance', {})
        if perf:
            print(f"‚ïë       Metric      ‚îÇ Reference ‚îÇ Current ‚îÇ Change")
            print("‚ïë  " + "-" * 50)
            for metric in ['accuracy', 'precision', 'recall', 'f1']:
                ref_val = perf['reference'].get(metric, 0)
                curr_val = perf['current'].get(metric, 0)
                change = curr_val - ref_val
                arrow = "‚Üë" if change > 0 else "‚Üì" if change < 0 else "‚Üí"
                print(f"‚ïë    {metric:12} ‚îÇ  {ref_val:.4f}  ‚îÇ {curr_val:.4f} ‚îÇ {arrow} {abs(change):.4f}")
        print("‚ïë")
        
        # Alerts Summary
        print("‚ï†" + "‚ïê" * 78 + "‚ï£")
        print("‚ïë  üö® ALERTS SUMMARY")
        print("‚ïë  " + "-" * 40)
        
        if not self.reports['alerts']:
            print("‚ïë    ‚úÖ No alerts - All metrics within acceptable range")
        else:
            for alert in self.reports['alerts']:
                icon = "üî¥" if alert['type'] == 'CRITICAL' else "üü°"
                print(f"‚ïë    {icon} [{alert['type']}] {alert['category']}: {alert['message']}")
        
        print("‚ïë")
        print("‚ïö" + "‚ïê" * 78 + "‚ïù")
    
    def plot_dashboard(self):
        """‡∏™‡∏£‡πâ‡∏≤‡∏á Visual Dashboard"""
        fig = plt.figure(figsize=(16, 12))
        fig.suptitle(f'üìä Model Monitoring Dashboard: {self.model_name}', 
                     fontsize=16, fontweight='bold')
        
        # Create grid
        gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)
        
        # 1. Overall Status (top left)
        ax1 = fig.add_subplot(gs[0, 0])
        n_critical = sum(1 for a in self.reports['alerts'] if a['type'] == 'CRITICAL')
        n_warning = sum(1 for a in self.reports['alerts'] if a['type'] == 'WARNING')
        
        if n_critical > 0:
            status_color = 'red'
            status_text = 'CRITICAL'
        elif n_warning > 0:
            status_color = 'orange'
            status_text = 'WARNING'
        else:
            status_color = 'green'
            status_text = 'HEALTHY'
        
        ax1.pie([1], colors=[status_color], startangle=90)
        circle = plt.Circle((0, 0), 0.7, fc='white')
        ax1.add_patch(circle)
        ax1.text(0, 0, status_text, ha='center', va='center', fontsize=14, fontweight='bold')
        ax1.set_title('System Status')
        
        # 2. Performance Comparison (top middle and right)
        ax2 = fig.add_subplot(gs[0, 1:])
        perf = self.reports.get('performance', {})
        if perf:
            metrics = ['accuracy', 'precision', 'recall', 'f1']
            ref_values = [perf['reference'].get(m, 0) for m in metrics]
            curr_values = [perf['current'].get(m, 0) for m in metrics]
            
            x = np.arange(len(metrics))
            width = 0.35
            
            ax2.bar(x - width/2, ref_values, width, label='Reference', color='steelblue', alpha=0.8)
            ax2.bar(x + width/2, curr_values, width, label='Current', color='coral', alpha=0.8)
            ax2.set_xticks(x)
            ax2.set_xticklabels([m.capitalize() for m in metrics])
            ax2.set_ylabel('Score')
            ax2.set_title('Performance Metrics Comparison')
            ax2.legend()
            ax2.set_ylim([0, 1.1])
            ax2.axhline(y=0.8, color='red', linestyle='--', alpha=0.5)
        
        # 3. Data Quality (middle left)
        ax3 = fig.add_subplot(gs[1, 0])
        dq = self.reports['data_quality']
        
        quality_scores = []
        quality_labels = []
        
        # Missing score
        missing_df = dq.get('missing', pd.DataFrame())
        missing_score = 100 if len(missing_df) == 0 else max(0, 100 - len(missing_df) * 10)
        quality_scores.append(missing_score)
        quality_labels.append('Missing\nValues')
        
        # Duplicate score
        dup = dq.get('duplicates', {})
        dup_pct = dup.get('duplicate_percentage', 0)
        dup_score = max(0, 100 - dup_pct * 10)
        quality_scores.append(dup_score)
        quality_labels.append('Duplicates')
        
        # Outlier score
        outlier_df = dq.get('outliers', pd.DataFrame())
        if len(outlier_df) > 0:
            avg_outlier_pct = outlier_df['outlier_percentage'].mean()
            outlier_score = max(0, 100 - avg_outlier_pct * 5)
        else:
            outlier_score = 100
        quality_scores.append(outlier_score)
        quality_labels.append('Outliers')
        
        colors = ['green' if s >= 80 else 'orange' if s >= 60 else 'red' for s in quality_scores]
        ax3.barh(quality_labels, quality_scores, color=colors, alpha=0.7)
        ax3.set_xlim([0, 100])
        ax3.set_title('Data Quality Scores')
        ax3.axvline(x=80, color='green', linestyle='--', alpha=0.5)
        
        # 4. Drift Detection Summary (middle center and right)
        ax4 = fig.add_subplot(gs[1, 1:])
        drift = self.reports.get('drift', {})
        
        if 'feature_drift' in drift:
            feature_df = drift['feature_drift']
            n_features = len(feature_df)
            n_drifted = feature_df['drift_detected'].sum()
            
            sizes = [n_drifted, n_features - n_drifted]
            labels = [f'Drifted ({n_drifted})', f'Stable ({n_features - n_drifted})']
            colors_pie = ['coral', 'steelblue']
            explode = (0.05, 0)
            
            ax4.pie(sizes, labels=labels, colors=colors_pie, autopct='%1.1f%%', 
                   explode=explode, startangle=90)
            ax4.set_title('Feature Drift Summary')
        
        # 5. Alerts List (bottom)
        ax5 = fig.add_subplot(gs[2, :])
        ax5.axis('off')
        
        alerts_text = "üö® ALERTS:\n" + "-" * 50 + "\n"
        if not self.reports['alerts']:
            alerts_text += "‚úÖ No alerts - All systems operating normally"
        else:
            for alert in self.reports['alerts']:
                icon = "üî¥" if alert['type'] == 'CRITICAL' else "üü°"
                alerts_text += f"{icon} [{alert['type']}] {alert['category']}: {alert['message']}\n"
        
        ax5.text(0.1, 0.5, alerts_text, transform=ax5.transAxes,
                fontsize=11, verticalalignment='center', fontfamily='monospace',
                bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.8))
        
        plt.tight_layout()
        plt.show()
    
    def export_report(self, filename=None):
        """Export report ‡πÄ‡∏õ‡πá‡∏ô dictionary"""
        if filename is None:
            filename = f"monitoring_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        report = {
            'model_name': self.model_name,
            'timestamp': datetime.now().isoformat(),
            'data_quality': {
                'missing_columns': len(self.reports['data_quality'].get('missing', pd.DataFrame())),
                'duplicates': self.reports['data_quality'].get('duplicates', {})
            },
            'performance': self.reports.get('performance', {}),
            'drift': {
                'target_drift_detected': self.reports.get('drift', {}).get('target_drift', {}).get('drift_detected', False),
                'feature_drift_count': len(self.reports.get('drift', {}).get('feature_drift', pd.DataFrame())[
                    self.reports.get('drift', {}).get('feature_drift', pd.DataFrame()).get('drift_detected', False) == True
                ]) if 'feature_drift' in self.reports.get('drift', {}) else 0
            },
            'alerts': self.reports['alerts'],
            'overall_status': 'CRITICAL' if any(a['type'] == 'CRITICAL' for a in self.reports['alerts']) 
                             else 'WARNING' if any(a['type'] == 'WARNING' for a in self.reports['alerts'])
                             else 'HEALTHY'
        }
        
        print(f"üìÑ Report exported: {filename}")
        return report

#%%
# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏•‡∏∞‡∏£‡∏±‡∏ô Dashboard
print("üñ•Ô∏è ‡∏™‡∏£‡πâ‡∏≤‡∏á Monitoring Dashboard...")
dashboard = ModelMonitoringDashboard(rf_model, "Credit Risk Random Forest")

# ‡∏£‡∏±‡∏ô Full Monitoring
reports = dashboard.run_full_monitoring(
    reference_data=reference_data,
    current_data=current_data,
    target_col='default',
    scaler=scaler
)

#%%
# ‡πÅ‡∏™‡∏î‡∏á Text Dashboard
dashboard.display_dashboard()

#%%
# ‡πÅ‡∏™‡∏î‡∏á Visual Dashboard
dashboard.plot_dashboard()

#%%
# Export Report
exported_report = dashboard.export_report()
print("\nüìã Exported Report Summary:")
for key, value in exported_report.items():
    if key != 'alerts':
        print(f"   {key}: {value}")

#%% [markdown]
# ---
# ## üéì Section 5: ‡∏™‡∏£‡∏∏‡∏õ‡πÅ‡∏•‡∏∞‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥
# ### Summary and Best Practices

#%% [markdown]
# ### 5.1 ‡∏™‡∏£‡∏∏‡∏õ‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ
# 
# ‡πÉ‡∏ô LAB ‡∏ô‡∏µ‡πâ ‡πÄ‡∏£‡∏≤‡πÑ‡∏î‡πâ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö Model Monitoring ‡∏î‡∏±‡∏á‡∏ô‡∏µ‡πâ:
# 
# #### 1. Data Quality Monitoring
# - ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö missing values, duplicates, outliers
# - ‡∏™‡∏£‡πâ‡∏≤‡∏á quality scores ‡πÅ‡∏•‡∏∞ alerts
# - ‡πÉ‡∏ä‡πâ threshold-based monitoring
# 
# #### 2. Model Performance Tracking
# - ‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏° classification metrics (Accuracy, Precision, Recall, F1)
# - ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö baseline vs current performance
# - ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö performance degradation
# 
# #### 3. Drift Detection
# - ‡πÉ‡∏ä‡πâ statistical tests (KS, Chi-square, PSI)
# - ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö feature drift ‡πÅ‡∏•‡∏∞ target drift
# - ‡∏™‡∏£‡πâ‡∏≤‡∏á prediction drift analysis
# 
# #### 4. Monitoring Dashboard
# - ‡∏£‡∏ß‡∏° metrics ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÉ‡∏ô single view
# - ‡∏™‡∏£‡πâ‡∏≤‡∏á alerts ‡πÅ‡∏•‡∏∞ recommendations
# - Export reports ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö stakeholders

#%% [markdown]
# ### 5.2 Best Practices ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Model Monitoring
# 
# 1. **‡∏Å‡∏≥‡∏´‡∏ô‡∏î Baseline ‡∏ó‡∏µ‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô**
#    - ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å performance metrics ‡πÄ‡∏°‡∏∑‡πà‡∏≠ deploy
#    - ‡πÄ‡∏Å‡πá‡∏ö reference data distribution
# 
# 2. **Monitor ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏™‡∏°‡πà‡∏≥‡πÄ‡∏™‡∏°‡∏≠**
#    - ‡∏ï‡∏±‡πâ‡∏á schedule ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö monitoring (‡∏£‡∏≤‡∏¢‡∏ß‡∏±‡∏ô/‡∏£‡∏≤‡∏¢‡∏™‡∏±‡∏õ‡∏î‡∏≤‡∏´‡πå)
#    - Automate monitoring pipeline
# 
# 3. **‡∏ï‡∏±‡πâ‡∏á Threshold ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°**
#    - ‡πÑ‡∏°‡πà sensitive ‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ (false alarms)
#    - ‡πÑ‡∏°‡πà loose ‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ (miss real issues)
# 
# 4. **‡∏°‡∏µ Action Plan**
#    - ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏ß‡πà‡∏≤‡∏à‡∏∞‡∏ó‡∏≥‡∏≠‡∏∞‡πÑ‡∏£‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÄ‡∏Å‡∏¥‡∏î alert
#    - Retrain strategy ‡πÄ‡∏°‡∏∑‡πà‡∏≠ performance ‡∏ï‡πà‡∏≥‡∏Å‡∏ß‡πà‡∏≤‡πÄ‡∏Å‡∏ì‡∏ë‡πå
# 
# 5. **Document Everything**
#    - ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å monitoring results
#    - Track model versions ‡πÅ‡∏•‡∏∞ changes

#%%
# ‡∏™‡∏£‡∏∏‡∏õ Code ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Quick Monitoring
def quick_model_monitor(model, scaler, reference_data, current_data, target_col):
    """
    ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö quick monitoring
    ‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏£‡∏ß‡∏î‡πÄ‡∏£‡πá‡∏ß
    """
    print("üöÄ Quick Model Monitoring")
    print("=" * 50)
    
    # Prepare data
    X_ref, y_ref = prepare_features(reference_data, target_col)
    X_curr, y_curr = prepare_features(current_data, target_col)
    
    X_ref_scaled = scaler.transform(X_ref)
    X_curr_scaled = scaler.transform(X_curr)
    
    # Predictions
    y_pred_ref = model.predict(X_ref_scaled)
    y_pred_curr = model.predict(X_curr_scaled)
    
    # Calculate metrics
    ref_acc = accuracy_score(y_ref, y_pred_ref)
    curr_acc = accuracy_score(y_curr, y_pred_curr)
    degradation = (ref_acc - curr_acc) / ref_acc * 100
    
    # Target drift
    ref_rate = y_ref.mean()
    curr_rate = y_curr.mean()
    rate_change = abs(curr_rate - ref_rate)
    
    # Print summary
    print(f"üìä Reference Accuracy: {ref_acc:.4f}")
    print(f"üìä Current Accuracy:   {curr_acc:.4f}")
    print(f"üìâ Degradation:        {degradation:.2f}%")
    print()
    print(f"üéØ Reference Target Rate: {ref_rate:.2%}")
    print(f"üéØ Current Target Rate:   {curr_rate:.2%}")
    print(f"üìà Rate Change:           {rate_change:.2%}")
    print()
    
    # Status
    if degradation > 10 or rate_change > 0.1:
        print("üî¥ Status: CRITICAL - Consider retraining!")
    elif degradation > 5 or rate_change > 0.05:
        print("üü° Status: WARNING - Monitor closely")
    else:
        print("üü¢ Status: HEALTHY - Model performing well")
    
    print("=" * 50)
    
    return {
        'reference_accuracy': ref_acc,
        'current_accuracy': curr_acc,
        'degradation_pct': degradation,
        'reference_target_rate': ref_rate,
        'current_target_rate': curr_rate,
        'rate_change': rate_change
    }

#%%
# ‡∏ó‡∏î‡∏™‡∏≠‡∏ö Quick Monitor
print("\nüìä ‡∏ó‡∏î‡∏™‡∏≠‡∏ö Quick Model Monitor:")
quick_results = quick_model_monitor(rf_model, scaler, reference_data, current_data, 'default')

#%% [markdown]
# ### 5.3 ‡πÅ‡∏ö‡∏ö‡∏ù‡∏∂‡∏Å‡∏´‡∏±‡∏î (Exercises)
# 
# ‡∏•‡∏≠‡∏á‡∏ó‡∏≥‡πÅ‡∏ö‡∏ö‡∏ù‡∏∂‡∏Å‡∏´‡∏±‡∏î‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏ô‡∏µ‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à:
# 
# 1. **Exercise 1**: ‡πÄ‡∏û‡∏¥‡πà‡∏° drift level ‡πÄ‡∏õ‡πá‡∏ô 0.5 ‡πÅ‡∏•‡∏∞ 0.7 ‡πÅ‡∏•‡πâ‡∏ß‡∏î‡∏π‡∏ú‡∏•‡∏Å‡∏£‡∏∞‡∏ó‡∏ö‡∏ï‡πà‡∏≠ performance
# 
# 2. **Exercise 2**: ‡∏õ‡∏£‡∏±‡∏ö threshold ‡∏Ç‡∏≠‡∏á alerts ‡πÅ‡∏•‡∏∞‡∏î‡∏π‡∏ß‡πà‡∏≤ alerts ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£
# 
# 3. **Exercise 3**: ‡πÄ‡∏û‡∏¥‡πà‡∏° metric ‡πÉ‡∏´‡∏°‡πà ‡πÄ‡∏ä‡πà‡∏ô AUC-ROC ‡πÉ‡∏ô performance monitoring
# 
# 4. **Exercise 4**: ‡∏™‡∏£‡πâ‡∏≤‡∏á monitoring ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö regression model ‡πÅ‡∏ó‡∏ô classification

#%%
# Exercise 1: ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏±‡∏ö drift level ‡∏ó‡∏µ‡πà‡∏™‡∏π‡∏á‡∏Ç‡∏∂‡πâ‡∏ô
print("üìù Exercise 1: ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏±‡∏ö High Drift Data")
print()

high_drift_data = create_credit_data(n_samples=1000, seed=456, drift_level=0.7)
print("High Drift Data (drift_level=0.7):")
quick_results_high = quick_model_monitor(rf_model, scaler, reference_data, high_drift_data, 'default')

#%% [markdown]
# ---
# ## üéâ ‡∏à‡∏ö LAB: Model Monitoring with Scikit-Learn
# 
# ### ‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ:
# 
# 1. ‚úÖ Data Quality Monitoring
#    - ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö missing values, duplicates, outliers
#    - ‡∏™‡∏£‡πâ‡∏≤‡∏á quality reports ‡πÅ‡∏•‡∏∞ alerts
# 
# 2. ‚úÖ Model Performance Tracking
#    - ‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏° classification/regression metrics
#    - ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö reference vs current performance
# 
# 3. ‚úÖ Drift Detection
#    - ‡πÉ‡∏ä‡πâ statistical tests (KS, Chi-square, PSI)
#    - ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö feature drift ‡πÅ‡∏•‡∏∞ target drift
# 
# 4. ‚úÖ Monitoring Dashboard
#    - ‡∏™‡∏£‡πâ‡∏≤‡∏á comprehensive dashboard
#    - Generate alerts ‡πÅ‡∏•‡∏∞ reports
# 
# ### ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ñ‡∏±‡∏î‡πÑ‡∏õ:
# 
# - ‡∏ô‡∏≥ monitoring ‡πÑ‡∏õ‡πÉ‡∏ä‡πâ‡∏Å‡∏±‡∏ö production model
# - ‡∏ï‡∏±‡πâ‡∏á automated monitoring pipeline
# - ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏Å‡∏±‡∏ö alerting system (email, Slack)
# - ‡∏™‡∏£‡πâ‡∏≤‡∏á retraining pipeline ‡πÄ‡∏°‡∏∑‡πà‡∏≠ performance ‡∏ï‡πà‡∏≥

#%%
print("üéâ ‡∏à‡∏ö LAB: Model Monitoring with Scikit-Learn")
print()
print("‡∏Ç‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÑ‡∏õ‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏±‡∏ô!")
print("Happy Monitoring! üöÄ")