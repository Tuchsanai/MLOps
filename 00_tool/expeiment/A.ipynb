{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d3e7a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "response = {\n",
    "  \"type\": \"message\",\n",
    "  \"role\": \"assistant\",\n",
    "  \"content\": [\n",
    "    {\n",
    "      \"type\": \"text\",\n",
    "      \"text\": \"# Complete MLOps Labs: Understanding Data Drift Concepts with sklearn\\n\\n## LAB 1: Understanding Data Drift Concepts\\n\\n```python\\n#%% [markdown]\\n# # LAB 1: Understanding Data Drift Concepts\\n# ## ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡πÅ‡∏ô‡∏ß‡∏Ñ‡∏¥‡∏î‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á Data Drift\\n# \\n# ### ‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ:\\n# 1. ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á Covariate Shift ‡πÅ‡∏•‡∏∞ Concept Drift\\n# 2. ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ Statistical tests ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö drift detection (KS, PSI, Wasserstein)\\n# 3. ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å drift detection method ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡∏Å‡∏±‡∏ö‡∏™‡∏ñ‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏ì‡πå‡∏ï‡πà‡∏≤‡∏á‡πÜ\\n#\\n# ### ‡∏ó‡∏§‡∏©‡∏é‡∏µ‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô:\\n# **Data Drift** ‡∏Ñ‡∏∑‡∏≠ ‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÄ‡∏ß‡∏•‡∏≤‡∏ú‡πà‡∏≤‡∏ô‡πÑ‡∏õ ‡∏ã‡∏∂‡πà‡∏á‡∏≠‡∏≤‡∏à‡∏™‡πà‡∏á‡∏ú‡∏•‡∏Å‡∏£‡∏∞‡∏ó‡∏ö‡∏ï‡πà‡∏≠‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡∏Ç‡∏≠‡∏á ML Model\\n#\\n# ‡∏°‡∏µ 2 ‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏´‡∏•‡∏±‡∏Å:\\n# - **Covariate Shift**: ‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡∏≠‡∏á input features P(X) ‡πÇ‡∏î‡∏¢‡∏ó‡∏µ‡πà‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå P(Y|X) ‡∏¢‡∏±‡∏á‡∏Ñ‡∏á‡πÄ‡∏î‡∏¥‡∏°\\n# - **Concept Drift**: ‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á input ‡πÅ‡∏•‡∏∞ output P(Y|X)\\n\\n#%% [markdown]\\n# ## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 1: ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° Environment ‡πÅ‡∏•‡∏∞ Import Libraries\\n# \\n# ‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô ‡πÄ‡∏£‡∏≤‡∏à‡∏∞ import libraries ‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î\\n\\n#%%\\n# Import libraries ‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô\\nimport numpy as np\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\nfrom scipy import stats\\nfrom sklearn.datasets import make_classification\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.metrics import accuracy_score, classification_report\\nimport warnings\\nwarnings.filterwarnings('ignore')\\n\\n# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ random seed ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ó‡∏≥‡∏ã‡πâ‡∏≥‡πÑ‡∏î‡πâ\\nnp.random.seed(42)\\n\\n# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ matplotlib ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢\\nplt.rcParams['figure.figsize'] = (12, 6)\\nplt.rcParams['font.size'] = 10\\n\\nprint(\\\"‚úÖ Libraries imported successfully!\\\")\\nprint(f\\\"NumPy version: {np.__version__}\\\")\\nprint(f\\\"Pandas version: {pd.__version__}\\\")\\n\\n#%% [markdown]\\n# ## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 2: ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à Covariate Shift\\n# \\n# ### ‡∏ó‡∏§‡∏©‡∏é‡∏µ Covariate Shift:\\n# - ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡∏∂‡πâ‡∏ô‡πÄ‡∏°‡∏∑‡πà‡∏≠ distribution ‡∏Ç‡∏≠‡∏á input features ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á\\n# - ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏£‡∏≤‡∏Ñ‡∏≤‡∏ö‡πâ‡∏≤‡∏ô‡∏ó‡∏µ‡πà train ‡∏Å‡∏±‡∏ö‡∏ö‡πâ‡∏≤‡∏ô‡πÉ‡∏ô‡πÄ‡∏°‡∏∑‡∏≠‡∏á ‡πÅ‡∏ï‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ö‡πâ‡∏≤‡∏ô‡πÉ‡∏ô‡∏ä‡∏ô‡∏ö‡∏ó\\n# - ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á features ‡∏Å‡∏±‡∏ö target ‡∏¢‡∏±‡∏á‡∏Ñ‡∏á‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°\\n#\\n# ‡∏™‡∏π‡∏ï‡∏£‡∏ó‡∏≤‡∏á‡∏Ñ‡∏ì‡∏¥‡∏ï‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå:\\n# - Training: P_train(X) ‚â† P_test(X)\\n# - ‡πÅ‡∏ï‡πà: P(Y|X) ‡∏Ñ‡∏á‡∏ó‡∏µ‡πà\\n\\n#%%\\ndef generate_covariate_shift_data():\\n    \\\"\\\"\\\"\\n    ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÅ‡∏™‡∏î‡∏á Covariate Shift\\n    \\n    ‡πÉ‡∏ô‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡∏µ‡πâ:\\n    - Training data: ‡∏≠‡∏≤‡∏¢‡∏∏‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤ 20-40 ‡∏õ‡∏µ\\n    - Production data: ‡∏≠‡∏≤‡∏¢‡∏∏‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤ 40-60 ‡∏õ‡∏µ\\n    - ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏≠‡∏≤‡∏¢‡∏∏‡∏Å‡∏±‡∏ö‡∏û‡∏§‡∏ï‡∏¥‡∏Å‡∏£‡∏£‡∏°‡∏Å‡∏≤‡∏£‡∏ã‡∏∑‡πâ‡∏≠‡∏¢‡∏±‡∏á‡∏Ñ‡∏á‡πÄ‡∏î‡∏¥‡∏°\\n    \\\"\\\"\\\"\\n    \\n    # Training data: ‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤‡∏≠‡∏≤‡∏¢‡∏∏‡∏ô‡πâ‡∏≠‡∏¢ (20-40)\\n    np.random.seed(42)\\n    n_train = 1000\\n    age_train = np.random.normal(30, 5, n_train)\\n    income_train = age_train * 1500 + np.random.normal(0, 5000, n_train)\\n    \\n    # ‡∏Å‡∏é‡∏Å‡∏≤‡∏£‡∏ã‡∏∑‡πâ‡∏≠: ‡∏ã‡∏∑‡πâ‡∏≠‡∏ñ‡πâ‡∏≤ income > 40000 + age * 500\\n    threshold_train = 40000 + age_train * 500\\n    purchase_train = (income_train > threshold_train).astype(int)\\n    \\n    train_df = pd.DataFrame({\\n        'age': age_train,\\n        'income': income_train,\\n        'purchase': purchase_train\\n    })\\n    \\n    # Production data: ‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤‡∏≠‡∏≤‡∏¢‡∏∏‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô (40-60) - Covariate Shift!\\n    n_prod = 1000\\n    age_prod = np.random.normal(50, 5, n_prod)  # ‡∏≠‡∏≤‡∏¢‡∏∏‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô!\\n    income_prod = age_prod * 1500 + np.random.normal(0, 5000, n_prod)\\n    \\n    # ‡∏Å‡∏é‡∏Å‡∏≤‡∏£‡∏ã‡∏∑‡πâ‡∏≠‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏° (‡πÑ‡∏°‡πà‡∏°‡∏µ Concept Drift)\\n    threshold_prod = 40000 + age_prod * 500\\n    purchase_prod = (income_prod > threshold_prod).astype(int)\\n    \\n    prod_df = pd.DataFrame({\\n        'age': age_prod,\\n        'income': income_prod,\\n        'purchase': purchase_prod\\n    })\\n    \\n    return train_df, prod_df\\n\\n# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•\\ntrain_covariate, prod_covariate = generate_covariate_shift_data()\\n\\nprint(\\\"üìä Training Data Summary:\\\")\\nprint(train_covariate.describe())\\nprint(\\\"\\\\nüìä Production Data Summary:\\\")\\nprint(prod_covariate.describe())\\n\\n#%%\\n# Visualize Covariate Shift\\nfig, axes = plt.subplots(1, 3, figsize=(15, 4))\\n\\n# Plot 1: Age Distribution\\naxes[0].hist(train_covariate['age'], bins=30, alpha=0.7, label='Training', color='blue')\\naxes[0].hist(prod_covariate['age'], bins=30, alpha=0.7, label='Production', color='red')\\naxes[0].set_xlabel('Age')\\naxes[0].set_ylabel('Frequency')\\naxes[0].set_title('Covariate Shift: Age Distribution\\\\n(Training vs Production)')\\naxes[0].legend()\\naxes[0].axvline(train_covariate['age'].mean(), color='blue', linestyle='--', label='Train Mean')\\naxes[0].axvline(prod_covariate['age'].mean(), color='red', linestyle='--', label='Prod Mean')\\n\\n# Plot 2: Income Distribution\\naxes[1].hist(train_covariate['income'], bins=30, alpha=0.7, label='Training', color='blue')\\naxes[1].hist(prod_covariate['income'], bins=30, alpha=0.7, label='Production', color='red')\\naxes[1].set_xlabel('Income')\\naxes[1].set_ylabel('Frequency')\\naxes[1].set_title('Covariate Shift: Income Distribution\\\\n(Training vs Production)')\\naxes[1].legend()\\n\\n# Plot 3: Relationship P(Y|X) ‡∏¢‡∏±‡∏á‡∏Ñ‡∏á‡πÄ‡∏î‡∏¥‡∏°\\naxes[2].scatter(train_covariate['age'], train_covariate['income'], \\n                c=train_covariate['purchase'], alpha=0.3, cmap='coolwarm', label='Training')\\naxes[2].set_xlabel('Age')\\naxes[2].set_ylabel('Income')\\naxes[2].set_title('P(Y|X) Relationship\\\\n(Decision boundary ‡∏¢‡∏±‡∏á‡∏Ñ‡∏á‡πÄ‡∏î‡∏¥‡∏°)')\\n\\nplt.tight_layout()\\nplt.savefig('covariate_shift_visualization.png', dpi=150, bbox_inches='tight')\\nplt.show()\\n\\nprint(\\\"\\\\nüí° ‡∏™‡∏±‡∏á‡πÄ‡∏Å‡∏ï: Distribution ‡∏Ç‡∏≠‡∏á Age ‡πÅ‡∏•‡∏∞ Income ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÑ‡∏õ ‡πÅ‡∏ï‡πà‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à‡∏ã‡∏∑‡πâ‡∏≠‡∏¢‡∏±‡∏á‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°\\\")\\n\\n#%% [markdown]\\n# ## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 3: ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à Concept Drift\\n# \\n# ### ‡∏ó‡∏§‡∏©‡∏é‡∏µ Concept Drift:\\n# - ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡∏∂‡πâ‡∏ô‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á input ‡πÅ‡∏•‡∏∞ output ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á\\n# - ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: ‡∏û‡∏§‡∏ï‡∏¥‡∏Å‡∏£‡∏£‡∏°‡∏Å‡∏≤‡∏£‡∏ã‡∏∑‡πâ‡∏≠‡∏Ç‡∏≠‡∏á‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏´‡∏•‡∏±‡∏á COVID-19\\n# - ‡πÅ‡∏°‡πâ input distribution ‡∏à‡∏∞‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏° ‡πÅ‡∏ï‡πà output ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô\\n#\\n# ‡∏™‡∏π‡∏ï‡∏£‡∏ó‡∏≤‡∏á‡∏Ñ‡∏ì‡∏¥‡∏ï‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå:\\n# - P(X) ‡∏≠‡∏≤‡∏à‡∏Ñ‡∏á‡∏ó‡∏µ‡πà‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏Å‡πá‡πÑ‡∏î‡πâ\\n# - ‡πÅ‡∏ï‡πà: P(Y|X) ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á\\n\\n#%%\\ndef generate_concept_drift_data():\\n    \\\"\\\"\\\"\\n    ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÅ‡∏™‡∏î‡∏á Concept Drift\\n    \\n    ‡πÉ‡∏ô‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡∏µ‡πâ:\\n    - Training data: ‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤‡∏ã‡∏∑‡πâ‡∏≠‡πÄ‡∏°‡∏∑‡πà‡∏≠ income ‡∏™‡∏π‡∏á‡∏Å‡∏ß‡πà‡∏≤ threshold\\n    - Production data: threshold ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô (‡∏û‡∏§‡∏ï‡∏¥‡∏Å‡∏£‡∏£‡∏°‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô)\\n    \\\"\\\"\\\"\\n    \\n    np.random.seed(42)\\n    n_samples = 1000\\n    \\n    # Training data\\n    age_train = np.random.normal(35, 10, n_samples)\\n    income_train = np.random.normal(50000, 15000, n_samples)\\n    \\n    # ‡∏Å‡∏é‡∏Å‡∏≤‡∏£‡∏ã‡∏∑‡πâ‡∏≠‡πÄ‡∏î‡∏¥‡∏°: ‡∏ã‡∏∑‡πâ‡∏≠‡∏ñ‡πâ‡∏≤ income > 45000\\n    purchase_train = (income_train > 45000).astype(int)\\n    \\n    train_df = pd.DataFrame({\\n        'age': age_train,\\n        'income': income_train,\\n        'purchase': purchase_train\\n    })\\n    \\n    # Production data: distribution ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°\\n    age_prod = np.random.normal(35, 10, n_samples)\\n    income_prod = np.random.normal(50000, 15000, n_samples)\\n    \\n    # ‡∏Å‡∏é‡∏Å‡∏≤‡∏£‡∏ã‡∏∑‡πâ‡∏≠‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô: ‡∏ã‡∏∑‡πâ‡∏≠‡∏ñ‡πâ‡∏≤ income > 55000 (Concept Drift!)\\n    # ‡∏≠‡∏≤‡∏à‡πÄ‡∏Å‡∏¥‡∏î‡∏à‡∏≤‡∏Å‡πÄ‡∏®‡∏£‡∏©‡∏ê‡∏Å‡∏¥‡∏à‡∏ï‡∏Å‡∏ï‡πà‡∏≥ ‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏£‡∏≤‡∏¢‡πÑ‡∏î‡πâ‡∏™‡∏π‡∏á‡∏Ç‡∏∂‡πâ‡∏ô‡∏ñ‡∏∂‡∏á‡∏à‡∏∞‡∏ã‡∏∑‡πâ‡∏≠\\n    purchase_prod = (income_prod > 55000).astype(int)\\n    \\n    prod_df = pd.DataFrame({\\n        'age': age_prod,\\n        'income': income_prod,\\n        'purchase': purchase_prod\\n    })\\n    \\n    return train_df, prod_df\\n\\n# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•\\ntrain_concept, prod_concept = generate_concept_drift_data()\\n\\nprint(\\\"üìä Training Data Summary:\\\")\\nprint(f\\\"Purchase Rate: {train_concept['purchase'].mean():.2%}\\\")\\nprint(train_concept.describe())\\n\\nprint(\\\"\\\\nüìä Production Data Summary:\\\")\\nprint(f\\\"Purchase Rate: {prod_concept['purchase'].mean():.2%}\\\")\\nprint(prod_concept.describe())\\n\\n#%%\\n# Visualize Concept Drift\\nfig, axes = plt.subplots(1, 3, figsize=(15, 4))\\n\\n# Plot 1: Income Distribution (‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Å‡∏±‡∏ô)\\naxes[0].hist(train_concept['income'], bins=30, alpha=0.7, label='Training', color='blue')\\naxes[0].hist(prod_concept['income'], bins=30, alpha=0.7, label='Production', color='red')\\naxes[0].set_xlabel('Income')\\naxes[0].set_ylabel('Frequency')\\naxes[0].set_title('No Covariate Shift: Income Distribution\\\\n(Training ‚âà Production)')\\naxes[0].legend()\\n\\n# Plot 2: Purchase Rate by Income Bin\\nincome_bins = np.linspace(20000, 80000, 10)\\n\\ntrain_purchase_rate = []\\nprod_purchase_rate = []\\nbin_centers = []\\n\\nfor i in range(len(income_bins)-1):\\n    mask_train = (train_concept['income'] >= income_bins[i]) & (train_concept['income'] < income_bins[i+1])\\n    mask_prod = (prod_concept['income'] >= income_bins[i]) & (prod_concept['income'] < income_bins[i+1])\\n    \\n    if mask_train.sum() > 0:\\n        train_purchase_rate.append(train_concept[mask_train]['purchase'].mean())\\n    else:\\n        train_purchase_rate.append(0)\\n        \\n    if mask_prod.sum() > 0:\\n        prod_purchase_rate.append(prod_concept[mask_prod]['purchase'].mean())\\n    else:\\n        prod_purchase_rate.append(0)\\n    \\n    bin_centers.append((income_bins[i] + income_bins[i+1]) / 2)\\n\\naxes[1].plot(bin_centers, train_purchase_rate, 'b-o', label='Training P(Y|X)', linewidth=2)\\naxes[1].plot(bin_centers, prod_purchase_rate, 'r-o', label='Production P(Y|X)', linewidth=2)\\naxes[1].set_xlabel('Income')\\naxes[1].set_ylabel('Purchase Probability')\\naxes[1].set_title('Concept Drift: P(Y|X) Changed!\\\\n(Decision boundary moved)')\\naxes[1].legend()\\naxes[1].axhline(0.5, color='gray', linestyle='--', alpha=0.5)\\n\\n# Plot 3: Decision Boundary Comparison\\naxes[2].scatter(train_concept['income'], train_concept['purchase'] + np.random.normal(0, 0.05, len(train_concept)), \\n                alpha=0.3, color='blue', label='Training')\\naxes[2].scatter(prod_concept['income'], prod_concept['purchase'] + np.random.normal(0, 0.05, len(prod_concept)), \\n                alpha=0.3, color='red', label='Production')\\naxes[2].axvline(45000, color='blue', linestyle='--', linewidth=2, label='Train Threshold (45k)')\\naxes[2].axvline(55000, color='red', linestyle='--', linewidth=2, label='Prod Threshold (55k)')\\naxes[2].set_xlabel('Income')\\naxes[2].set_ylabel('Purchase (with jitter)')\\naxes[2].set_title('Concept Drift Visualization\\\\n(Threshold shifted from 45k to 55k)')\\naxes[2].legend()\\n\\nplt.tight_layout()\\nplt.savefig('concept_drift_visualization.png', dpi=150, bbox_inches='tight')\\nplt.show()\\n\\nprint(\\\"\\\\nüí° ‡∏™‡∏±‡∏á‡πÄ‡∏Å‡∏ï: Distribution ‡∏Ç‡∏≠‡∏á Income ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏° ‡πÅ‡∏ï‡πà‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡∏Å‡∏±‡∏ö Purchase ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÑ‡∏õ!\\\")\\nprint(\\\"   - Training: ‡∏ã‡∏∑‡πâ‡∏≠‡πÄ‡∏°‡∏∑‡πà‡∏≠ income > 45,000\\\")\\nprint(\\\"   - Production: ‡∏ã‡∏∑‡πâ‡∏≠‡πÄ‡∏°‡∏∑‡πà‡∏≠ income > 55,000\\\")\\n\\n#%% [markdown]\\n# ## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 4: Statistical Tests ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Drift Detection\\n# \\n# ### 4.1 Kolmogorov-Smirnov (KS) Test\\n# \\n# **‡∏ó‡∏§‡∏©‡∏é‡∏µ:**\\n# - ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö cumulative distribution function (CDF) ‡∏Ç‡∏≠‡∏á 2 samples\\n# - ‡∏ß‡∏±‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á 2 CDFs\\n# - ‡∏Ç‡πâ‡∏≠‡∏î‡∏µ: ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏™‡∏°‡∏°‡∏ï‡∏¥ distribution, sensitive ‡∏ï‡πà‡∏≠‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á\\n# - ‡∏Ç‡πâ‡∏≠‡πÄ‡∏™‡∏µ‡∏¢: ‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ‡∏Å‡∏±‡∏ö continuous variables ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô\\n#\\n# **‡∏Å‡∏≤‡∏£‡∏ï‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°:**\\n# - KS Statistic: 0-1 (‡∏¢‡∏¥‡πà‡∏á‡∏™‡∏π‡∏á = ‡∏¢‡∏¥‡πà‡∏á‡∏ï‡πà‡∏≤‡∏á)\\n# - p-value < 0.05: reject null hypothesis ‚Üí ‡∏°‡∏µ drift\\n\\n#%%\\ndef kolmogorov_smirnov_test(data1, data2, feature_name=\\\"feature\\\"):\\n    \\\"\\\"\\\"\\n    ‡∏ó‡∏≥ KS Test ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö drift\\n    \\n    Parameters:\\n    -----------\\n    data1 : array-like - ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ä‡∏∏‡∏î‡πÅ‡∏£‡∏Å (reference/training)\\n    data2 : array-like - ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ä‡∏∏‡∏î‡∏ó‡∏µ‡πà‡∏™‡∏≠‡∏á (current/production)\\n    feature_name : str - ‡∏ä‡∏∑‡πà‡∏≠ feature\\n    \\n    Returns:\\n    --------\\n    dict : ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Ç‡∏≠‡∏á KS test\\n    \\\"\\\"\\\"\\n    statistic, p_value = stats.ks_2samp(data1, data2)\\n    \\n    # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î threshold ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö drift\\n    drift_detected = p_value < 0.05\\n    \\n    result = {\\n        'feature': feature_name,\\n        'test': 'Kolmogorov-Smirnov',\\n        'statistic': statistic,\\n        'p_value': p_value,\\n        'drift_detected': drift_detected,\\n        'interpretation': 'DRIFT DETECTED!' if drift_detected else 'No significant drift'\\n    }\\n    \\n    return result\\n\\n# ‡∏ó‡∏î‡∏™‡∏≠‡∏ö KS Test ‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Covariate Shift\\nprint(\\\"=\\\" * 60)\\nprint(\\\"üîç KS Test Results for Covariate Shift Data\\\")\\nprint(\\\"=\\\" * 60)\\n\\nks_age = kolmogorov_smirnov_test(train_covariate['age'], prod_covariate['age'], 'age')\\nks_income = kolmogorov_smirnov_test(train_covariate['income'], prod_covariate['income'], 'income')\\n\\nfor result in [ks_age, ks_income]:\\n    print(f\\\"\\\\nFeature: {result['feature']}\\\")\\n    print(f\\\"  KS Statistic: {result['statistic']:.4f}\\\")\\n    print(f\\\"  P-value: {result['p_value']:.6f}\\\")\\n    print(f\\\"  Result: {result['interpretation']}\\\")\\n\\n#%%\\n# Visualize KS Test\\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\\n\\nfor idx, (feature, ax) in enumerate(zip(['age', 'income'], axes)):\\n    # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì CDF\\n    sorted_train = np.sort(train_covariate[feature])\\n    sorted_prod = np.sort(prod_covariate[feature])\\n    \\n    cdf_train = np.arange(1, len(sorted_train) + 1) / len(sorted_train)\\n    cdf_prod = np.arange(1, len(sorted_prod) + 1) / len(sorted_prod)\\n    \\n    ax.plot(sorted_train, cdf_train, 'b-', linewidth=2, label='Training CDF')\\n    ax.plot(sorted_prod, cdf_prod, 'r-', linewidth=2, label='Production CDF')\\n    \\n    # ‡πÅ‡∏™‡∏î‡∏á KS statistic (maximum distance)\\n    ks_result = kolmogorov_smirnov_test(train_covariate[feature], prod_covariate[feature], feature)\\n    \\n    ax.set_xlabel(feature.capitalize())\\n    ax.set_ylabel('Cumulative Probability')\\n    ax.set_title(f'KS Test: {feature.capitalize()}\\\\nKS Statistic = {ks_result[\\\"statistic\\\"]:.4f}, p-value = {ks_result[\\\"p_value\\\"]:.4f}')\\n    ax.legend()\\n    ax.grid(True, alpha=0.3)\\n\\nplt.tight_layout()\\nplt.savefig('ks_test_visualization.png', dpi=150, bbox_inches='tight')\\nplt.show()\\n\\n#%% [markdown]\\n# ### 4.2 Population Stability Index (PSI)\\n# \\n# **‡∏ó‡∏§‡∏©‡∏é‡∏µ:**\\n# - ‡∏ß‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡∏≠‡∏á distribution ‡πÇ‡∏î‡∏¢‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö proportions ‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞ bin\\n# - ‡∏ô‡∏¥‡∏¢‡∏°‡πÉ‡∏ä‡πâ‡πÉ‡∏ô credit scoring ‡πÅ‡∏•‡∏∞ financial models\\n# - ‡∏™‡∏π‡∏ï‡∏£: PSI = Œ£ (Actual% - Expected%) √ó ln(Actual% / Expected%)\\n#\\n# **‡∏Å‡∏≤‡∏£‡∏ï‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏° PSI:**\\n# - PSI < 0.1: ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏ó‡∏µ‡πà‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç\\n# - 0.1 ‚â§ PSI < 0.25: ‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á ‡∏Ñ‡∏ß‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö\\n# - PSI ‚â• 0.25: ‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏°‡∏≤‡∏Å ‡∏ï‡πâ‡∏≠‡∏á‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£\\n\\n#%%\\ndef calculate_psi(expected, actual, bins=10, eps=1e-6):\\n    \\\"\\\"\\\"\\n    ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Population Stability Index (PSI)\\n    \\n    Parameters:\\n    -----------\\n    expected : array-like - ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• reference (training)\\n    actual : array-like - ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• current (production)\\n    bins : int - ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô bins ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö discretize\\n    eps : float - ‡∏Ñ‡πà‡∏≤‡πÄ‡∏•‡πá‡∏Å‡πÜ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô division by zero\\n    \\n    Returns:\\n    --------\\n    float : ‡∏Ñ‡πà‡∏≤ PSI\\n    dict : ‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì\\n    \\\"\\\"\\\"\\n    # ‡∏™‡∏£‡πâ‡∏≤‡∏á bins ‡∏à‡∏≤‡∏Å expected data\\n    breakpoints = np.percentile(expected, np.linspace(0, 100, bins + 1))\\n    breakpoints = np.unique(breakpoints)  # ‡∏•‡∏ö duplicates\\n    \\n    # ‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞ bin\\n    expected_counts, _ = np.histogram(expected, bins=breakpoints)\\n    actual_counts, _ = np.histogram(actual, bins=breakpoints)\\n    \\n    # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì proportions\\n    expected_props = expected_counts / len(expected) + eps\\n    actual_props = actual_counts / len(actual) + eps\\n    \\n    # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì PSI\\n    psi_values = (actual_props - expected_props) * np.log(actual_props / expected_props)\\n    psi = np.sum(psi_values)\\n    \\n    # ‡∏ï‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå\\n    if psi < 0.1:\\n        interpretation = \\\"No significant change (PSI < 0.1)\\\"\\n        severity = \\\"LOW\\\"\\n    elif psi < 0.25:\\n        interpretation = \\\"Moderate change - monitor closely (0.1 ‚â§ PSI < 0.25)\\\"\\n        severity = \\\"MEDIUM\\\"\\n    else:\\n        interpretation = \\\"Significant change - action required (PSI ‚â• 0.25)\\\"\\n        severity = \\\"HIGH\\\"\\n    \\n    return {\\n        'psi': psi,\\n        'interpretation': interpretation,\\n        'severity': severity,\\n        'bin_psi_values': psi_values,\\n        'expected_props': expected_props,\\n        'actual_props': actual_props,\\n        'breakpoints': breakpoints\\n    }\\n\\n# ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì PSI ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞ feature\\nprint(\\\"=\\\" * 60)\\nprint(\\\"üìä PSI Results for Covariate Shift Data\\\")\\nprint(\\\"=\\\" * 60)\\n\\nfor feature in ['age', 'income']:\\n    psi_result = calculate_psi(train_covariate[feature], prod_covariate[feature])\\n    print(f\\\"\\\\nFeature: {feature}\\\")\\n    print(f\\\"  PSI Value: {psi_result['psi']:.4f}\\\")\\n    print(f\\\"  Severity: {psi_result['severity']}\\\")\\n    print(f\\\"  Interpretation: {psi_result['interpretation']}\\\")\\n\\n#%%\\n# Visualize PSI\\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\\n\\nfor idx, feature in enumerate(['age', 'income']):\\n    psi_result = calculate_psi(train_covariate[feature], prod_covariate[feature])\\n    \\n    x = np.arange(len(psi_result['expected_props']))\\n    width = 0.35\\n    \\n    axes[idx].bar(x - width/2, psi_result['expected_props'], width, label='Expected (Train)', color='blue', alpha=0.7)\\n    axes[idx].bar(x + width/2, psi_result['actual_props'], width, label='Actual (Prod)', color='red', alpha=0.7)\\n    \\n    axes[idx].set_xlabel('Bin')\\n    axes[idx].set_ylabel('Proportion')\\n    axes[idx].set_title(f'PSI Analysis: {feature.capitalize()}\\\\nPSI = {psi_result[\\\"psi\\\"]:.4f} ({psi_result[\\\"severity\\\"]})')\\n    axes[idx].legend()\\n    axes[idx].set_xticks(x)\\n\\nplt.tight_layout()\\nplt.savefig('psi_visualization.png', dpi=150, bbox_inches='tight')\\nplt.show()\\n\\n#%% [markdown]\\n# ### 4.3 Wasserstein Distance (Earth Mover's Distance)\\n# \\n# **‡∏ó‡∏§‡∏©‡∏é‡∏µ:**\\n# - ‡∏ß‡∏±‡∏î \\\"‡∏á‡∏≤‡∏ô\\\" ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô distribution ‡∏´‡∏ô‡∏∂‡πà‡∏á‡πÑ‡∏õ‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏µ‡∏Å distribution\\n# - ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏ï‡πâ‡∏ô‡∏ó‡∏∏‡∏ô‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏Ç‡∏ô‡∏¢‡πâ‡∏≤‡∏¢‡∏î‡∏¥‡∏ô (Earth Mover)\\n# - ‡∏Ç‡πâ‡∏≠‡∏î‡∏µ: ‡∏Ñ‡∏≥‡∏ô‡∏∂‡∏á‡∏ñ‡∏∂‡∏á distance ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á bins, sensitive ‡∏ï‡πà‡∏≠ shift ‡πÉ‡∏ô‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á\\n# - ‡∏Ç‡πâ‡∏≠‡πÄ‡∏™‡∏µ‡∏¢: ‡∏ï‡πâ‡∏≠‡∏á normalize ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡πÑ‡∏î‡πâ\\n\\n#%%\\ndef wasserstein_distance_test(data1, data2, feature_name=\\\"feature\\\"):\\n    \\\"\\\"\\\"\\n    ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Wasserstein Distance ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö drift detection\\n    \\n    Parameters:\\n    -----------\\n    data1 : array-like - ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ä‡∏∏‡∏î‡πÅ‡∏£‡∏Å (reference)\\n    data2 : array-like - ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ä‡∏∏‡∏î‡∏ó‡∏µ‡πà‡∏™‡∏≠‡∏á (current)\\n    feature_name : str - ‡∏ä‡∏∑‡πà‡∏≠ feature\\n    \\n    Returns:\\n    --------\\n    dict : ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Ç‡∏≠‡∏á Wasserstein distance\\n    \\\"\\\"\\\"\\n    # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Wasserstein distance\\n    distance = stats.wasserstein_distance(data1, data2)\\n    \\n    # Normalize ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ standard deviation ‡∏Ç‡∏≠‡∏á reference data\\n    std_ref = np.std(data1)\\n    normalized_distance = distance / std_ref if std_ref > 0 else distance\\n    \\n    # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î threshold (‡∏õ‡∏£‡∏±‡∏ö‡πÑ‡∏î‡πâ‡∏ï‡∏≤‡∏° domain)\\n    if normalized_distance < 0.1:\\n        severity = \\\"LOW\\\"\\n        interpretation = \\\"No significant drift\\\"\\n    elif normalized_distance < 0.5:\\n        severity = \\\"MEDIUM\\\"\\n        interpretation = \\\"Moderate drift detected\\\"\\n    else:\\n        severity = \\\"HIGH\\\"\\n        interpretation = \\\"Significant drift detected\\\"\\n    \\n    return {\\n        'feature': feature_name,\\n        'distance': distance,\\n        'normalized_distance': normalized_distance,\\n        'severity': severity,\\n        'interpretation': interpretation\\n    }\\n\\n# ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Wasserstein Distance\\nprint(\\\"=\\\" * 60)\\nprint(\\\"üìè Wasserstein Distance Results for Covariate Shift Data\\\")\\nprint(\\\"=\\\" * 60)\\n\\nfor feature in ['age', 'income']:\\n    wd_result = wasserstein_distance_test(train_covariate[feature], prod_covariate[feature], feature)\\n    print(f\\\"\\\\nFeature: {feature}\\\")\\n    print(f\\\"  Wasserstein Distance: {wd_result['distance']:.4f}\\\")\\n    print(f\\\"  Normalized Distance: {wd_result['normalized_distance']:.4f}\\\")\\n    print(f\\\"  Severity: {wd_result['severity']}\\\")\\n    print(f\\\"  Interpretation: {wd_result['interpretation']}\\\")\\n\\n#%%\\n# Visualize Wasserstein Distance\\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\\n\\nfor idx, feature in enumerate(['age', 'income']):\\n    ax = axes[idx]\\n    \\n    # ‡πÅ‡∏™‡∏î‡∏á histogram ‡πÅ‡∏•‡∏∞ KDE\\n    ax.hist(train_covariate[feature], bins=30, alpha=0.5, density=True, label='Training', color='blue')\\n    ax.hist(prod_covariate[feature], bins=30, alpha=0.5, density=True, label='Production', color='red')\\n    \\n    # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Wasserstein distance\\n    wd_result = wasserstein_distance_test(train_covariate[feature], prod_covariate[feature], feature)\\n    \\n    # ‡πÅ‡∏™‡∏î‡∏á mean ‡πÅ‡∏•‡∏∞ \\\"‡∏Å‡∏≤‡∏£‡∏Ç‡∏ô‡∏¢‡πâ‡∏≤‡∏¢\\\"\\n    mean_train = train_covariate[feature].mean()\\n    mean_prod = prod_covariate[feature].mean()\\n    \\n    ax.axvline(mean_train, color='blue', linestyle='--', linewidth=2, label=f'Train Mean: {mean_train:.1f}')\\n    ax.axvline(mean_prod, color='red', linestyle='--', linewidth=2, label=f'Prod Mean: {mean_prod:.1f}')\\n    \\n    # ‡πÅ‡∏™‡∏î‡∏á arrow ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö \\\"earth moving\\\"\\n    ax.annotate('', xy=(mean_prod, 0.01), xytext=(mean_train, 0.01),\\n                arrowprops=dict(arrowstyle='->', color='green', lw=3))\\n    \\n    ax.set_xlabel(feature.capitalize())\\n    ax.set_ylabel('Density')\\n    ax.set_title(f'Wasserstein Distance: {feature.capitalize()}\\\\nDistance = {wd_result[\\\"distance\\\"]:.2f} ({wd_result[\\\"severity\\\"]})')\\n    ax.legend(loc='upper right')\\n\\nplt.tight_layout()\\nplt.savefig('wasserstein_visualization.png', dpi=150, bbox_inches='tight')\\nplt.show()\\n\\nprint(\\\"\\\\nüí° ‡∏•‡∏π‡∏Å‡∏®‡∏£‡∏™‡∏µ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ß‡πÅ‡∏™‡∏î‡∏á‡∏ó‡∏¥‡∏®‡∏ó‡∏≤‡∏á‡∏Ç‡∏≠‡∏á '‡∏Å‡∏≤‡∏£‡∏Ç‡∏ô‡∏¢‡πâ‡∏≤‡∏¢' ‡∏à‡∏≤‡∏Å Training ‡πÑ‡∏õ Production\\\")\\n\\n#%% [markdown]\\n# ## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 5: ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡πÅ‡∏•‡∏∞‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Drift Detection Method\\n# \\n# ### ‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πâ‡∏≠‡∏î‡∏µ-‡∏Ç‡πâ‡∏≠‡πÄ‡∏™‡∏µ‡∏¢‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞ method:\\n#\\n# | Method | ‡∏Ç‡πâ‡∏≠‡∏î‡∏µ | ‡∏Ç‡πâ‡∏≠‡πÄ‡∏™‡∏µ‡∏¢ | ‡πÉ‡∏ä‡πâ‡πÄ‡∏°‡∏∑‡πà‡∏≠ |\\n# |--------|-------|---------|---------|\\n# | KS Test | ‡πÑ‡∏°‡πà‡∏™‡∏°‡∏°‡∏ï‡∏¥ distribution, sensitive | ‡πÉ‡∏ä‡πâ‡∏Å‡∏±‡∏ö continuous ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô | ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ statistical significance |\\n# | PSI | ‡∏°‡∏µ threshold ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô, industry standard | ‡∏ï‡πâ‡∏≠‡∏á binning | Credit scoring, Risk models |\\n# | Wasserstein | ‡∏Ñ‡∏≥‡∏ô‡∏∂‡∏á‡∏ñ‡∏∂‡∏á distance, ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏á‡πà‡∏≤‡∏¢ | ‡∏ï‡πâ‡∏≠‡∏á normalize | ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö shift ‡πÉ‡∏ô‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á |\\n\\n#%%\\ndef comprehensive_drift_analysis(reference_data, current_data, feature_name):\\n    \\\"\\\"\\\"\\n    ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå drift ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡∏ó‡∏∏‡∏Å methods ‡πÅ‡∏•‡∏∞‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå\\n    \\\"\\\"\\\"\\n    results = {\\n        'feature': feature_name,\\n        'ks': kolmogorov_smirnov_test(reference_data, current_data, feature_name),\\n        'psi': calculate_psi(reference_data, current_data),\\n        'wasserstein': wasserstein_distance_test(reference_data, current_data, feature_name)\\n    }\\n    \\n    # ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•\\n    drift_votes = 0\\n    if results['ks']['drift_detected']:\\n        drift_votes += 1\\n    if results['psi']['severity'] in ['MEDIUM', 'HIGH']:\\n        drift_votes += 1\\n    if results['wasserstein']['severity'] in ['MEDIUM', 'HIGH']:\\n        drift_votes += 1\\n    \\n    results['consensus'] = 'DRIFT' if drift_votes >= 2 else 'NO DRIFT'\\n    results['drift_votes'] = drift_votes\\n    \\n    return results\\n\\n# ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå\\nprint(\\\"=\\\" * 80)\\nprint(\\\"üìä COMPREHENSIVE DRIFT ANALYSIS COMPARISON\\\")\\nprint(\\\"=\\\" * 80)\\n\\ncomparison_results = []\\n\\nfor feature in ['age', 'income']:\\n    result = comprehensive_drift_analysis(\\n        train_covariate[feature], \\n        prod_covariate[feature], \\n        feature\\n    )\\n    comparison_results.append(result)\\n    \\n    print(f\\\"\\\\nüîç Feature: {feature.upper()}\\\")\\n    print(\\\"-\\\" * 40)\\n    print(f\\\"  KS Test: stat={result['ks']['statistic']:.4f}, p={result['ks']['p_value']:.4f} ‚Üí {result['ks']['interpretation']}\\\")\\n    print(f\\\"  PSI: {result['psi']['psi']:.4f} ‚Üí {result['psi']['severity']}\\\")\\n    print(f\\\"  Wasserstein: {result['wasserstein']['normalized_distance']:.4f} ‚Üí {result['wasserstein']['severity']}\\\")\\n    print(f\\\"  üìã CONSENSUS ({result['drift_votes']}/3 votes): {result['consensus']}\\\")\\n\\n#%%\\n# ‡∏™‡∏£‡πâ‡∏≤‡∏á comparison table\\ncomparison_df = pd.DataFrame([\\n    {\\n        'Feature': r['feature'],\\n        'KS Statistic': f\\\"{r['ks']['statistic']:.4f}\\\",\\n        'KS p-value': f\\\"{r['ks']['p_value']:.4f}\\\",\\n        'KS Drift': '‚úì' if r['ks']['drift_detected'] else '‚úó',\\n        'PSI': f\\\"{r['psi']['psi']:.4f}\\\",\\n        'PSI Severity': r['psi']['severity'],\\n        'Wasserstein': f\\\"{r['wasserstein']['normalized_distance']:.4f}\\\",\\n        'WD Severity': r['wasserstein']['severity'],\\n        'Consensus': r['consensus']\\n    }\\n    for r in comparison_results\\n])\\n\\nprint(\\\"\\\\n\\\" + \\\"=\\\" * 80)\\nprint(\\\"üìä SUMMARY TABLE\\\")\\nprint(\\\"=\\\" * 80)\\nprint(comparison_df.to_string(index=False))\\n\\n#%% [markdown]\\n# ## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 6: ‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Drift Detection Method\\n# \\n# ### Decision Tree ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Method:\\n#\\n# ```\\n# 1. Data Type?\\n#    ‚îú‚îÄ‚îÄ Continuous ‚Üí ‡πÑ‡∏õ‡∏Ç‡πâ‡∏≠ 2\\n#    ‚îî‚îÄ‚îÄ Categorical ‚Üí ‡πÉ‡∏ä‡πâ Chi-squared test ‡∏´‡∏£‡∏∑‡∏≠ PSI\\n#\\n# 2. ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ Statistical Significance?\\n#    ‚îú‚îÄ‚îÄ ‡πÉ‡∏ä‡πà ‚Üí KS Test ‡∏´‡∏£‡∏∑‡∏≠ Chi-squared\\n#    ‚îî‚îÄ‚îÄ ‡πÑ‡∏°‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô ‚Üí PSI ‡∏´‡∏£‡∏∑‡∏≠ Wasserstein\\n#\\n# 3. Industry Requirement?\\n#    ‚îú‚îÄ‚îÄ Finance/Credit ‚Üí PSI (‡∏°‡∏µ regulatory standards)\\n#    ‚îî‚îÄ‚îÄ ‡∏≠‡∏∑‡πà‡∏ô‡πÜ ‚Üí ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ï‡∏≤‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°\\n#\\n# 4. ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏Ñ‡∏ß‡∏≤‡∏° sensitive ‡∏™‡∏π‡∏á?\\n#    ‚îú‚îÄ‚îÄ ‡πÉ‡∏ä‡πà ‚Üí Wasserstein (detect small shifts)\\n#    ‚îî‚îÄ‚îÄ ‡∏õ‡∏Å‡∏ï‡∏¥ ‚Üí KS ‡∏´‡∏£‡∏∑‡∏≠ PSI\\n# ```\\n\\n#%%\\ndef recommend_drift_method(data_type, needs_significance, industry, high_sensitivity):\\n    \\\"\\\"\\\"\\n    ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥ drift detection method ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°\\n    \\n    Parameters:\\n    -----------\\n    data_type : str - 'continuous' ‡∏´‡∏£‡∏∑‡∏≠ 'categorical'\\n    needs_significance : bool - ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ statistical significance ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà\\n    industry : str - 'finance', 'healthcare', 'general'\\n    high_sensitivity : bool - ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ sensitivity ‡∏™‡∏π‡∏á‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà\\n    \\n    Returns:\\n    --------\\n    str : recommended method ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•\\n    \\\"\\\"\\\"\\n    recommendations = []\\n    \\n    if data_type == 'categorical':\\n        recommendations.append({\\n            'method': 'Chi-squared Test',\\n            'reason': '‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö categorical data',\\n            'priority': 1\\n        })\\n        recommendations.append({\\n            'method': 'PSI (binned)',\\n            'reason': '‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏ä‡πâ‡∏Å‡∏±‡∏ö categorical ‡πÑ‡∏î‡πâ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ category ‡πÄ‡∏õ‡πá‡∏ô bins',\\n            'priority': 2\\n        })\\n    else:  # continuous\\n        if industry == 'finance':\\n            recommendations.append({\\n                'method': 'PSI',\\n                'reason': 'Industry standard ‡πÉ‡∏ô finance, ‡∏°‡∏µ regulatory requirements',\\n                'priority': 1\\n            })\\n        \\n        if needs_significance:\\n            recommendations.append({\\n                'method': 'KS Test',\\n                'reason': '‡πÉ‡∏´‡πâ p-value ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö statistical significance',\\n                'priority': 1 if industry != 'finance' else 2\\n            })\\n        \\n        if high_sensitivity:\\n            recommendations.append({\\n                'method': 'Wasserstein Distance',\\n                'reason': 'Sensitive ‡∏ï‡πà‡∏≠ small shifts ‡πÅ‡∏•‡∏∞ location changes',\\n                'priority': 2\\n            })\\n        \\n        # Default recommendation\\n        if not recommendations:\\n            recommendations.append({\\n                'method': 'PSI + KS Test',\\n                'reason': '‡πÉ‡∏ä‡πâ‡∏ó‡∏±‡πâ‡∏á‡∏™‡∏≠‡∏á‡πÄ‡∏û‡∏∑‡πà‡∏≠ cross-validate',\\n                'priority': 1\\n            })\\n    \\n    # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏ï‡∏≤‡∏° priority\\n    recommendations.sort(key=lambda x: x['priority'])\\n    \\n    return recommendations\\n\\n# ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô\\nprint(\\\"=\\\" * 60)\\nprint(\\\"üéØ DRIFT METHOD RECOMMENDATION EXAMPLES\\\")\\nprint(\\\"=\\\" * 60)\\n\\nscenarios = [\\n    {'data_type': 'continuous', 'needs_significance': True, 'industry': 'finance', 'high_sensitivity': False},\\n    {'data_type': 'continuous', 'needs_significance': False, 'industry': 'general', 'high_sensitivity': True},\\n    {'data_type': 'categorical', 'needs_significance': True, 'industry': 'healthcare', 'high_sensitivity': False},\\n]\\n\\nfor i, scenario in enumerate(scenarios, 1):\\n    print(f\\\"\\\\nüìã Scenario {i}:\\\")\\n    print(f\\\"   Data Type: {scenario['data_type']}\\\")\\n    print(f\\\"   Needs Significance: {scenario['needs_significance']}\\\")\\n    print(f\\\"   Industry: {scenario['industry']}\\\")\\n    print(f\\\"   High Sensitivity: {scenario['high_sensitivity']}\\\")\\n    \\n    recs = recommend_drift_method(**scenario)\\n    print(\\\"   Recommendations:\\\")\\n    for j, rec in enumerate(recs, 1):\\n        print(f\\\"   {j}. {rec['method']} - {rec['reason']}\\\")\\n\\n#%% [markdown]\\n# ## ‡∏™‡∏£‡∏∏‡∏õ LAB 1\\n# \\n# ### ‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ:\\n# 1. **Covariate Shift**: P(X) ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô ‡πÅ‡∏ï‡πà P(Y|X) ‡∏Ñ‡∏á‡∏ó‡∏µ‡πà\\n# 2. **Concept Drift**: P(Y|X) ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô (relationship ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô)\\n# 3. **KS Test**: ‡∏ß‡∏±‡∏î maximum distance ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á CDFs, ‡πÉ‡∏´‡πâ p-value\\n# 4. **PSI**: ‡∏ß‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡∏≠‡∏á proportions, ‡∏°‡∏µ threshold ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô\\n# 5. **Wasserstein**: ‡∏ß‡∏±‡∏î \\\"‡∏á‡∏≤‡∏ô\\\" ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô distribution\\n# \\n# ### Best Practices:\\n# - ‡πÉ‡∏ä‡πâ‡∏´‡∏•‡∏≤‡∏¢ methods ‡πÄ‡∏û‡∏∑‡πà‡∏≠ cross-validate\\n# - ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å method ‡∏ï‡∏≤‡∏° data type ‡πÅ‡∏•‡∏∞ business requirements\\n# - ‡∏ï‡∏±‡πâ‡∏á threshold ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡∏Å‡∏±‡∏ö context\\n\\n#%%\\nprint(\\\"=\\\" * 60)\\nprint(\\\"‚úÖ LAB 1 COMPLETED!\\\")\\nprint(\\\"=\\\" * 60)\\nprint(\\\"\\\"\\\"\\nüìö Key Takeaways:\\n1. Covariate Shift vs Concept Drift - ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á\\n2. KS Test - ‡πÉ‡∏ä‡πâ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ statistical significance\\n3. PSI - Industry standard ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö finance\\n4. Wasserstein - Sensitive ‡∏ï‡πà‡∏≠ location shifts\\n\\nüîú Next: LAB 2 - Feature Drift Detection\\n\\\"\\\"\\\")\\n```\\n\\n---\\n\\n## LAB 2: Feature Drift Detection\\n\\n```python\\n#%% [markdown]\\n# # LAB 2: Feature Drift Detection\\n# ## ‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö Drift ‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞ Feature\\n# \\n# ### ‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ:\\n# 1. ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö drift ‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞ feature ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏∞‡∏ö‡∏ö\\n# 2. ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå numerical vs categorical feature drift\\n# 3. ‡∏™‡∏£‡πâ‡∏≤‡∏á visualization ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö feature distributions over time\\n#\\n# ### ‡∏ó‡∏§‡∏©‡∏é‡∏µ:\\n# ‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö drift ‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞ feature ‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡πÄ‡∏û‡∏£‡∏≤‡∏∞:\\n# - ‡∏ä‡πà‡∏ß‡∏¢‡∏£‡∏∞‡∏ö‡∏∏ root cause ‡∏Ç‡∏≠‡∏á model performance degradation\\n# - ‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏ß‡πà‡∏≤ feature ‡πÑ‡∏´‡∏ô‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î\\n# - ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ prioritize ‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÑ‡∏î‡πâ\\n\\n#%% [markdown]\\n# ## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 1: ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° Environment\\n\\n#%%\\nimport numpy as np\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\nfrom scipy import stats\\nfrom sklearn.datasets import make_classification\\nfrom sklearn.preprocessing import StandardScaler\\nimport warnings\\nwarnings.filterwarnings('ignore')\\n\\nnp.random.seed(42)\\nplt.rcParams['figure.figsize'] = (14, 8)\\n\\nprint(\\\"‚úÖ Libraries imported successfully!\\\")\\n\\n#%% [markdown]\\n# ## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 2: ‡∏™‡∏£‡πâ‡∏≤‡∏á Dataset ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏´‡∏•‡∏≤‡∏¢ Features\\n# \\n# ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á dataset ‡∏ó‡∏µ‡πà‡∏à‡∏≥‡∏•‡∏≠‡∏á‡∏™‡∏ñ‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏ì‡πå‡∏à‡∏£‡∏¥‡∏á:\\n# - ‡∏°‡∏µ‡∏ó‡∏±‡πâ‡∏á numerical ‡πÅ‡∏•‡∏∞ categorical features\\n# - ‡∏ö‡∏≤‡∏á features ‡∏°‡∏µ drift ‡∏ö‡∏≤‡∏á features ‡πÑ‡∏°‡πà‡∏°‡∏µ\\n# - ‡∏°‡∏µ drift ‡πÉ‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ï‡πà‡∏≤‡∏á‡πÜ\\n\\n#%%\\ndef create_multi_feature_dataset(n_samples=2000, drift_level='mixed'):\\n    \\\"\\\"\\\"\\n    ‡∏™‡∏£‡πâ‡∏≤‡∏á dataset ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏´‡∏•‡∏≤‡∏¢ features ‡∏û‡∏£‡πâ‡∏≠‡∏° simulated drift\\n    \\n    Parameters:\\n    -----------\\n    n_samples : int - ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô samples ‡∏ï‡πà‡∏≠ dataset\\n    drift_level : str - 'none', 'mild', 'severe', 'mixed'\\n    \\n    Returns:\\n    --------\\n    tuple : (reference_df, current_df, feature_info)\\n    \\\"\\\"\\\"\\n    \\n    np.random.seed(42)\\n    \\n    # === Reference Data (Training) ===\\n    reference_data = {\\n        # Numerical Features\\n        'age': np.random.normal(35, 10, n_samples),\\n        'income': np.random.normal(50000, 15000, n_samples),\\n        'credit_score': np.random.normal(650, 80, n_samples),\\n        'account_balance': np.random.exponential(10000, n_samples),\\n        'transaction_count': np.random.poisson(20, n_samples),\\n        \\n        # Categorical Features (encoded as integers)\\n        'region': np.random.choice([0, 1, 2, 3], n_samples, p=[0.3, 0.3, 0.25, 0.15]),\\n        'customer_type': np.random.choice([0, 1, 2], n_samples, p=[0.5, 0.35, 0.15]),\\n        'product_category': np.random.choice([0, 1, 2, 3, 4], n_samples, p=[0.2, 0.2, 0.2, 0.2, 0.2])\\n    }\\n    \\n    # === Current Data (Production) with various drift levels ===\\n    if drift_level == 'none':\\n        current_data = {k: v.copy() for k, v in reference_data.items()}\\n        # ‡πÄ‡∏û‡∏¥‡πà‡∏° random noise ‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢\\n        for key in ['age', 'income', 'credit_score']:\\n            current_data[key] = np.random.normal(\\n                reference_data[key].mean(),\\n                reference_data[key].std(),\\n                n_samples\\n            )\\n    \\n    elif drift_level == 'mixed':\\n        current_data = {\\n            # No drift\\n            'age': np.random.normal(35, 10, n_samples),  # ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°\\n            'credit_score': np.random.normal(650, 80, n_samples),  # ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°\\n            \\n            # Mild drift\\n            'income': np.random.normal(55000, 15000, n_samples),  # mean shift ‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢\\n            'transaction_count': np.random.poisson(25, n_samples),  # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡∏∂‡πâ‡∏ô‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢\\n            \\n            # Severe drift\\n            'account_balance': np.random.exponential(20000, n_samples),  # scale ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏°‡∏≤‡∏Å\\n            \\n            # Categorical drift\\n            'region': np.random.choice([0, 1, 2, 3], n_samples, p=[0.15, 0.15, 0.35, 0.35]),  # distribution ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô\\n            'customer_type': np.random.choice([0, 1, 2], n_samples, p=[0.5, 0.35, 0.15]),  # ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°\\n            'product_category': np.random.choice([0, 1, 2, 3, 4], n_samples, p=[0.4, 0.1, 0.1, 0.2, 0.2])  # ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏ö‡πâ‡∏≤‡∏á\\n        }\\n    \\n    reference_df = pd.DataFrame(reference_data)\\n    current_df = pd.DataFrame(current_data)\\n    \\n    # Feature info\\n    feature_info = {\\n        'numerical': ['age', 'income', 'credit_score', 'account_balance', 'transaction_count'],\\n        'categorical': ['region', 'customer_type', 'product_category'],\\n        'expected_drift': {\\n            'age': 'none',\\n            'income': 'mild',\\n            'credit_score': 'none',\\n            'account_balance': 'severe',\\n            'transaction_count': 'mild',\\n            'region': 'severe',\\n            'customer_type': 'none',\\n            'product_category': 'mild'\\n        }\\n    }\\n    \\n    return reference_df, current_df, feature_info\\n\\n# ‡∏™‡∏£‡πâ‡∏≤‡∏á dataset\\nreference_df, current_df, feature_info = create_multi_feature_dataset(drift_level='mixed')\\n\\nprint(\\\"üìä Reference Data Shape:\\\", reference_df.shape)\\nprint(\\\"üìä Current Data Shape:\\\", current_df.shape)\\nprint(\\\"\\\\nüìã Feature Types:\\\")\\nprint(f\\\"  Numerical: {feature_info['numerical']}\\\")\\nprint(f\\\"  Categorical: {feature_info['categorical']}\\\")\\n\\nprint(\\\"\\\\nüìã Expected Drift Levels:\\\")\\nfor feature, level in feature_info['expected_drift'].items():\\n    print(f\\\"  {feature}: {level}\\\")\\n\\n#%%\\n# ‡πÅ‡∏™‡∏î‡∏á summary statistics\\nprint(\\\"\\\\n\\\" + \\\"=\\\" * 60)\\nprint(\\\"REFERENCE DATA SUMMARY\\\")\\nprint(\\\"=\\\" * 60)\\nprint(reference_df.describe())\\n\\nprint(\\\"\\\\n\\\" + \\\"=\\\" * 60)\\nprint(\\\"CURRENT DATA SUMMARY\\\")\\nprint(\\\"=\\\" * 60)\\nprint(current_df.describe())\\n\\n#%% [markdown]\\n# ## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 3: ‡∏™‡∏£‡πâ‡∏≤‡∏á Feature Drift Detector Class\\n# \\n# ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á class ‡∏ó‡∏µ‡πà‡∏£‡∏ß‡∏°‡∏ó‡∏∏‡∏Å drift detection methods ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡∏™‡∏∞‡∏î‡∏ß‡∏Å\\n\\n#%%\\nclass FeatureDriftDetector:\\n    \\\"\\\"\\\"\\n    Class ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö drift ‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞ feature\\n    \\n    ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á numerical ‡πÅ‡∏•‡∏∞ categorical features\\n    ‡πÉ‡∏ä‡πâ‡∏´‡∏•‡∏≤‡∏¢ statistical tests ‡πÄ‡∏û‡∏∑‡πà‡∏≠ comprehensive analysis\\n    \\\"\\\"\\\"\\n    \\n    def __init__(self, reference_data, current_data, numerical_features=None, categorical_features=None):\\n        \\\"\\\"\\\"\\n        Initialize detector\\n        \\n        Parameters:\\n        -----------\\n        reference_data : pd.DataFrame - ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• reference (training)\\n        current_data : pd.DataFrame - ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• current (production)\\n        numerical_features : list - ‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠ numerical features\\n        categorical_features : list - ‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠ categorical features\\n        \\\"\\\"\\\"\\n        self.reference = reference_data\\n        self.current = current_data\\n        \\n        # Auto-detect feature types if not provided\\n        if numerical_features is None:\\n            self.numerical_features = reference_data.select_dtypes(include=[np.number]).columns.tolist()\\n        else:\\n            self.numerical_features = numerical_features\\n            \\n        if categorical_features is None:\\n            self.categorical_features = []\\n        else:\\n            self.categorical_features = categorical_features\\n        \\n        self.results = {}\\n    \\n    def ks_test(self, feature):\\n        \\\"\\\"\\\"Kolmogorov-Smirnov test ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö numerical features\\\"\\\"\\\"\\n        stat, p_value = stats.ks_2samp(\\n            self.reference[feature].dropna(),\\n            self.current[feature].dropna()\\n        )\\n        return {'statistic': stat, 'p_value': p_value}\\n    \\n    def calculate_psi(self, feature, bins=10):\\n        \\\"\\\"\\\"Population Stability Index\\\"\\\"\\\"\\n        ref_data = self.reference[feature].dropna()\\n        cur_data = self.current[feature].dropna()\\n        \\n        # ‡∏™‡∏£‡πâ‡∏≤‡∏á bins\\n        breakpoints = np.percentile(ref_data, np.linspace(0, 100, bins + 1))\\n        breakpoints = np.unique(breakpoints)\\n        \\n        ref_counts, _ = np.histogram(ref_data, bins=breakpoints)\\n        cur_counts, _ = np.histogram(cur_data, bins=breakpoints)\\n        \\n        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì proportions\\n        eps = 1e-6\\n        ref_props = ref_counts / len(ref_data) + eps\\n        cur_props = cur_counts / len(cur_data) + eps\\n        \\n        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì PSI\\n        psi = np.sum((cur_props - ref_props) * np.log(cur_props / ref_props))\\n        \\n        return {'psi': psi}\\n    \\n    def wasserstein_test(self, feature):\\n        \\\"\\\"\\\"Wasserstein Distance\\\"\\\"\\\"\\n        ref_data = self.reference[feature].dropna()\\n        cur_data = self.current[feature].dropna()\\n        \\n        distance = stats.wasserstein_distance(ref_data, cur_data)\\n        \\n        # Normalize by std of reference\\n        std = ref_data.std()\\n        normalized = distance / std if std > 0 else distance\\n        \\n        return {'distance': distance, 'normalized_distance': normalized}\\n    \\n    def chi_squared_test(self, feature):\\n        \\\"\\\"\\\"Chi-squared test ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö categorical features\\\"\\\"\\\"\\n        # ‡∏ô‡∏±‡∏ö frequency ‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞ category\\n        ref_counts = self.reference[feature].value_counts()\\n        cur_counts = self.current[feature].value_counts()\\n        \\n        # ‡∏£‡∏ß‡∏° categories ‡πÉ‡∏´‡πâ‡πÄ‡∏ó‡πà‡∏≤‡∏Å‡∏±‡∏ô\\n        all_categories = set(ref_counts.index) | set(cur_counts.index)\\n        ref_freq = [ref_counts.get(cat, 0) for cat in all_categories]\\n        cur_freq = [cur_counts.get(cat, 0) for cat in all_categories]\\n        \\n        # Normalize to expected frequencies\\n        total_ref = sum(ref_freq)\\n        total_cur = sum(cur_freq)\\n        expected = [(r / total_ref) * total_cur for r in ref_freq]\\n        \\n        # Chi-squared test\\n        try:\\n            stat, p_value = stats.chisquare(cur_freq, expected)\\n        except:\\n            stat, p_value = 0, 1.0\\n        \\n        return {'statistic': stat, 'p_value': p_value}\\n    \\n    def analyze_numerical_feature(self, feature):\\n        \\\"\\\"\\\"‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå drift ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö numerical feature\\\"\\\"\\\"\\n        results = {\\n            'feature': feature,\\n            'type': 'numerical',\\n            'ks_test': self.ks_test(feature),\\n            'psi': self.calculate_psi(feature),\\n            'wasserstein': self.wasserstein_test(feature)\\n        }\\n        \\n        # ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•\\n        ks_drift = results['ks_test']['p_value'] < 0.05\\n        psi_value = results['psi']['psi']\\n        \\n        if psi_value < 0.1:\\n            psi_severity = 'none'\\n        elif psi_value < 0.25:\\n            psi_severity = 'mild'\\n        else:\\n            psi_severity = 'severe'\\n        \\n        results['drift_detected'] = ks_drift or psi_severity != 'none'\\n        results['severity'] = psi_severity\\n        \\n        return results\\n    \\n    def analyze_categorical_feature(self, feature):\\n        \\\"\\\"\\\"‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå drift ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö categorical feature\\\"\\\"\\\"\\n        chi2_result = self.chi_squared_test(feature)\\n        \\n        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì PSI ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö categorical\\n        ref_props = self.reference[feature].value_counts(normalize=True)\\n        cur_props = self.current[feature].value_counts(normalize=True)\\n        \\n        all_cats = set(ref_props.index) | set(cur_props.index)\\n        eps = 1e-6\\n        psi = 0\\n        for cat in all_cats:\\n            ref_p = ref_props.get(cat, 0) + eps\\n            cur_p = cur_props.get(cat, 0) + eps\\n            psi += (cur_p - ref_p) * np.log(cur_p / ref_p)\\n        \\n        results = {\\n            'feature': feature,\\n            'type': 'categorical',\\n            'chi_squared': chi2_result,\\n            'psi': {'psi': psi}\\n        }\\n        \\n        # ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•\\n        chi2_drift = chi2_result['p_value'] < 0.05\\n        \\n        if psi < 0.1:\\n            psi_severity = 'none'\\n        elif psi < 0.25:\\n            psi_severity = 'mild'\\n        else:\\n            psi_severity = 'severe'\\n        \\n        results['drift_detected'] = chi2_drift or psi_severity != 'none'\\n        results['severity'] = psi_severity\\n        \\n        return results\\n    \\n    def analyze_all_features(self):\\n        \\\"\\\"\\\"‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå drift ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏∏‡∏Å features\\\"\\\"\\\"\\n        all_results = {}\\n        \\n        # Numerical features\\n        for feature in self.numerical_features:\\n            all_results[feature] = self.analyze_numerical_feature(feature)\\n        \\n        # Categorical features\\n        for feature in self.categorical_features:\\n            all_results[feature] = self.analyze_categorical_feature(feature)\\n        \\n        self.results = all_results\\n        return all_results\\n    \\n    def get_summary_report(self):\\n        \\\"\\\"\\\"‡∏™‡∏£‡πâ‡∏≤‡∏á summary report\\\"\\\"\\\"\\n        if not self.results:\\n            self.analyze_all_features()\\n        \\n        summary = []\\n        for feature, result in self.results.items():\\n            row = {\\n                'Feature': feature,\\n                'Type': result['type'],\\n                'Drift Detected': '‚úì' if result['drift_detected'] else '‚úó',\\n                'Severity': result['severity'].upper(),\\n                'PSI': f\\\"{result['psi']['psi']:.4f}\\\"\\n            }\\n            \\n            if result['type'] == 'numerical':\\n                row['KS p-value'] = f\\\"{result['ks_test']['p_value']:.4f}\\\"\\n            else:\\n                row['Chi2 p-value'] = f\\\"{result['chi_squared']['p_value']:.4f}\\\"\\n            \\n            summary.append(row)\\n        \\n        return pd.DataFrame(summary)\\n\\n#%%\\n# ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô FeatureDriftDetector\\ndetector = FeatureDriftDetector(\\n    reference_data=reference_df,\\n    current_data=current_df,\\n    numerical_features=feature_info['numerical'],\\n    categorical_features=feature_info['categorical']\\n)\\n\\n# ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ó‡∏∏‡∏Å features\\nall_results = detector.analyze_all_features()\\n\\n# ‡πÅ‡∏™‡∏î‡∏á summary report\\nprint(\\\"=\\\" * 80)\\nprint(\\\"üìä FEATURE DRIFT DETECTION REPORT\\\")\\nprint(\\\"=\\\" * 80)\\nsummary_df = detector.get_summary_report()\\nprint(summary_df.to_string(index=False))\\n\\n#%% [markdown]\\n# ## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 4: Visualize Feature Distributions\\n# \\n# ‡∏Å‡∏≤‡∏£ visualize ‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏´‡πâ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à drift ‡πÑ‡∏î‡πâ‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô\\n\\n#%%\\ndef plot_numerical_feature_drift(reference, current, feature_name, ax=None):\\n    \\\"\\\"\\\"\\n    ‡∏™‡∏£‡πâ‡∏≤‡∏á visualization ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö numerical feature drift\\n    \\\"\\\"\\\"\\n    if ax is None:\\n        fig, ax = plt.subplots(figsize=(10, 4))\\n    \\n    # Histogram\\n    ax.hist(reference[feature_name], bins=30, alpha=0.5, label='Reference', color='blue', density=True)\\n    ax.hist(current[feature_name], bins=30, alpha=0.5, label='Current', color='red', density=True)\\n    \\n    # Statistics\\n    ref_mean = reference[feature_name].mean()\\n    cur_mean = current[feature_name].mean()\\n    \\n    ax.axvline(ref_mean, color='blue', linestyle='--', linewidth=2)\\n    ax.axvline(cur_mean, color='red', linestyle='--', linewidth=2)\\n    \\n    # Calculate PSI for title\\n    detector_temp = FeatureDriftDetector(reference, current, [feature_name], [])\\n    result = detector_temp.analyze_numerical_feature(feature_name)\\n    \\n    ax.set_title(f'{feature_name}\\\\nPSI: {result[\\\"psi\\\"][\\\"psi\\\"]:.4f} | Severity: {result[\\\"severity\\\"].upper()}')\\n    ax.set_xlabel(feature_name)\\n    ax.set_ylabel('Density')\\n    ax.legend()\\n    \\n    return ax\\n\\ndef plot_categorical_feature_drift(reference, current, feature_name, ax=None):\\n    \\\"\\\"\\\"\\n    ‡∏™‡∏£‡πâ‡∏≤‡∏á visualization ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö categorical feature drift\\n    \\\"\\\"\\\"\\n    if ax is None:\\n        fig, ax = plt.subplots(figsize=(10, 4))\\n    \\n    # Calculate proportions\\n    ref_props = reference[feature_name].value_counts(normalize=True).sort_index()\\n    cur_props = current[feature_name].value_counts(normalize=True).sort_index()\\n    \\n    # Align categories\\n    all_cats = sorted(set(ref_props.index) | set(cur_props.index))\\n    ref_values = [ref_props.get(cat, 0) for cat in all_cats]\\n    cur_values = [cur_props.get(cat, 0) for cat in all_cats]\\n    \\n    x = np.arange(len(all_cats))\\n    width = 0.35\\n    \\n    ax.bar(x - width/2, ref_values, width, label='Reference', color='blue', alpha=0.7)\\n    ax.bar(x + width/2, cur_values, width, label='Current', color='red', alpha=0.7)\\n    \\n    ax.set_xticks(x)\\n    ax.set_xticklabels([f'Cat {c}' for c in all_cats])\\n    \\n    # Calculate PSI for title\\n    detector_temp = FeatureDriftDetector(reference, current, [], [feature_name])\\n    result = detector_temp.analyze_categorical_feature(feature_name)\\n    \\n    ax.set_title(f'{feature_name}\\\\nPSI: {result[\\\"psi\\\"][\\\"psi\\\"]:.4f} | Severity: {result[\\\"severity\\\"].upper()}')\\n    ax.set_xlabel('Category')\\n    ax.set_ylabel('Proportion')\\n    ax.legend()\\n    \\n    return ax\\n\\n#%%\\n# Plot all numerical features\\nfig, axes = plt.subplots(2, 3, figsize=(15, 10))\\naxes = axes.flatten()\\n\\nfor idx, feature in enumerate(feature_info['numerical']):\\n    plot_numerical_feature_drift(reference_df, current_df, feature, axes[idx])\\n\\n# Remove empty subplot\\nif len(feature_info['numerical']) < 6:\\n    axes[-1].set_visible(False)\\n\\nplt.suptitle('Numerical Features: Distribution Comparison\\\\n(Reference vs Current)', fontsize=14, fontweight='bold')\\nplt.tight_layout()\\nplt.savefig('numerical_features_drift.png', dpi=150, bbox_inches='tight')\\nplt.show()\\n\\n#%%\\n# Plot all categorical features\\nfig, axes = plt.subplots(1, 3, figsize=(15, 4))\\n\\nfor idx, feature in enumerate(feature_info['categorical']):\\n    plot_categorical_feature_drift(reference_df, current_df, feature, axes[idx])\\n\\nplt.suptitle('Categorical Features: Distribution Comparison\\\\n(Reference vs Current)', fontsize=14, fontweight='bold')\\nplt.tight_layout()\\nplt.savefig('categorical_features_drift.png', dpi=150, bbox_inches='tight')\\nplt.show()\\n\\n#%% [markdown]\\n# ## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 5: Feature Drift Ranking ‡πÅ‡∏•‡∏∞ Prioritization\\n# \\n# ‡∏à‡∏±‡∏î‡∏•‡∏≥‡∏î‡∏±‡∏ö features ‡∏ï‡∏≤‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏∏‡∏ô‡πÅ‡∏£‡∏á‡∏Ç‡∏≠‡∏á drift\\n\\n#%%\\ndef rank_features_by_drift(detector):\\n    \\\"\\\"\\\"\\n    ‡∏à‡∏±‡∏î‡∏•‡∏≥‡∏î‡∏±‡∏ö features ‡∏ï‡∏≤‡∏° drift severity\\n    \\\"\\\"\\\"\\n    if not detector.results:\\n        detector.analyze_all_features()\\n    \\n    rankings = []\\n    for feature, result in detector.results.items():\\n        rankings.append({\\n            'feature': feature,\\n            'psi': result['psi']['psi'],\\n            'drift_detected': result['drift_detected'],\\n            'severity': result['severity'],\\n            'type': result['type']\\n        })\\n    \\n    # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏ï‡∏≤‡∏° PSI (‡∏°‡∏≤‡∏Å‡πÑ‡∏õ‡∏ô‡πâ‡∏≠‡∏¢)\\n    rankings.sort(key=lambda x: x['psi'], reverse=True)\\n    \\n    return rankings\\n\\n# ‡∏à‡∏±‡∏î‡∏•‡∏≥‡∏î‡∏±‡∏ö features\\nrankings = rank_features_by_drift(detector)\\n\\nprint(\\\"=\\\" * 60)\\nprint(\\\"üìä FEATURE DRIFT RANKING (Sorted by PSI)\\\")\\nprint(\\\"=\\\" * 60)\\n\\nfor rank, item in enumerate(rankings, 1):\\n    severity_emoji = {'none': 'üü¢', 'mild': 'üü°', 'severe': 'üî¥'}[item['severity']]\\n    print(f\\\"{rank}. {item['feature']:<20} | PSI: {item['psi']:.4f} | {severity_emoji} {item['severity'].upper()}\\\")\\n\\n#%%\\n# ‡∏™‡∏£‡πâ‡∏≤‡∏á Drift Ranking Visualization\\nfig, ax = plt.subplots(figsize=(12, 6))\\n\\nfeatures = [r['feature'] for r in rankings]\\npsi_values = [r['psi'] for r in rankings]\\ncolors = ['red' if r['severity'] == 'severe' else 'orange' if r['severity'] == 'mild' else 'green' for r in rankings]\\n\\nbars = ax.barh(features, psi_values, color=colors, alpha=0.7, edgecolor='black')\\n\\n# Add threshold lines\\nax.axvline(0.1, color='orange', linestyle='--', label='Mild Threshold (0.1)')\\nax.axvline(0.25, color='red', linestyle='--', label='Severe Threshold (0.25)')\\n\\nax.set_xlabel('PSI Value')\\nax.set_ylabel('Feature')\\nax.set_title('Feature Drift Ranking by PSI\\\\n(Red=Severe, Orange=Mild, Green=None)')\\nax.legend()\\n\\n# Add value labels\\nfor bar, psi in zip(bars, psi_values):\\n    ax.text(psi + 0.01, bar.get_y() + bar.get_height()/2, f'{psi:.3f}', va='center')\\n\\nplt.tight_layout()\\nplt.savefig('feature_drift_ranking.png', dpi=150, bbox_inches='tight')\\nplt.show()\\n\\n#%% [markdown]\\n# ## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 6: Time-based Distribution Analysis\\n# \\n# ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ß‡πà‡∏≤ distribution ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÄ‡∏ß‡∏•‡∏≤‡∏ú‡πà‡∏≤‡∏ô‡πÑ‡∏õ\\n\\n#%%\\ndef simulate_time_series_data(n_periods=6, samples_per_period=500):\\n    \\\"\\\"\\\"\\n    ‡∏à‡∏≥‡∏•‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏ï‡∏≤‡∏°‡πÄ‡∏ß‡∏•‡∏≤\\n    \\\"\\\"\\\"\\n    all_data = []\\n    \\n    for period in range(n_periods):\\n        np.random.seed(42 + period)\\n        \\n        # Gradual drift: mean ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡∏∂‡πâ‡∏ô‡∏ó‡∏µ‡∏•‡∏∞‡∏ô‡πâ‡∏≠‡∏¢\\n        age_mean = 35 + period * 2  # drift ‡πÉ‡∏ô age\\n        income_mean = 50000  # ‡πÑ‡∏°‡πà‡∏°‡∏µ drift\\n        \\n        data = pd.DataFrame({\\n            'period': period,\\n            'age': np.random.normal(age_mean, 10, samples_per_period),\\n            'income': np.random.normal(income_mean, 15000, samples_per_period),\\n            'credit_score': np.random.normal(650, 80, samples_per_period)\\n        })\\n        all_data.append(data)\\n    \\n    return pd.concat(all_data, ignore_index=True)\\n\\n# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• time series\\ntime_series_data = simulate_time_series_data()\\nprint(f\\\"üìä Time Series Data Shape: {time_series_data.shape}\\\")\\nprint(f\\\"üìã Periods: {time_series_data['period'].unique()}\\\")\\n\\n#%%\\n# Visualize distribution over time\\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\\n\\nfeatures_to_plot = ['age', 'income', 'credit_score']\\ncolors = plt.cm.viridis(np.linspace(0, 1, 6))\\n\\nfor idx, feature in enumerate(features_to_plot):\\n    ax = axes[idx]\\n    \\n    for period in range(6):\\n        period_data = time_series_data[time_series_data['period'] == period][feature]\\n        ax.hist(period_data, bins=20, alpha=0.3, color=colors[period], label=f'Period {period}')\\n        ax.axvline(period_data.mean(), color=colors[period], linestyle='--', alpha=0.8)\\n    \\n    ax.set_title(f'{feature.capitalize()} Distribution Over Time')\\n    ax.set_xlabel(feature)\\n    ax.set_ylabel('Frequency')\\n    ax.legend(fontsize=8)\\n\\nplt.suptitle('Feature Distributions Across Time Periods\\\\n(Dashed lines = Mean per period)', fontsize=12, fontweight='bold')\\nplt.tight_layout()\\nplt.savefig('time_series_distributions.png', dpi=150, bbox_inches='tight')\\nplt.show()\\n\\n#%%\\n# Calculate PSI over time (comparing each period to Period 0)\\ndef calculate_psi_over_time(data, feature, reference_period=0):\\n    \\\"\\\"\\\"\\n    ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì PSI ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Å‡∏±‡∏ö reference period\\n    \\\"\\\"\\\"\\n    ref_data = data[data['period'] == reference_period][feature]\\n    \\n    results = []\\n    for period in data['period'].unique():\\n        if period == reference_period:\\n            results.append({'period': period, 'psi': 0})\\n        else:\\n            cur_data = data[data['period'] == period][feature]\\n            \\n            # Calculate PSI\\n            breakpoints = np.percentile(ref_data, np.linspace(0, 100, 11))\\n            breakpoints = np.unique(breakpoints)\\n            \\n            ref_counts, _ = np.histogram(ref_data, bins=breakpoints)\\n            cur_counts, _ = np.histogram(cur_data, bins=breakpoints)\\n            \\n            eps = 1e-6\\n            ref_props = ref_counts / len(ref_data) + eps\\n            cur_props = cur_counts / len(cur_data) + eps\\n            \\n            psi = np.sum((cur_props - ref_props) * np.log(cur_props / ref_props))\\n            results.append({'period': period, 'psi': psi})\\n    \\n    return pd.DataFrame(results)\\n\\n# ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì PSI ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞ feature\\nfig, ax = plt.subplots(figsize=(10, 6))\\n\\nfor feature in ['age', 'income', 'credit_score']:\\n    psi_over_time = calculate_psi_over_time(time_series_data, feature)\\n    ax.plot(psi_over_time['period'], psi_over_time['psi'], marker='o', label=feature, linewidth=2)\\n\\nax.axhline(0.1, color='orange', linestyle='--', alpha=0.7, label='Mild Threshold')\\nax.axhline(0.25, color='red', linestyle='--', alpha=0.7, label='Severe Threshold')\\n\\nax.set_xlabel('Time Period')\\nax.set_ylabel('PSI (compared to Period 0)')\\nax.set_title('PSI Trend Over Time\\\\n(Reference: Period 0)')\\nax.legend()\\nax.grid(True, alpha=0.3)\\n\\nplt.tight_layout()\\nplt.savefig('psi_over_time.png', dpi=150, bbox_inches='tight')\\nplt.show()\\n\\nprint(\\\"\\\\nüí° ‡∏™‡∏±‡∏á‡πÄ‡∏Å‡∏ï: Age ‡πÅ‡∏™‡∏î‡∏á gradual drift (PSI ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡∏∂‡πâ‡∏ô‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏¢‡πÜ)\\\")\\nprint(\\\"   ‡πÉ‡∏ô‡∏Ç‡∏ì‡∏∞‡∏ó‡∏µ‡πà Income ‡πÅ‡∏•‡∏∞ Credit Score ‡∏Ñ‡πà‡∏≠‡∏ô‡∏Ç‡πâ‡∏≤‡∏á stable\\\")\\n\\n#%% [markdown]\\n# ## ‡∏™‡∏£‡∏∏‡∏õ LAB 2\\n# \\n# ### ‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ:\\n# 1. **FeatureDriftDetector Class**: ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏°‡∏∑‡∏≠‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå drift ‡∏ó‡∏∏‡∏Å features\\n# 2. **Numerical vs Categorical**: ‡πÉ‡∏ä‡πâ methods ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡∏Å‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞ type\\n# 3. **Visualization**: ‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏´‡πâ‡πÄ‡∏´‡πá‡∏ô drift ‡πÑ‡∏î‡πâ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô\\n# 4. **Ranking**: ‡∏à‡∏±‡∏î‡∏•‡∏≥‡∏î‡∏±‡∏ö features ‡∏ï‡∏≤‡∏° severity ‡πÄ‡∏û‡∏∑‡πà‡∏≠ prioritization\\n# 5. **Time-based Analysis**: ‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏° drift ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÄ‡∏ß‡∏•‡∏≤‡∏ú‡πà‡∏≤‡∏ô‡πÑ‡∏õ\\n\\n#%%\\nprint(\\\"=\\\" * 60)\\nprint(\\\"‚úÖ LAB 2 COMPLETED!\\\")\\nprint(\\\"=\\\" * 60)\\nprint(\\\"\\\"\\\"\\nüìö Key Takeaways:\\n1. Numerical features: ‡πÉ‡∏ä‡πâ KS Test + PSI + Wasserstein\\n2. Categorical features: ‡πÉ‡∏ä‡πâ Chi-squared + PSI\\n3. Visualization ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à drift\\n4. Track PSI over time ‡πÄ‡∏û‡∏∑‡πà‡∏≠ detect gradual drift\\n\\nüîú Next: LAB 3 - Multivariate Drift Analysis\\n\\\"\\\"\\\")\\n```\\n\\n---\\n\\n## LAB 3: Multivariate Drift Analysis\\n\\n```python\\n#%% [markdown]\\n# # LAB 3: Multivariate Drift Analysis\\n# ## ‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå Drift ‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏¥‡∏î‡∏à‡∏≤‡∏Å‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á Features\\n# \\n# ### ‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ:\\n# 1. ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö drift ‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏¥‡∏î‡∏à‡∏≤‡∏Å‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á features\\n# 2. ‡πÉ‡∏ä‡πâ Dataset-level drift detection\\n# 3. ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå Correlation changes ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á features\\n#\\n# ### ‡∏ó‡∏§‡∏©‡∏é‡∏µ:\\n# **Multivariate Drift** ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡∏∂‡πâ‡∏ô‡πÄ‡∏°‡∏∑‡πà‡∏≠:\\n# - ‡πÅ‡∏ï‡πà‡∏•‡∏∞ feature ‡∏î‡∏π‡∏õ‡∏Å‡∏ï‡∏¥‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÅ‡∏¢‡∏Å\\n# - ‡πÅ‡∏ï‡πà‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á features ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÑ‡∏õ\\n# - ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: correlation ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á age ‡πÅ‡∏•‡∏∞ income ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô\\n\\n#%% [markdown]\\n# ## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 1: ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° Environment\\n\\n#%%\\nimport numpy as np\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\nfrom scipy import stats\\nfrom sklearn.decomposition import PCA\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.covariance import EmpiricalCovariance\\nimport warnings\\nwarnings.filterwarnings('ignore')\\n\\nnp.random.seed(42)\\nplt.rcParams['figure.figsize'] = (12, 8)\\n\\nprint(\\\"‚úÖ Libraries imported successfully!\\\")\\n\\n#%% [markdown]\\n# ## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 2: ‡∏™‡∏£‡πâ‡∏≤‡∏á Dataset ‡∏ó‡∏µ‡πà‡∏°‡∏µ Multivariate Drift\\n# \\n# ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà:\\n# - Marginal distributions ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Å‡∏±‡∏ô (‡πÑ‡∏°‡πà‡∏°‡∏µ univariate drift)\\n# - ‡πÅ‡∏ï‡πà correlation structure ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô (multivariate drift)\\n\\n#%%\\ndef create_multivariate_drift_data(n_samples=2000):\\n    \\\"\\\"\\\"\\n    ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ multivariate drift\\n    - Marginal distributions ‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢‡∏Å‡∏±‡∏ô\\n    - Correlation structure ‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô\\n    \\\"\\\"\\\"\\n    np.random.seed(42)\\n    \\n    # === Reference Data ===\\n    # Correlated features: age ‡πÅ‡∏•‡∏∞ income ‡∏°‡∏µ positive correlation ‡∏™‡∏π‡∏á\\n    mean_ref = [35, 50000, 650]  # age, income, credit_score\\n    cov_ref = [\\n        [100, 3000, 50],      # age variance ‡πÅ‡∏•‡∏∞ covariance\\n        [3000, 225000000, 100000],  # income variance ‡πÅ‡∏•‡∏∞ covariance\\n        [50, 100000, 6400]    # credit_score variance ‡πÅ‡∏•‡∏∞ covariance\\n    ]\\n    \\n    ref_data = np.random.multivariate_normal(mean_ref, cov_ref, n_samples)\\n    reference_df = pd.DataFrame(ref_data, columns=['age', 'income', 'credit_score'])\\n    \\n    # === Current Data (Multivariate Drift) ===\\n    # Same marginals but different correlation structure\\n    mean_cur = [35, 50000, 650]  # means ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°\\n    \\n    # Correlation structure ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô: age ‡πÅ‡∏•‡∏∞ income correlation ‡∏•‡∏î‡∏•‡∏á\\n    cov_cur = [\\n        [100, 500, 50],       # correlation ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á age-income ‡∏•‡∏î‡∏•‡∏á‡∏°‡∏≤‡∏Å!\\n        [500, 225000000, 100000],\\n        [50, 100000, 6400]\\n    ]\\n    \\n    cur_data = np.random.multivariate_normal(mean_cur, cov_cur, n_samples)\\n    current_df = pd.DataFrame(cur_data, columns=['age', 'income', 'credit_score'])\\n    \\n    return reference_df, current_df\\n\\n# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•\\nref_multi, cur_multi = create_multivariate_drift_data()\\n\\nprint(\\\"üìä Reference Data:\\\")\\nprint(ref_multi.describe())\\nprint(\\\"\\\\nüìä Current Data:\\\")\\nprint(cur_multi.describe())\\n\\n#%%\\n# ‡πÅ‡∏™‡∏î‡∏á correlation matrix comparison\\nfig, axes = plt.subplots(1, 3, figsize=(15, 4))\\n\\n# Reference correlation matrix\\ncorr_ref = ref_multi.corr()\\nsns.heatmap(corr_ref, annot=True, cmap='coolwarm', center=0, ax=axes[0], \\n            vmin=-1, vmax=1, fmt='.3f')\\naxes[0].set_title('Reference Correlation Matrix')\\n\\n# Current correlation matrix\\ncorr_cur = cur_multi.corr()\\nsns.heatmap(corr_cur, annot=True, cmap='coolwarm', center=0, ax=axes[1],\\n            vmin=-1, vmax=1, fmt='.3f')\\naxes[1].set_title('Current Correlation Matrix')\\n\\n# Difference\\ncorr_diff = corr_cur - corr_ref\\nsns.heatmap(corr_diff, annot=True, cmap='RdBu', center=0, ax=axes[2],\\n            vmin=-1, vmax=1, fmt='.3f')\\naxes[2].set_title('Correlation Difference\\\\n(Current - Reference)')\\n\\nplt.tight_layout()\\nplt.savefig('correlation_comparison.png', dpi=150, bbox_inches='tight')\\nplt.show()\\n\\nprint(\\\"\\\\nüí° ‡∏™‡∏±‡∏á‡πÄ‡∏Å‡∏ï: Correlation ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á Age-Income ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏à‡∏≤‡∏Å ~0.6 ‡πÄ‡∏õ‡πá‡∏ô ~0.03\\\")\\n\\n#%% [markdown]\\n# ## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 3: Univariate vs Multivariate Drift Detection\\n# \\n# ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏ß‡πà‡∏≤ univariate methods ‡∏û‡∏•‡∏≤‡∏î‡∏≠‡∏∞‡πÑ‡∏£‡πÑ‡∏õ\\n\\n#%%\\nfrom scipy import stats\\n\\ndef univariate_drift_check(ref_df, cur_df):\\n    \\\"\\\"\\\"‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö drift ‡∏î‡πâ‡∏ß‡∏¢ univariate methods\\\"\\\"\\\"\\n    results = []\\n    \\n    for col in ref_df.columns:\\n        # KS Test\\n        ks_stat, ks_pval = stats.ks_2samp(ref_df[col], cur_df[col])\\n        \\n        # Mean comparison\\n        ref_mean = ref_df[col].mean()\\n        cur_mean = cur_df[col].mean()\\n        mean_diff_pct = abs(cur_mean - ref_mean) / ref_mean * 100\\n        \\n        # Std comparison\\n        ref_std = ref_df[col].std()\\n        cur_std = cur_df[col].std()\\n        std_diff_pct = abs(cur_std - ref_std) / ref_std * 100\\n        \\n        results.append({\\n            'feature': col,\\n            'ks_statistic': ks_stat,\\n            'ks_pvalue': ks_pval,\\n            'drift_detected': ks_pval < 0.05,\\n            'mean_diff_%': mean_diff_pct,\\n            'std_diff_%': std_diff_pct\\n        })\\n    \\n    return pd.DataFrame(results)\\n\\n# ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö univariate drift\\nunivariate_results = univariate_drift_check(ref_multi, cur_multi)\\nprint(\\\"=\\\" * 60)\\nprint(\\\"üìä UNIVARIATE DRIFT DETECTION RESULTS\\\")\\nprint(\\\"=\\\" * 60)\\nprint(univariate_results.to_string(index=False))\\n\\nprint(\\\"\\\\nüí° ‡∏™‡∏±‡∏á‡πÄ‡∏Å‡∏ï: Univariate methods ‡πÑ‡∏°‡πà‡∏û‡∏ö drift ‡∏ó‡∏µ‡πà‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç!\\\")\\nprint(\\\"   ‡πÅ‡∏ï‡πà‡πÄ‡∏£‡∏≤‡∏£‡∏π‡πâ‡∏ß‡πà‡∏≤ correlation structure ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÑ‡∏õ\\\")\\n\\n#%% [markdown]\\n# ## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 4: Correlation-based Drift Detection\\n# \\n# ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡∏≠‡∏á correlation structure\\n\\n#%%\\ndef correlation_drift_test(ref_df, cur_df, significance_level=0.05):\\n    \\\"\\\"\\\"\\n    ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö drift ‡πÉ‡∏ô correlation structure\\n    \\n    ‡πÉ‡∏ä‡πâ Fisher's Z transformation ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö correlations\\n    \\\"\\\"\\\"\\n    results = []\\n    \\n    # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì correlation matrices\\n    ref_corr = ref_df.corr()\\n    cur_corr = cur_df.corr()\\n    \\n    n_ref = len(ref_df)\\n    n_cur = len(cur_df)\\n    \\n    # ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞ pair\\n    for i, col1 in enumerate(ref_df.columns):\\n        for j, col2 in enumerate(ref_df.columns):\\n            if i >= j:  # ‡∏Ç‡πâ‡∏≤‡∏° diagonal ‡πÅ‡∏•‡∏∞ lower triangle\\n                continue\\n            \\n            r_ref = ref_corr.loc[col1, col2]\\n            r_cur = cur_corr.loc[col1, col2]\\n            \\n            # Fisher's Z transformation\\n            z_ref = np.arctanh(r_ref)\\n            z_cur = np.arctanh(r_cur)\\n            \\n            # Standard error\\n            se = np.sqrt(1/(n_ref-3) + 1/(n_cur-3))\\n            \\n            # Z-test statistic\\n            z_stat = (z_ref - z_cur) / se\\n            p_value = 2 * (1 - stats.norm.cdf(abs(z_stat)))\\n            \\n            results.append({\\n                'feature_pair': f'{col1} - {col2}',\\n                'ref_correlation': r_ref,\\n                'cur_correlation': r_cur,\\n                'correlation_change': r_cur - r_ref,\\n                'z_statistic': z_stat,\\n                'p_value': p_value,\\n                'significant_change': p_value < significance_level\\n            })\\n    \\n    return pd.DataFrame(results)\\n\\n# ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö correlation drift\\ncorr_drift_results = correlation_drift_test(ref_multi, cur_multi)\\nprint(\\\"=\\\" * 60)\\nprint(\\\"üìä CORRELATION DRIFT DETECTION RESULTS\\\")\\nprint(\\\"=\\\" * 60)\\nprint(corr_drift_results.to_string(index=False))\\n\\n#%%\\n# Visualize correlation changes\\nfig, ax = plt.subplots(figsize=(10, 6))\\n\\nx = range(len(corr_drift_results))\\ncolors = ['red' if sig else 'green' for sig in corr_drift_results['significant_change']]\\n\\nbars = ax.bar(x, corr_drift_results['correlation_change'], color=colors, alpha=0.7, edgecolor='black')\\n\\nax.set_xticks(x)\\nax.set_xticklabels(corr_drift_results['feature_pair'], rotation=45, ha='right')\\nax.set_ylabel('Correlation Change')\\nax.set_title('Correlation Changes Between Reference and Current Data\\\\n(Red = Statistically Significant)')\\nax.axhline(0, color='black', linestyle='-', linewidth=0.5)\\n\\n# Add value labels\\nfor bar, val in zip(bars, corr_drift_results['correlation_change']):\\n    height = bar.get_height()\\n    ax.text(bar.get_x() + bar.get_width()/2., height, f'{val:.3f}',\\n            ha='center', va='bottom' if height > 0 else 'top')\\n\\nplt.tight_layout()\\nplt.savefig('correlation_drift.png', dpi=150, bbox_inches='tight')\\nplt.show()\\n\\n#%% [markdown]\\n# ## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 5: PCA-based Multivariate Drift Detection\\n# \\n# ‡πÉ‡∏ä‡πâ PCA ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö drift ‡πÉ‡∏ô multivariate structure\\n\\n#%%\\ndef pca_drift_detection(ref_df, cur_df, n_components=None):\\n    \\\"\\\"\\\"\\n    ‡πÉ‡∏ä‡πâ PCA ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö multivariate drift\\n    \\n    ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö:\\n    1. Explained variance ratios\\n    2. Principal component directions\\n    3. Reconstruction errors\\n    \\\"\\\"\\\"\\n    if n_components is None:\\n        n_components = min(len(ref_df.columns), 3)\\n    \\n    # Standardize data\\n    scaler = StandardScaler()\\n    ref_scaled = scaler.fit_transform(ref_df)\\n    cur_scaled = scaler.transform(cur_df)\\n    \\n    # Fit PCA on reference data\\n    pca_ref = PCA(n_components=n_components)\\n    pca_ref.fit(ref_scaled)\\n    \\n    # Transform both datasets using reference PCA\\n    ref_transformed = pca_ref.transform(ref_scaled)\\n    cur_transformed = pca_ref.transform(cur_scaled)\\n    \\n    # 1. Compare explained variance\\n    ref_explained_var = pca_ref.explained_variance_ratio_\\n    \\n    # Fit PCA on current data for comparison\\n    pca_cur = PCA(n_components=n_components)\\n    pca_cur.fit(cur_scaled)\\n    cur_explained_var = pca_cur.explained_variance_ratio_\\n    \\n    # 2. Compare principal components (using cosine similarity)\\n    component_similarities = []\\n    for i in range(n_components):\\n        cos_sim = np.dot(pca_ref.components_[i], pca_cur.components_[i])\\n        cos_sim = abs(cos_sim)  # Absolute value ‡πÄ‡∏û‡∏£‡∏≤‡∏∞ sign ‡∏≠‡∏≤‡∏à flip\\n        component_similarities.append(cos_sim)\\n    \\n    # 3. Reconstruction error on current data using reference PCA\\n    ref_reconstructed = pca_ref.inverse_transform(ref_transformed)\\n    cur_reconstructed = pca_ref.inverse_transform(cur_transformed)\\n    \\n    ref_recon_error = np.mean((ref_scaled - ref_reconstructed) ** 2)\\n    cur_recon_error = np.mean((cur_scaled - cur_reconstructed) ** 2)\\n    \\n    results = {\\n        'ref_explained_variance': ref_explained_var,\\n        'cur_explained_variance': cur_explained_var,\\n        'component_similarities': component_similarities,\\n        'ref_reconstruction_error': ref_recon_error,\\n        'cur_reconstruction_error': cur_recon_error,\\n        'reconstruction_error_ratio': cur_recon_error / ref_recon_error if ref_recon_error > 0 else 1,\\n        'ref_components': pca_ref.components_,\\n        'cur_components': pca_cur.components_,\\n        'pca_ref': pca_ref,\\n        'ref_transformed': ref_transformed,\\n        'cur_transformed': cur_transformed\\n    }\\n    \\n    return results\\n\\n# ‡∏ó‡∏≥ PCA drift detection\\npca_results = pca_drift_detection(ref_multi, cur_multi)\\n\\nprint(\\\"=\\\" * 60)\\nprint(\\\"üìä PCA-BASED DRIFT DETECTION RESULTS\\\")\\nprint(\\\"=\\\" * 60)\\n\\nprint(\\\"\\\\n1. Explained Variance Comparison:\\\")\\nfor i in range(len(pca_results['ref_explained_variance'])):\\n    ref_ev = pca_results['ref_explained_variance'][i]\\n    cur_ev = pca_results['cur_explained_variance'][i]\\n    print(f\\\"   PC{i+1}: Reference = {ref_ev:.4f}, Current = {cur_ev:.4f}, Diff = {cur_ev - ref_ev:.4f}\\\")\\n\\nprint(\\\"\\\\n2. Component Similarities (1.0 = identical):\\\")\\nfor i, sim in enumerate(pca_results['component_similarities']):\\n    status = \\\"‚úì Similar\\\" if sim > 0.9 else \\\"‚ö†Ô∏è Changed!\\\"\\n    print(f\\\"   PC{i+1}: Cosine Similarity = {sim:.4f} {status}\\\")\\n\\nprint(\\\"\\\\n3. Reconstruction Error:\\\")\\nprint(f\\\"   Reference: {pca_results['ref_reconstruction_error']:.6f}\\\")\\nprint(f\\\"   Current: {pca_results['cur_reconstruction_error']:.6f}\\\")\\nprint(f\\\"   Ratio: {pca_results['reconstruction_error_ratio']:.4f}x\\\")\\n\\n#%%\\n# Visualize PCA results\\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\\n\\n# Plot 1: Explained variance comparison\\nax1 = axes[0, 0]\\nx = range(1, len(pca_results['ref_explained_variance']) + 1)\\nwidth = 0.35\\nax1.bar([i - width/2 for i in x], pca_results['ref_explained_variance'], \\n        width, label='Reference', color='blue', alpha=0.7)\\nax1.bar([i + width/2 for i in x], pca_results['cur_explained_variance'], \\n        width, label='Current', color='red', alpha=0.7)\\nax1.set_xlabel('Principal Component')\\nax1.set_ylabel('Explained Variance Ratio')\\nax1.set_title('Explained Variance Comparison')\\nax1.legend()\\nax1.set_xticks(x)\\n\\n# Plot 2: Component similarities\\nax2 = axes[0, 1]\\ncolors = ['green' if s > 0.9 else 'red' for s in pca_results['component_similarities']]\\nax2.bar(x, pca_results['component_similarities'], color=colors, alpha=0.7, edgecolor='black')\\nax2.axhline(0.9, color='orange', linestyle='--', label='Similarity Threshold (0.9)')\\nax2.set_xlabel('Principal Component')\\nax2.set_ylabel('Cosine Similarity')\\nax2.set_title('Principal Component Similarity\\\\n(Reference vs Current)')\\nax2.set_xticks(x)\\nax2.legend()\\n\\n# Plot 3: Scatter plot in PC space (Reference)\\nax3 = axes[1, 0]\\nax3.scatter(pca_results['ref_transformed'][:, 0], pca_results['ref_transformed'][:, 1], \\n            alpha=0.3, label='Reference', color='blue')\\nax3.scatter(pca_results['cur_transformed'][:, 0], pca_results['cur_transformed'][:, 1], \\n            alpha=0.3, label='Current', color='red')\\nax3.set_xlabel('PC1')\\nax3.set_ylabel('PC2')\\nax3.set_title('Data in PCA Space\\\\n(Using Reference PCA)')\\nax3.legend()\\n\\n# Plot 4: Component loadings comparison\\nax4 = axes[1, 1]\\nfeatures = ref_multi.columns.tolist()\\nx = np.arange(len(features))\\nwidth = 0.35\\n\\nfor i in range(2):  # ‡πÅ‡∏™‡∏î‡∏á PC1 ‡πÅ‡∏•‡∏∞ PC2\\n    ref_loadings = pca_results['ref_components'][i]\\n    cur_loadings = pca_results['cur_components'][i]\\n    \\n    ax4.barh([f'PC{i+1} Ref' for f in features] if i == 0 else [f'PC{i+1} Cur' for f in features], \\n             ref_loadings if i == 0 else cur_loadings)\\n\\nax4.set_xlabel('Loading')\\nax4.set_title('PC Loadings (First 2 Components)')\\n\\nplt.tight_layout()\\nplt.savefig('pca_drift_analysis.png', dpi=150, bbox_inches='tight')\\nplt.show()\\n\\n#%% [markdown]\\n# ## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 6: Mahalanobis Distance for Dataset-level Drift\\n# \\n# ‡πÉ‡∏ä‡πâ Mahalanobis distance ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ß‡∏±‡∏î multivariate drift\\n\\n#%%\\ndef mahalanobis_drift_detection(ref_df, cur_df, threshold_percentile=95):\\n    \\\"\\\"\\\"\\n    ‡πÉ‡∏ä‡πâ Mahalanobis distance ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö multivariate drift\\n    \\n    ‡∏ß‡∏±‡∏î‡∏ß‡πà‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡∏°‡πà‡∏≠‡∏¢‡∏π‡πà‡∏´‡πà‡∏≤‡∏á‡∏à‡∏≤‡∏Å distribution ‡∏Ç‡∏≠‡∏á reference data ‡πÄ‡∏ó‡πà‡∏≤‡πÑ‡∏£\\n    \\\"\\\"\\\"\\n    # Standardize features\\n    scaler = StandardScaler()\\n    ref_scaled = scaler.fit_transform(ref_df)\\n    cur_scaled = scaler.transform(cur_df)\\n    \\n    # Fit covariance on reference data\\n    cov = EmpiricalCovariance().fit(ref_scaled)\\n    \\n    # Calculate Mahalanobis distances\\n    ref_distances = cov.mahalanobis(ref_scaled)\\n    cur_distances = cov.mahalanobis(cur_scaled)\\n    \\n    # Determine threshold from reference data\\n    threshold = np.percentile(ref_distances, threshold_percentile)\\n    \\n    # Count outliers\\n    ref_outliers = np.sum(ref_distances > threshold) / len(ref_distances) * 100\\n    cur_outliers = np.sum(cur_distances > threshold) / len(cur_distances) * 100\\n    \\n    # Statistical comparison\\n    ks_stat, ks_pval = stats.ks_2samp(ref_distances, cur_distances)\\n    \\n    results = {\\n        'ref_distances': ref_distances,\\n        'cur_distances': cur_distances,\\n        'threshold': threshold,\\n        'ref_mean_distance': np.mean(ref_distances),\\n        'cur_mean_distance': np.mean(cur_distances),\\n        'ref_outlier_pct': ref_outliers,\\n        'cur_outlier_pct': cur_outliers,\\n        'ks_statistic': ks_stat,\\n        'ks_pvalue': ks_pval,\\n        'drift_detected': ks_pval < 0.05\\n    }\\n    \\n    return results\\n\\n# ‡∏ó‡∏≥ Mahalanobis drift detection\\nmaha_results = mahalanobis_drift_detection(ref_multi, cur_multi)\\n\\nprint(\\\"=\\\" * 60)\\nprint(\\\"üìä MAHALANOBIS DISTANCE DRIFT DETECTION\\\")\\nprint(\\\"=\\\" * 60)\\nprint(f\\\"\\\\nMean Mahalanobis Distance:\\\")\\nprint(f\\\"  Reference: {maha_results['ref_mean_distance']:.4f}\\\")\\nprint(f\\\"  Current: {maha_results['cur_mean_distance']:.4f}\\\")\\nprint(f\\\"\\\\nOutlier Percentage (beyond {maha_results['threshold']:.2f} threshold):\\\")\\nprint(f\\\"  Reference: {maha_results['ref_outlier_pct']:.2f}%\\\")\\nprint(f\\\"  Current: {maha_results['cur_outlier_pct']:.2f}%\\\")\\nprint(f\\\"\\\\nKS Test on Distances:\\\")\\nprint(f\\\"  Statistic: {maha_results['ks_statistic']:.4f}\\\")\\nprint(f\\\"  P-value: {maha_results['ks_pvalue']:.4f}\\\")\\nprint(f\\\"  Drift Detected: {'Yes ‚úì' if maha_results['drift_detected'] else 'No'}\\\")\\n\\n#%%\\n# Visualize Mahalanobis distances\\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\\n\\n# Plot 1: Distribution of Mahalanobis distances\\nax1 = axes[0]\\nax1.hist(maha_results['ref_distances'], bins=50, alpha=0.5, label='Reference', color='blue', density=True)\\nax1.hist(maha_results['cur_distances'], bins=50, alpha=0.5, label='Current', color='red', density=True)\\nax1.axvline(maha_results['threshold'], color='orange', linestyle='--', linewidth=2, \\n            label=f'95th percentile threshold ({maha_results[\\\"threshold\\\"]:.2f})')\\nax1.set_xlabel('Mahalanobis Distance')\\nax1.set_ylabel('Density')\\nax1.set_title('Distribution of Mahalanobis Distances')\\nax1.legend()\\n\\n# Plot 2: CDF comparison\\nax2 = axes[1]\\nsorted_ref = np.sort(maha_results['ref_distances'])\\nsorted_cur = np.sort(maha_results['cur_distances'])\\n\\nax2.plot(sorted_ref, np.arange(1, len(sorted_ref)+1)/len(sorted_ref), \\n         'b-', label='Reference CDF', linewidth=2)\\nax2.plot(sorted_cur, np.arange(1, len(sorted_cur)+1)/len(sorted_cur), \\n         'r-', label='Current CDF', linewidth=2)\\nax2.axvline(maha_results['threshold'], color='orange', linestyle='--', linewidth=2)\\nax2.set_xlabel('Mahalanobis Distance')\\nax2.set_ylabel('Cumulative Probability')\\nax2.set_title(f'CDF Comparison\\\\nKS Statistic = {maha_results[\\\"ks_statistic\\\"]:.4f}')\\nax2.legend()\\n\\nplt.tight_layout()\\nplt.savefig('mahalanobis_drift.png', dpi=150, bbox_inches='tight')\\nplt.show()\\n\\n#%% [markdown]\\n# ## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 7: Comprehensive Multivariate Drift Report\\n\\n#%%\\nclass MultivariateriftDetector:\\n    \\\"\\\"\\\"\\n    Class ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö comprehensive multivariate drift detection\\n    \\\"\\\"\\\"\\n    \\n    def __init__(self, reference_df, current_df):\\n        self.reference = reference_df\\n        self.current = current_df\\n        self.results = {}\\n    \\n    def analyze(self):\\n        \\\"\\\"\\\"‡∏ó‡∏≥ comprehensive analysis\\\"\\\"\\\"\\n        \\n        # 1. Correlation analysis\\n        corr_results = correlation_drift_test(self.reference, self.current)\\n        self.results['correlation'] = corr_results\\n        \\n        # 2. PCA analysis\\n        pca_results = pca_drift_detection(self.reference, self.current)\\n        self.results['pca'] = pca_results\\n        \\n        # 3. Mahalanobis analysis\\n        maha_results = mahalanobis_drift_detection(self.reference, self.current)\\n        self.results['mahalanobis'] = maha_results\\n        \\n        # 4. Overall assessment\\n        drift_indicators = {\\n            'correlation_drift': any(corr_results['significant_change']),\\n            'pca_structure_change': any(sim < 0.9 for sim in pca_results['component_similarities']),\\n            'mahalanobis_drift': maha_results['drift_detected']\\n        }\\n        \\n        drift_count = sum(drift_indicators.values())\\n        \\n        self.results['summary'] = {\\n            'drift_indicators': drift_indicators,\\n            'drift_count': drift_count,\\n            'overall_assessment': 'HIGH' if drift_count >= 2 else 'MEDIUM' if drift_count == 1 else 'LOW'\\n        }\\n        \\n        return self.results\\n    \\n    def print_report(self):\\n        \\\"\\\"\\\"‡∏û‡∏¥‡∏°‡∏û‡πå report ‡∏™‡∏£‡∏∏‡∏õ\\\"\\\"\\\"\\n        if not self.results:\\n            self.analyze()\\n        \\n        print(\\\"=\\\" * 70)\\n        print(\\\"üìä COMPREHENSIVE MULTIVARIATE DRIFT REPORT\\\")\\n        print(\\\"=\\\" * 70)\\n        \\n        print(\\\"\\\\n1Ô∏è‚É£ CORRELATION DRIFT:\\\")\\n        print(\\\"-\\\" * 40)\\n        corr_df = self.results['correlation']\\n        significant_pairs = corr_df[corr_df['significant_change']]\\n        if len(significant_pairs) > 0:\\n            print(\\\"   ‚ö†Ô∏è Significant correlation changes detected:\\\")\\n            for _, row in significant_pairs.iterrows():\\n                print(f\\\"      {row['feature_pair']}: {row['ref_correlation']:.3f} ‚Üí {row['cur_correlation']:.3f}\\\")\\n        else:\\n            print(\\\"   ‚úì No significant correlation changes\\\")\\n        \\n        print(\\\"\\\\n2Ô∏è‚É£ PCA STRUCTURE ANALYSIS:\\\")\\n        print(\\\"-\\\" * 40)\\n        pca_res = self.results['pca']\\n        for i, sim in enumerate(pca_res['component_similarities']):\\n            status = \\\"‚úì\\\" if sim > 0.9 else \\\"‚ö†Ô∏è Changed\\\"\\n            print(f\\\"   PC{i+1} similarity: {sim:.4f} {status}\\\")\\n        print(f\\\"   Reconstruction error ratio: {pca_res['reconstruction_error_ratio']:.4f}x\\\")\\n        \\n        print(\\\"\\\\n3Ô∏è‚É£ MAHALANOBIS DISTANCE ANALYSIS:\\\")\\n        print(\\\"-\\\" * 40)\\n        maha_res = self.results['mahalanobis']\\n        print(f\\\"   Mean distance ratio: {maha_res['cur_mean_distance']/maha_res['ref_mean_distance']:.4f}x\\\")\\n        print(f\\\"   Outlier % (Reference): {maha_res['ref_outlier_pct']:.2f}%\\\")\\n        print(f\\\"   Outlier % (Current): {maha_res['cur_outlier_pct']:.2f}%\\\")\\n        print(f\\\"   KS Test p-value: {maha_res['ks_pvalue']:.4f}\\\")\\n        \\n        print(\\\"\\\\n\\\" + \\\"=\\\" * 70)\\n        print(\\\"üìã OVERALL ASSESSMENT\\\")\\n        print(\\\"=\\\" * 70)\\n        summary = self.results['summary']\\n        print(f\\\"   Drift Indicators Triggered: {summary['drift_count']}/3\\\")\\n        for indicator, triggered in summary['drift_indicators'].items():\\n            status = \\\"‚ö†Ô∏è\\\" if triggered else \\\"‚úì\\\"\\n            print(f\\\"      {status} {indicator}: {'Yes' if triggered else 'No'}\\\")\\n        \\n        severity_emoji = {'LOW': 'üü¢', 'MEDIUM': 'üü°', 'HIGH': 'üî¥'}\\n        print(f\\\"\\\\n   üéØ OVERALL MULTIVARIATE DRIFT: {severity_emoji[summary['overall_assessment']]} {summary['overall_assessment']}\\\")\\n\\n# ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô\\ndetector = MultivariateriftDetector(ref_multi, cur_multi)\\ndetector.analyze()\\ndetector.print_report()\\n\\n#%% [markdown]\\n# ## ‡∏™‡∏£‡∏∏‡∏õ LAB 3\\n# \\n# ### ‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ:\\n# 1. **Multivariate Drift**: ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡∏∂‡πâ‡∏ô‡πÄ‡∏°‡∏∑‡πà‡∏≠ relationship ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á features ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô\\n# 2. **Correlation Analysis**: ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡∏≠‡∏á pairwise correlations\\n# 3. **PCA Analysis**: ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡∏≠‡∏á multivariate structure\\n# 4. **Mahalanobis Distance**: ‡∏ß‡∏±‡∏î dataset-level drift\\n# \\n# ### Key Insights:\\n# - Univariate methods ‡∏≠‡∏≤‡∏à‡∏û‡∏•‡∏≤‡∏î multivariate drift\\n# - ‡πÉ‡∏ä‡πâ‡∏´‡∏•‡∏≤‡∏¢ methods ‡∏£‡πà‡∏ß‡∏°‡∏Å‡∏±‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°\\n# - Monitor ‡∏ó‡∏±‡πâ‡∏á individual features ‡πÅ‡∏•‡∏∞ relationships\\n\\n#%%\\nprint(\\\"=\\\" * 60)\\nprint(\\\"‚úÖ LAB 3 COMPLETED!\\\")\\nprint(\\\"=\\\" * 60)\\nprint(\\\"\\\"\\\"\\nüìö Key Takeaways:\\n1. Multivariate drift ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏î‡πâ‡∏ß‡∏¢ univariate methods ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ\\n2. Correlation drift analysis ‡πÉ‡∏ä‡πâ Fisher's Z transformation\\n3. PCA structure comparison ‡∏î‡∏π component similarities\\n4. Mahalanobis distance ‡∏ß‡∏±‡∏î overall distribution shift\\n\\nüîú Next: LAB 4 - Drift Detection in Production Simulation\\n\\\"\\\"\\\")\\n```\\n\\n---\\n\\n## LAB 4: Drift Detection in Production Simulation\\n\\n```python\\n#%% [markdown]\\n# # LAB 4: Drift Detection in Production Simulation\\n# ## ‡∏Å‡∏≤‡∏£‡∏à‡∏≥‡∏•‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö Drift ‡πÉ‡∏ô Production Environment\\n# \\n# ### ‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ:\\n# 1. ‡∏™‡∏£‡πâ‡∏≤‡∏á simulated data stream ‡∏ó‡∏µ‡πà‡∏°‡∏µ gradual drift\\n# 2. ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö sudden vs gradual drift\\n# 3. Implement sliding window monitoring\\n#\\n# ### ‡∏ó‡∏§‡∏©‡∏é‡∏µ:\\n# ‡πÉ‡∏ô production environment:\\n# - ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏°‡∏≤‡πÄ‡∏õ‡πá‡∏ô stream ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà batch\\n# - Drift ‡∏≠‡∏≤‡∏à‡πÄ‡∏Å‡∏¥‡∏î‡πÅ‡∏ö‡∏ö sudden ‡∏´‡∏£‡∏∑‡∏≠ gradual\\n# - ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ monitoring strategy ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°\\n\\n#%% [markdown]\\n# ## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 1: ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° Environment\\n\\n#%%\\nimport numpy as np\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\nfrom scipy import stats\\nfrom collections import deque\\nfrom datetime import datetime, timedelta\\nimport warnings\\nwarnings.filterwarnings('ignore')\\n\\nnp.random.seed(42)\\nplt.rcParams['figure.figsize'] = (14, 6)\\n\\nprint(\\\"‚úÖ Libraries imported successfully!\\\")\\n\\n#%% [markdown]\\n# ## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 2: ‡∏™‡∏£‡πâ‡∏≤‡∏á Data Stream Simulator\\n# \\n# ‡∏à‡∏≥‡∏•‡∏≠‡∏á data stream ‡∏ó‡∏µ‡πà‡∏°‡∏µ drift ‡∏´‡∏•‡∏≤‡∏¢‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö\\n\\n#%%\\nclass DataStreamSimulator:\\n    \\\"\\\"\\\"\\n    Simulator ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡∏£‡πâ‡∏≤‡∏á data stream ‡∏ó‡∏µ‡πà‡∏°‡∏µ drift patterns ‡∏ï‡πà‡∏≤‡∏á‡πÜ\\n    \\n    Drift Types:\\n    - sudden: ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏ó‡∏±‡∏ô‡∏ó‡∏µ‡∏ó‡∏µ‡πà‡∏à‡∏∏‡∏î‡πÉ‡∏î‡∏à‡∏∏‡∏î‡∏´‡∏ô‡∏∂‡πà‡∏á\\n    - gradual: ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏ä‡πâ‡∏≤‡πÜ ‡∏ï‡∏≤‡∏°‡πÄ‡∏ß‡∏•‡∏≤\\n    - incremental: ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡∏±‡πâ‡∏ô‡∏ö‡∏±‡∏ô‡πÑ‡∏î\\n    - seasonal: ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏ï‡∏≤‡∏° pattern ‡∏ã‡πâ‡∏≥\\n    - no_drift: ‡πÑ‡∏°‡πà‡∏°‡∏µ drift\\n    \\\"\\\"\\\"\\n    \\n    def __init__(self, base_mean=50, base_std=10, random_seed=42):\\n        self.base_mean = base_mean\\n        self.base_std = base_std\\n        self.random_seed = random_seed\\n        np.random.seed(random_seed)\\n    \\n    def generate_stream(self, n_samples, drift_type='no_drift', drift_params=None):\\n        \\\"\\\"\\\"\\n        ‡∏™‡∏£‡πâ‡∏≤‡∏á data stream\\n        \\n        Parameters:\\n        -----------\\n        n_samples : int - ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô samples\\n        drift_type : str - ‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Ç‡∏≠‡∏á drift\\n        drift_params : dict - parameters ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö drift\\n        \\\"\\\"\\\"\\n        if drift_params is None:\\n            drift_params = {}\\n        \\n        data = np.zeros(n_samples)\\n        timestamps = [datetime(2024, 1, 1) + timedelta(hours=i) for i in range(n_samples)]\\n        \\n        if drift_type == 'no_drift':\\n            data = np.random.normal(self.base_mean, self.base_std, n_samples)\\n        \\n        elif drift_type == 'sudden':\\n            drift_point = drift_params.get('drift_point', n_samples // 2)\\n            new_mean = drift_params.get('new_mean', self.base_mean + 2 * self.base_std)\\n            \\n            data[:drift_point] = np.random.normal(self.base_mean, self.base_std, drift_point)\\n            data[drift_point:] = np.random.normal(new_mean, self.base_std, n_samples - drift_point)\\n        \\n        elif drift_type == 'gradual':\\n            drift_start = drift_params.get('drift_start', n_samples // 4)\\n            drift_end = drift_params.get('drift_end', 3 * n_samples // 4)\\n            final_mean = drift_params.get('final_mean', self.base_mean + 2 * self.base_std)\\n            \\n            for i in range(n_samples):\\n                if i < drift_start:\\n                    current_mean = self.base_mean\\n                elif i > drift_end:\\n                    current_mean = final_mean\\n                else:\\n                    # Linear interpolation\\n                    progress = (i - drift_start) / (drift_end - drift_start)\\n                    current_mean = self.base_mean + progress * (final_mean - self.base_mean)\\n                \\n                data[i] = np.random.normal(current_mean, self.base_std)\\n        \\n        elif drift_type == 'incremental':\\n            step_size = drift_params.get('step_size', n_samples // 5)\\n            step_increase = drift_params.get('step_increase', self.base_std * 0.5)\\n            \\n            for i in range(n_samples):\\n                step = i // step_size\\n                current_mean = self.base_mean + step * step_increase\\n                data[i] = np.random.normal(current_mean, self.base_std)\\n        \\n        elif drift_type == 'seasonal':\\n            period = drift_params.get('period', 168)  # 1 week in hours\\n            amplitude = drift_params.get('amplitude', self.base_std)\\n            \\n            for i in range(n_samples):\\n                seasonal_effect = amplitude * np.sin(2 * np.pi * i / period)\\n                data[i] = np.random.normal(self.base_mean + seasonal_effect, self.base_std)\\n        \\n        return pd.DataFrame({\\n            'timestamp': timestamps,\\n            'value': data,\\n            'index': range(n_samples)\\n        })\\n\\n# ‡∏™‡∏£‡πâ‡∏≤‡∏á simulator\\nsimulator = DataStreamSimulator(base_mean=50, base_std=10)\\n\\nprint(\\\"‚úÖ DataStreamSimulator created\\\")\\n\\n#%%\\n# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á drift patterns\\nfig, axes = plt.subplots(2, 3, figsize=(15, 8))\\naxes = axes.flatten()\\n\\ndrift_types = ['no_drift', 'sudden', 'gradual', 'incremental', 'seasonal']\\ndrift_params_list = [\\n    {},\\n    {'drift_point': 500, 'new_mean': 70},\\n    {'drift_start': 200, 'drift_end': 800, 'final_mean': 70},\\n    {'step_size': 200, 'step_increase': 5},\\n    {'period': 200, 'amplitude': 15}\\n]\\n\\nstreams = {}\\nfor i, (dtype, params) in enumerate(zip(drift_types, drift_params_list)):\\n    stream = simulator.generate_stream(1000, drift_type=dtype, drift_params=params)\\n    streams[dtype] = stream\\n    \\n    ax = axes[i]\\n    ax.plot(stream['index'], stream['value'], alpha=0.7, linewidth=0.5)\\n    \\n    # Add rolling mean\\n    rolling_mean = stream['value'].rolling(window=50).mean()\\n    ax.plot(stream['index'], rolling_mean, 'r-', linewidth=2, label='Rolling Mean (50)')\\n    \\n    ax.axhline(50, color='green', linestyle='--', alpha=0.5, label='Original Mean')\\n    ax.set_title(f'{dtype.replace(\\\"_\\\", \\\" \\\").title()}')\\n    ax.set_xlabel('Time Index')\\n    ax.set_ylabel('Value')\\n    ax.legend(fontsize=8)\\n\\n# Hide empty subplot\\naxes[-1].set_visible(False)\\n\\nplt.suptitle('Different Types of Data Drift', fontsize=14, fontweight='bold')\\nplt.tight_layout()\\nplt.savefig('drift_types.png', dpi=150, bbox_inches='tight')\\nplt.show()\\n\\n#%% [markdown]\\n# ## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 3: Sliding Window Drift Detector\\n# \\n# Implement sliding window ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö real-time drift detection\\n\\n#%%\\nclass SlidingWindowDriftDetector:\\n    \\\"\\\"\\\"\\n    Drift detector ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ sliding window approach\\n    \\n    ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö:\\n    - Real-time monitoring\\n    - Streaming data\\n    - Memory-efficient processing\\n    \\\"\\\"\\\"\\n    \\n    def __init__(self, reference_window_size=200, test_window_size=100, \\n                 ks_threshold=0.05, psi_threshold=0.1):\\n        \\\"\\\"\\\"\\n        Parameters:\\n        -----------\\n        reference_window_size : int - ‡∏Ç‡∏ô‡∏≤‡∏î‡∏Ç‡∏≠‡∏á reference window\\n        test_window_size : int - ‡∏Ç‡∏ô‡∏≤‡∏î‡∏Ç‡∏≠‡∏á test window\\n        ks_threshold : float - threshold ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö KS test p-value\\n        psi_threshold : float - threshold ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö PSI\\n        \\\"\\\"\\\"\\n        self.reference_window_size = reference_window_size\\n        self.test_window_size = test_window_size\\n        self.ks_threshold = ks_threshold\\n        self.psi_threshold = psi_threshold\\n        \\n        # ‡πÉ‡∏ä‡πâ deque ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö efficient sliding window\\n        self.reference_buffer = deque(maxlen=reference_window_size)\\n        self.test_buffer = deque(maxlen=test_window_size)\\n        \\n        self.history = []\\n        self.drift_points = []\\n        self.is_initialized = False\\n    \\n    def calculate_psi(self, reference, test, bins=10):\\n        \\\"\\\"\\\"‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì PSI\\\"\\\"\\\"\\n        breakpoints = np.percentile(reference, np.linspace(0, 100, bins + 1))\\n        breakpoints = np.unique(breakpoints)\\n        \\n        ref_counts, _ = np.histogram(reference, bins=breakpoints)\\n        test_counts, _ = np.histogram(test, bins=breakpoints)\\n        \\n        eps = 1e-6\\n        ref_props = ref_counts / len(reference) + eps\\n        test_props = test_counts / len(test) + eps\\n        \\n        psi = np.sum((test_props - ref_props) * np.log(test_props / ref_props))\\n        return psi\\n    \\n    def update(self, value, timestamp=None):\\n        \\\"\\\"\\\"\\n        Update detector ‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡πà‡∏≤‡πÉ‡∏´‡∏°‡πà\\n        \\n        Returns:\\n        --------\\n        dict : detection result\\n        \\\"\\\"\\\"\\n        # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡πà‡∏≤‡πÉ‡∏ô buffers\\n        if not self.is_initialized:\\n            self.reference_buffer.append(value)\\n            if len(self.reference_buffer) >= self.reference_window_size:\\n                self.is_initialized = True\\n            return {'drift_detected': False, 'status': 'initializing'}\\n        \\n        self.test_buffer.append(value)\\n        \\n        # ‡∏£‡∏≠‡∏à‡∏ô‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏≠‡πÉ‡∏ô test buffer\\n        if len(self.test_buffer) < self.test_window_size:\\n            return {'drift_detected': False, 'status': 'collecting'}\\n        \\n        # ‡∏ó‡∏≥ drift detection\\n        ref_array = np.array(self.reference_buffer)\\n        test_array = np.array(self.test_buffer)\\n        \\n        # KS Test\\n        ks_stat, ks_pval = stats.ks_2samp(ref_array, test_array)\\n        \\n        # PSI\\n        psi = self.calculate_psi(ref_array, test_array)\\n        \\n        # Detection logic\\n        ks_drift = ks_pval < self.ks_threshold\\n        psi_drift = psi > self.psi_threshold\\n        drift_detected = ks_drift or psi_drift\\n        \\n        result = {\\n            'timestamp': timestamp,\\n            'drift_detected': drift_detected,\\n            'ks_statistic': ks_stat,\\n            'ks_pvalue': ks_pval,\\n            'psi': psi,\\n            'ref_mean': np.mean(ref_array),\\n            'test_mean': np.mean(test_array),\\n            'status': 'DRIFT' if drift_detected else 'normal'\\n        }\\n        \\n        self.history.append(result)\\n        \\n        if drift_detected:\\n            self.drift_points.append(len(self.history) - 1)\\n        \\n        return result\\n    \\n    def reset_reference(self):\\n        \\\"\\\"\\\"Reset reference window ‡∏î‡πâ‡∏ß‡∏¢ test window ‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô\\\"\\\"\\\"\\n        self.reference_buffer.clear()\\n        for val in self.test_buffer:\\n            self.reference_buffer.append(val)\\n        self.test_buffer.clear()\\n        print(\\\"üìã Reference window reset with current data\\\")\\n    \\n    def get_history_df(self):\\n        \\\"\\\"\\\"‡πÅ‡∏õ‡∏•‡∏á history ‡πÄ‡∏õ‡πá‡∏ô DataFrame\\\"\\\"\\\"\\n        return pd.DataFrame(self.history)\\n\\n#%%\\n# ‡∏ó‡∏î‡∏™‡∏≠‡∏ö SlidingWindowDriftDetector ‡∏Å‡∏±‡∏ö sudden drift\\nprint(\\\"=\\\" * 60)\\nprint(\\\"üîç Testing Sliding Window Detector with SUDDEN DRIFT\\\")\\nprint(\\\"=\\\" * 60)\\n\\ndetector = SlidingWindowDriftDetector(\\n    reference_window_size=200,\\n    test_window_size=100,\\n    ks_threshold=0.05,\\n    psi_threshold=0.1\\n)\\n\\nsudden_stream = streams['sudden']\\n\\nfor idx, row in sudden_stream.iterrows():\\n    result = detector.update(row['value'], timestamp=row['timestamp'])\\n    \\n    # Print only drift events\\n    if result.get('drift_detected', False):\\n        print(f\\\"‚ö†Ô∏è DRIFT at index {idx}: KS p-value={result['ks_pvalue']:.4f}, PSI={result['psi']:.4f}\\\")\\n\\nprint(f\\\"\\\\nüìä Total drift points detected: {len(detector.drift_points)}\\\")\\n\\n#%%\\n# Visualize detection results\\nhistory_df = detector.get_history_df()\\n\\nfig, axes = plt.subplots(3, 1, figsize=(14, 10), sharex=True)\\n\\n# Plot 1: Original data with drift points\\nax1 = axes[0]\\nax1.plot(sudden_stream['index'], sudden_stream['value'], alpha=0.5, label='Data')\\nax1.plot(sudden_stream['index'], sudden_stream['value'].rolling(50).mean(), \\n         'b-', linewidth=2, label='Rolling Mean')\\n\\n# Mark drift points\\nfor dp in detector.drift_points:\\n    ax1.axvline(dp + 200 + 100, color='red', alpha=0.5, linestyle='--')  # Offset for initialization\\n\\nax1.axvline(500, color='green', linestyle='--', linewidth=2, label='Actual Drift Point')\\nax1.set_ylabel('Value')\\nax1.set_title('Data Stream with Detected Drift Points')\\nax1.legend()\\n\\n# Plot 2: KS p-value over time\\nax2 = axes[1]\\nax2.plot(history_df.index + 200 + 100, history_df['ks_pvalue'], 'b-', linewidth=1)\\nax2.axhline(0.05, color='red', linestyle='--', label='Threshold (0.05)')\\nax2.set_ylabel('KS p-value')\\nax2.set_title('KS Test p-value Over Time')\\nax2.legend()\\nax2.set_yscale('log')\\n\\n# Plot 3: PSI over time\\nax3 = axes[2]\\nax3.plot(history_df.index + 200 + 100, history_df['psi'], 'g-', linewidth=1)\\nax3.axhline(0.1, color='red', linestyle='--', label='Threshold (0.1)')\\nax3.set_ylabel('PSI')\\nax3.set_xlabel('Time Index')\\nax3.set_title('PSI Over Time')\\nax3.legend()\\n\\nplt.tight_layout()\\nplt.savefig('sliding_window_detection.png', dpi=150, bbox_inches='tight')\\nplt.show()\\n\\n#%% [markdown]\\n# ## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 4: Detecting Gradual Drift\\n# \\n# Gradual drift ‡∏¢‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤ sudden drift ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏ä‡πâ‡∏≤‡πÜ\\n\\n#%%\\nprint(\\\"=\\\" * 60)\\nprint(\\\"üîç Testing Sliding Window Detector with GRADUAL DRIFT\\\")\\nprint(\\\"=\\\" * 60)\\n\\ndetector_gradual = SlidingWindowDriftDetector(\\n    reference_window_size=200,\\n    test_window_size=100\\n)\\n\\ngradual_stream = streams['gradual']\\n\\nfor idx, row in gradual_stream.iterrows():\\n    result = detector_gradual.update(row['value'], timestamp=row['timestamp'])\\n\\nhistory_gradual = detector_gradual.get_history_df()\\n\\nprint(f\\\"\\\\nüìä Total drift points detected: {len(detector_gradual.drift_points)}\\\")\\nprint(f\\\"   First detection at index: {detector_gradual.drift_points[0] + 300 if detector_gradual.drift_points else 'N/A'}\\\")\\n\\n#%%\\n# Compare detection of sudden vs gradual drift\\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\\n\\n# Sudden drift\\nax1 = axes[0, 0]\\nax1.plot(sudden_stream['index'], sudden_stream['value'], alpha=0.3)\\nax1.plot(sudden_stream['index'], sudden_stream['value'].rolling(50).mean(), 'b-', linewidth=2)\\nfor dp in detector.drift_points:\\n    ax1.axvline(dp + 300, color='red', alpha=0.3)\\nax1.axvline(500, color='green', linestyle='--', linewidth=2, label='Actual Drift')\\nax1.set_title('Sudden Drift Detection')\\nax1.legend()\\n\\nax2 = axes[0, 1]\\nhistory_df = detector.get_history_df()\\nax2.plot(history_df.index + 300, history_df['psi'], 'g-')\\nax2.axhline(0.1, color='red', linestyle='--')\\nax2.set_title('Sudden Drift: PSI')\\n\\n# Gradual drift\\nax3 = axes[1, 0]\\nax3.plot(gradual_stream['index'], gradual_stream['value'], alpha=0.3)\\nax3.plot(gradual_stream['index'], gradual_stream['value'].rolling(50).mean(), 'b-', linewidth=2)\\nfor dp in detector_gradual.drift_points:\\n    ax3.axvline(dp + 300, color='red', alpha=0.3)\\nax3.axvspan(200, 800, alpha=0.2, color='green', label='Drift Period')\\nax3.set_title('Gradual Drift Detection')\\nax3.legend()\\n\\nax4 = axes[1, 1]\\nax4.plot(history_gradual.index + 300, history_gradual['psi'], 'g-')\\nax4.axhline(0.1, color='red', linestyle='--')\\nax4.set_title('Gradual Drift: PSI')\\n\\nplt.tight_layout()\\nplt.savefig('sudden_vs_gradual_detection.png', dpi=150, bbox_inches='tight')\\nplt.show()\\n\\nprint(\\\"\\\\nüí° Observation:\\\")\\nprint(\\\"   - Sudden drift: ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡πÑ‡∏î‡πâ‡πÄ‡∏£‡πá‡∏ß‡πÅ‡∏•‡∏∞‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô\\\")\\nprint(\\\"   - Gradual drift: ‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤‡∏ô‡∏≤‡∏ô‡∏Å‡∏ß‡πà‡∏≤‡∏à‡∏∞ detect ‡πÑ‡∏î‡πâ\\\")\\n\\n#%% [markdown]\\n# ## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 5: Adaptive Reference Window\\n# \\n# ‡∏õ‡∏£‡∏±‡∏ö reference window ‡πÄ‡∏û‡∏∑‡πà‡∏≠ handle gradual drift\\n\\n#%%\\nclass AdaptiveDriftDetector:\\n    \\\"\\\"\\\"\\n    Drift detector ‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏±‡∏ö reference window ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥\\n    \\n    Features:\\n    - ‡∏õ‡∏£‡∏±‡∏ö reference ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö drift\\n    - ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô false alarms ‡∏à‡∏≤‡∏Å temporary spikes\\n    - Track both short-term ‡πÅ‡∏•‡∏∞ long-term drift\\n    \\\"\\\"\\\"\\n    \\n    def __init__(self, reference_window_size=200, test_window_size=50,\\n                 confirmation_window=3, psi_threshold=0.1):\\n        \\\"\\\"\\\"\\n        Parameters:\\n        -----------\\n        confirmation_window : int - ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ï‡∏¥‡∏î‡∏ï‡πà‡∏≠‡∏Å‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á detect ‡∏Å‡πà‡∏≠‡∏ô‡∏¢‡∏∑‡∏ô‡∏¢‡∏±‡∏ô drift\\n        \\\"\\\"\\\"\\n        self.reference_window_size = reference_window_size\\n        self.test_window_size = test_window_size\\n        self.confirmation_window = confirmation_window\\n        self.psi_threshold = psi_threshold\\n        \\n        self.reference_buffer = deque(maxlen=reference_window_size)\\n        self.test_buffer = deque(maxlen=test_window_size)\\n        \\n        self.consecutive_drift_count = 0\\n        self.confirmed_drifts = []\\n        self.history = []\\n        self.adaptation_count = 0\\n    \\n    def calculate_psi(self, reference, test, bins=10):\\n        \\\"\\\"\\\"‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì PSI\\\"\\\"\\\"\\n        breakpoints = np.percentile(reference, np.linspace(0, 100, bins + 1))\\n        breakpoints = np.unique(breakpoints)\\n        \\n        ref_counts, _ = np.histogram(reference, bins=breakpoints)\\n        test_counts, _ = np.histogram(test, bins=breakpoints)\\n        \\n        eps = 1e-6\\n        ref_props = ref_counts / len(reference) + eps\\n        test_props = test_counts / len(test) + eps\\n        \\n        psi = np.sum((test_props - ref_props) * np.log(test_props / ref_props))\\n        return psi\\n    \\n    def adapt_reference(self):\\n        \\\"\\\"\\\"‡∏õ‡∏£‡∏±‡∏ö reference window\\\"\\\"\\\"\\n        # ‡∏ú‡∏™‡∏° old reference ‡∏Å‡∏±‡∏ö new data\\n        old_weight = 0.5\\n        old_ref = list(self.reference_buffer)\\n        new_data = list(self.test_buffer)\\n        \\n        # ‡∏™‡∏£‡πâ‡∏≤‡∏á new reference\\n        self.reference_buffer.clear()\\n        \\n        # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ö‡∏≤‡∏á‡∏™‡πà‡∏ß‡∏ô‡∏à‡∏≤‡∏Å old reference\\n        n_old = int(len(old_ref) * old_weight)\\n        for val in old_ref[-n_old:]:\\n            self.reference_buffer.append(val)\\n        \\n        # ‡πÄ‡∏û‡∏¥‡πà‡∏° new data\\n        for val in new_data:\\n            self.reference_buffer.append(val)\\n        \\n        self.adaptation_count += 1\\n        self.consecutive_drift_count = 0\\n    \\n    def update(self, value, timestamp=None):\\n        \\\"\\\"\\\"Update detector\\\"\\\"\\\"\\n        # Initialize\\n        if len(self.reference_buffer) < self.reference_window_size:\\n            self.reference_buffer.append(value)\\n            return {'status': 'initializing', 'drift_detected': False}\\n        \\n        self.test_buffer.append(value)\\n        \\n        if len(self.test_buffer) < self.test_window_size:\\n            return {'status': 'collecting', 'drift_detected': False}\\n        \\n        # Drift detection\\n        ref_array = np.array(self.reference_buffer)\\n        test_array = np.array(self.test_buffer)\\n        \\n        psi = self.calculate_psi(ref_array, test_array)\\n        potential_drift = psi > self.psi_threshold\\n        \\n        if potential_drift:\\n            self.consecutive_drift_count += 1\\n        else:\\n            self.consecutive_drift_count = 0\\n        \\n        # Confirmed drift (multiple consecutive detections)\\n        confirmed = self.consecutive_drift_count >= self.confirmation_window\\n        \\n        result = {\\n            'timestamp': timestamp,\\n            'psi': psi,\\n            'potential_drift': potential_drift,\\n            'confirmed_drift': confirmed,\\n            'consecutive_count': self.consecutive_drift_count,\\n            'adaptation_count': self.adaptation_count,\\n            'ref_mean': np.mean(ref_array),\\n            'test_mean': np.mean(test_array)\\n        }\\n        \\n        self.history.append(result)\\n        \\n        # Adapt if confirmed\\n        if confirmed:\\n            self.confirmed_drifts.append(len(self.history) - 1)\\n            self.adapt_reference()\\n            result['adapted'] = True\\n        \\n        return result\\n    \\n    def get_history_df(self):\\n        return pd.DataFrame(self.history)\\n\\n#%%\\n# ‡∏ó‡∏î‡∏™‡∏≠‡∏ö Adaptive Detector\\nprint(\\\"=\\\" * 60)\\nprint(\\\"üîç Testing ADAPTIVE Drift Detector with GRADUAL DRIFT\\\")\\nprint(\\\"=\\\" * 60)\\n\\nadaptive_detector = AdaptiveDriftDetector(\\n    reference_window_size=200,\\n    test_window_size=50,\\n    confirmation_window=3,\\n    psi_threshold=0.1\\n)\\n\\nfor idx, row in gradual_stream.iterrows():\\n    result = adaptive_detector.update(row['value'], timestamp=row['timestamp'])\\n    \\n    if result.get('confirmed_drift', False):\\n        print(f\\\"‚úÖ CONFIRMED DRIFT at index {idx}: PSI={result['psi']:.4f}, Adapted!\\\")\\n\\nprint(f\\\"\\\\nüìä Total confirmed drifts: {len(adaptive_detector.confirmed_drifts)}\\\")\\nprint(f\\\"üìä Total adaptations: {adaptive_detector.adaptation_count}\\\")\\n\\n#%%\\n# Visualize adaptive detection\\nadaptive_history = adaptive_detector.get_history_df()\\n\\nfig, axes = plt.subplots(3, 1, figsize=(14, 10), sharex=True)\\n\\n# Plot 1: Data with adaptations\\nax1 = axes[0]\\nax1.plot(gradual_stream['index'], gradual_stream['value'], alpha=0.3)\\nax1.plot(gradual_stream['index'], gradual_stream['value'].rolling(50).mean(), 'b-', linewidth=2)\\n\\noffset = 200 + 50  # initialization + test window\\nfor drift_idx in adaptive_detector.confirmed_drifts:\\n    ax1.axvline(drift_idx + offset, color='red', linestyle='--', alpha=0.7, linewidth=2)\\n\\nax1.set_title('Gradual Drift with Adaptive Detection')\\nax1.set_ylabel('Value')\\n\\n# Plot 2: PSI with threshold\\nax2 = axes[1]\\nax2.plot(adaptive_history.index + offset, adaptive_history['psi'], 'g-')\\nax2.axhline(0.1, color='red', linestyle='--', label='Threshold')\\nax2.set_ylabel('PSI')\\nax2.legend()\\n\\n# Plot 3: Reference vs Test mean\\nax3 = axes[2]\\nax3.plot(adaptive_history.index + offset, adaptive_history['ref_mean'], 'b-', label='Reference Mean')\\nax3.plot(adaptive_history.index + offset, adaptive_history['test_mean'], 'r-', label='Test Mean')\\nax3.set_xlabel('Time Index')\\nax3.set_ylabel('Mean')\\nax3.legend()\\n\\nplt.tight_layout()\\nplt.savefig('adaptive_detection.png', dpi=150, bbox_inches='tight')\\nplt.show()\\n\\nprint(\\\"\\\\nüí° Adaptive detector ‡∏õ‡∏£‡∏±‡∏ö reference window ‡πÄ‡∏°‡∏∑‡πà‡∏≠ detect drift\\\")\\nprint(\\\"   ‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏° gradual drift ‡πÑ‡∏î‡πâ‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á\\\")\\n\\n#%% [markdown]\\n# ## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 6: Page-Hinkley Test for Change Detection\\n# \\n# Algorithm ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö detect mean shift ‡πÉ‡∏ô streaming data\\n\\n#%%\\nclass PageHinkleyDetector:\\n    \\\"\\\"\\\"\\n    Page-Hinkley test ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö detect mean shift\\n    \\n    ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö:\\n    - Streaming data\\n    - Detect upward ‡∏´‡∏£‡∏∑‡∏≠ downward shifts\\n    - Low memory footprint\\n    \\\"\\\"\\\"\\n    \\n    def __init__(self, delta=0.005, lambda_=50, alpha=0.9999):\\n        \\\"\\\"\\\"\\n        Parameters:\\n        -----------\\n        delta : float - magnitude ‡∏ó‡∏µ‡πà‡∏¢‡∏≠‡∏°‡∏£‡∏±‡∏ö‡πÑ‡∏î‡πâ‡∏Ç‡∏≠‡∏á change\\n        lambda_ : float - detection threshold\\n        alpha : float - forgetting factor ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö mean estimation\\n        \\\"\\\"\\\"\\n        self.delta = delta\\n        self.lambda_ = lambda_\\n        self.alpha = alpha\\n        \\n        self.mean = 0\\n        self.sum = 0\\n        self.min_sum = float('inf')\\n        self.max_sum = float('-inf')\\n        \\n        self.n_samples = 0\\n        self.history = []\\n        self.drift_points = []\\n    \\n    def update(self, value, timestamp=None):\\n        \\\"\\\"\\\"\\n        Update detector ‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡πà‡∏≤‡πÉ‡∏´‡∏°‡πà\\n        \\\"\\\"\\\"\\n        self.n_samples += 1\\n        \\n        # Update mean\\n        if self.n_samples == 1:\\n            self.mean = value\\n        else:\\n            self.mean = self.alpha * self.mean + (1 - self.alpha) * value\\n        \\n        # Update cumulative sum\\n        self.sum += value - self.mean - self.delta\\n        \\n        # Update min/max\\n        self.min_sum = min(self.min_sum, self.sum)\\n        self.max_sum = max(self.max_sum, self.sum)\\n        \\n        # Calculate test statistics\\n        ph_positive = self.sum - self.min_sum  # Detect upward shift\\n        ph_negative = self.max_sum - self.sum  # Detect downward shift\\n        \\n        # Detection\\n        drift_up = ph_positive > self.lambda_\\n        drift_down = ph_negative > self.lambda_\\n        drift_detected = drift_up or drift_down\\n        \\n        result = {\\n            'timestamp': timestamp,\\n            'value': value,\\n            'mean': self.mean,\\n            'sum': self.sum,\\n            'ph_positive': ph_positive,\\n            'ph_negative': ph_negative,\\n            'drift_detected': drift_detected,\\n            'drift_direction': 'up' if drift_up else ('down' if drift_down else None)\\n        }\\n        \\n        self.history.append(result)\\n        \\n        if drift_detected:\\n            self.drift_points.append(self.n_samples - 1)\\n            self.reset()\\n        \\n        return result\\n    \\n    def reset(self):\\n        \\\"\\\"\\\"Reset statistics ‡∏´‡∏•‡∏±‡∏á detect drift\\\"\\\"\\\"\\n        self.sum = 0\\n        self.min_sum = float('inf')\\n        self.max_sum = float('-inf')\\n    \\n    def get_history_df(self):\\n        return pd.DataFrame(self.history)\\n\\n#%%\\n# ‡∏ó‡∏î‡∏™‡∏≠‡∏ö Page-Hinkley\\nprint(\\\"=\\\" * 60)\\nprint(\\\"üîç Testing PAGE-HINKLEY Detector\\\")\\nprint(\\\"=\\\" * 60)\\n\\nph_detector = PageHinkleyDetector(delta=0.01, lambda_=30)\\n\\n# ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏±‡∏ö sudden drift\\nfor idx, row in sudden_stream.iterrows():\\n    result = ph_detector.update(row['value'], timestamp=row['timestamp'])\\n    \\n    if result['drift_detected']:\\n        print(f\\\"‚ö†Ô∏è DRIFT at index {idx}: Direction = {result['drift_direction']}\\\")\\n\\nprint(f\\\"\\\\nüìä Total drifts detected: {len(ph_detector.drift_points)}\\\")\\n\\n#%%\\n# Visualize Page-Hinkley results\\nph_history = ph_detector.get_history_df()\\n\\nfig, axes = plt.subplots(3, 1, figsize=(14, 10), sharex=True)\\n\\n# Plot 1: Data\\nax1 = axes[0]\\nax1.plot(sudden_stream['index'], sudden_stream['value'], alpha=0.3)\\nax1.plot(ph_history.index, ph_history['mean'], 'r-', linewidth=2, label='Estimated Mean')\\nfor dp in ph_detector.drift_points:\\n    ax1.axvline(dp, color='red', linestyle='--', alpha=0.7)\\nax1.set_title('Page-Hinkley Detection')\\nax1.set_ylabel('Value')\\nax1.legend()\\n\\n# Plot 2: PH Statistics\\nax2 = axes[1]\\nax2.plot(ph_history.index, ph_history['ph_positive'], 'g-', label='PH+ (upward)')\\nax2.plot(ph_history.index, ph_history['ph_negative'], 'b-', label='PH- (downward)')\\nax2.axhline(30, color='red', linestyle='--', label='Threshold')\\nax2.set_ylabel('PH Statistics')\\nax2.legend()\\n\\n# Plot 3: Cumulative Sum\\nax3 = axes[2]\\nax3.plot(ph_history.index, ph_history['sum'], 'purple')\\nax3.set_xlabel('Time Index')\\nax3.set_ylabel('Cumulative Sum')\\n\\nplt.tight_layout()\\nplt.savefig('page_hinkley_detection.png', dpi=150, bbox_inches='tight')\\nplt.show()\\n\\n#%% [markdown]\\n# ## ‡∏™‡∏£‡∏∏‡∏õ LAB 4\\n# \\n# ### ‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ:\\n# 1. **Data Stream Simulation**: ‡∏™‡∏£‡πâ‡∏≤‡∏á streams ‡∏ó‡∏µ‡πà‡∏°‡∏µ drift patterns ‡∏ï‡πà‡∏≤‡∏á‡πÜ\\n# 2. **Sliding Window**: ‡∏ß‡∏¥‡∏ò‡∏µ‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö streaming drift detection\\n# 3. **Adaptive Detection**: ‡∏õ‡∏£‡∏±‡∏ö reference window ‡πÄ‡∏°‡∏∑‡πà‡∏≠ detect drift\\n# 4. **Page-Hinkley**: Algorithm ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö mean shift detection\\n# \\n# ### Comparison:\\n# | Method | Pros | Cons | Best For |\\n# |--------|------|------|----------|\\n# | Sliding Window | Simple, intuitive | Fixed reference | Sudden drift |\\n# | Adaptive | Handles gradual drift | More complex | Production |\\n# | Page-Hinkley | Low memory, fast | Mean shift only | Real-time |\\n\\n#%%\\nprint(\\\"=\\\" * 60)\\nprint(\\\"‚úÖ LAB 4 COMPLETED!\\\")\\nprint(\\\"=\\\" * 60)\\nprint(\\\"\\\"\\\"\\nüìö Key Takeaways:\\n1. Different drift types require different detection strategies\\n2. Sliding window is fundamental for streaming detection\\n3. Adaptive reference helps with gradual drift\\n4. Page-Hinkley is efficient for mean shift detection\\n\\nüîú Next: LAB 5 - Custom Metrics & Drift Thresholds\\n\\\"\\\"\\\")\\n```\\n\\n---\\n\\n## LAB 5: Custom Metrics & Drift Thresholds\\n\\n```python\\n#%% [markdown]\\n# # LAB 5: Custom Metrics & Drift Thresholds\\n# ## ‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á Custom Drift Metrics ‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏±‡∏ö Thresholds\\n# \\n# ### ‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ:\\n# 1. ‡∏™‡∏£‡πâ‡∏≤‡∏á custom drift metrics ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö domain\\n# 2. ‡∏õ‡∏£‡∏±‡∏ö threshold ‡∏ï‡∏≤‡∏° business requirements\\n# 3. Handle false positives/negatives ‡πÉ‡∏ô drift detection\\n#\\n# ### ‡∏ó‡∏§‡∏©‡∏é‡∏µ:\\n# Default thresholds ‡∏≠‡∏≤‡∏à‡πÑ‡∏°‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö‡∏ó‡∏∏‡∏Å use case:\\n# - ‡∏ö‡∏≤‡∏á domain ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ sensitivity ‡∏™‡∏π‡∏á\\n# - ‡∏ö‡∏≤‡∏á domain ‡∏¢‡∏≠‡∏°‡∏£‡∏±‡∏ö drift ‡πÑ‡∏î‡πâ‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏´‡∏ô‡∏∂‡πà‡∏á\\n# - Cost of false positive vs false negative ‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô\\n\\n#%% [markdown]\\n# ## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 1: ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° Environment\\n\\n#%%\\nimport numpy as np\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\nfrom scipy import stats\\nfrom sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\\nimport warnings\\nwarnings.filterwarnings('ignore')\\n\\nnp.random.seed(42)\\nplt.rcParams['figure.figsize'] = (12, 6)\\n\\nprint(\\\"‚úÖ Libraries imported successfully!\\\")\\n\\n#%% [markdown]\\n# ## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 2: ‡∏™‡∏£‡πâ‡∏≤‡∏á Dataset ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Threshold Tuning\\n# \\n# ‡∏™‡∏£‡πâ‡∏≤‡∏á labeled dataset ‡∏ó‡∏µ‡πà‡∏£‡∏π‡πâ‡∏ß‡πà‡∏≤‡∏°‡∏µ drift ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà\\n\\n#%%\\ndef create_labeled_drift_dataset(n_scenarios=100):\\n    \\\"\\\"\\\"\\n    ‡∏™‡∏£‡πâ‡∏≤‡∏á dataset ‡∏û‡∏£‡πâ‡∏≠‡∏° ground truth labels\\n    \\n    Returns:\\n    --------\\n    list of dicts: ‡πÅ‡∏ï‡πà‡∏•‡∏∞ scenario ‡∏°‡∏µ reference, current, ‡πÅ‡∏•‡∏∞ label\\n    \\\"\\\"\\\"\\n    scenarios = []\\n    \\n    for i in range(n_scenarios):\\n        np.random.seed(i)\\n        n_samples = 500\\n        \\n        # Reference data\\n        ref_mean = 50\\n        ref_std = 10\\n        reference = np.random.normal(ref_mean, ref_std, n_samples)\\n        \\n        # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏ß‡πà‡∏≤‡∏°‡∏µ drift ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà\\n        has_drift = i < n_scenarios // 2  # 50% ‡∏°‡∏µ drift\\n        \\n        if has_drift:\\n            # ‡∏™‡∏£‡πâ‡∏≤‡∏á drift ‡πÉ‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ï‡πà‡∏≤‡∏á‡πÜ\\n            drift_level = (i % 5 + 1) * 0.5  # 0.5, 1.0, 1.5, 2.0, 2.5 std\\n            current_mean = ref_mean + drift_level * ref_std\\n            drift_magnitude = drift_level\\n        else:\\n            # ‡πÑ‡∏°‡πà‡∏°‡∏µ drift ‡πÅ‡∏ï‡πà‡∏°‡∏µ noise ‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢\\n            current_mean = ref_mean + np.random.uniform(-0.1, 0.1) * ref_std\\n            drift_magnitude = 0\\n        \\n        current = np.random.normal(current_mean, ref_std, n_samples)\\n        \\n        scenarios.append({\\n            'id': i,\\n            'reference': reference,\\n            'current': current,\\n            'has_drift': has_drift,\\n            'drift_magnitude': drift_magnitude,\\n            'ref_mean': ref_mean,\\n            'current_mean': current_mean\\n        })\\n    \\n    return scenarios\\n\\n# ‡∏™‡∏£‡πâ‡∏≤‡∏á dataset\\nscenarios = create_labeled_drift_dataset(100)\\n\\nprint(f\\\"üìä Created {len(scenarios)} scenarios\\\")\\nprint(f\\\"   With drift: {sum(1 for s in scenarios if s['has_drift'])}\\\")\\nprint(f\\\"   Without drift: {sum(1 for s in scenarios if not s['has_drift'])}\\\")\\n\\n#%%\\n# Visualize drift magnitude distribution\\ndrift_mags = [s['drift_magnitude'] for s in scenarios if s['has_drift']]\\nplt.figure(figsize=(10, 4))\\nplt.hist(drift_mags, bins=10, edgecolor='black', alpha=0.7)\\nplt.xlabel('Drift Magnitude (in std)')\\nplt.ylabel('Count')\\nplt.title('Distribution of Drift Magnitudes in Scenarios with Drift')\\nplt.show()\\n\\n#%% [markdown]\\n# ## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 3: ‡∏™‡∏£‡πâ‡∏≤‡∏á Custom Drift Metrics\\n\\n#%%\\nclass CustomDriftMetrics:\\n    \\\"\\\"\\\"\\n    Class ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì custom drift metrics\\n    \\\"\\\"\\\"\\n    \\n    @staticmethod\\n    def psi(reference, current, bins=10):\\n        \\\"\\\"\\\"Population Stability Index\\\"\\\"\\\"\\n        breakpoints = np.percentile(reference, np.linspace(0, 100, bins + 1))\\n        breakpoints = np.unique(breakpoints)\\n        \\n        ref_counts, _ = np.histogram(reference, bins=breakpoints)\\n        cur_counts, _ = np.histogram(current, bins=breakpoints)\\n        \\n        eps = 1e-6\\n        ref_props = ref_counts / len(reference) + eps\\n        cur_props = cur_counts / len(current) + eps\\n        \\n        return np.sum((cur_props - ref_props) * np.log(cur_props / ref_props))\\n    \\n    @staticmethod\\n    def normalized_wasserstein(reference, current):\\n        \\\"\\\"\\\"Wasserstein distance normalized by reference std\\\"\\\"\\\"\\n        distance = stats.wasserstein_distance(reference, current)\\n        return distance / np.std(reference)\\n    \\n    @staticmethod\\n    def mean_shift_ratio(reference, current):\\n        \\\"\\\"\\\"Mean shift as ratio of reference std\\\"\\\"\\\"\\n        mean_diff = abs(np.mean(current) - np.mean(reference))\\n        return mean_diff / np.std(reference)\\n    \\n    @staticmethod\\n    def std_ratio(reference, current):\\n        \\\"\\\"\\\"Ratio of standard deviations\\\"\\\"\\\"\\n        return np.std(current) / np.std(reference)\\n    \\n    @staticmethod\\n    def percentile_shift(reference, current, percentiles=[25, 50, 75]):\\n        \\\"\\\"\\\"Average shift in percentiles\\\"\\\"\\\"\\n        shifts = []\\n        ref_std = np.std(reference)\\n        \\n        for p in percentiles:\\n            ref_p = np.percentile(reference, p)\\n            cur_p = np.percentile(current, p)\\n            shifts.append(abs(cur_p - ref_p) / ref_std)\\n        \\n        return np.mean(shifts)\\n    \\n    @staticmethod\\n    def jensen_shannon_divergence(reference, current, bins=10):\\n        \\\"\\\"\\\"Jensen-Shannon divergence\\\"\\\"\\\"\\n        # ‡∏™‡∏£‡πâ‡∏≤‡∏á histograms\\n        all_data = np.concatenate([reference, current])\\n        bins = np.histogram_bin_edges(all_data, bins=bins)\\n        \\n        ref_hist, _ = np.histogram(reference, bins=bins, density=True)\\n        cur_hist, _ = np.histogram(current, bins=bins, density=True)\\n        \\n        # Normalize\\n        ref_hist = ref_hist / (ref_hist.sum() + 1e-10)\\n        cur_hist = cur_hist / (cur_hist.sum() + 1e-10)\\n        \\n        # Average distribution\\n        m = 0.5 * (ref_hist + cur_hist)\\n        \\n        # KL divergences\\n        kl_pm = np.sum(ref_hist * np.log((ref_hist + 1e-10) / (m + 1e-10)))\\n        kl_qm = np.sum(cur_hist * np.log((cur_hist + 1e-10) / (m + 1e-10)))\\n        \\n        return 0.5 * (kl_pm + kl_qm)\\n    \\n    @staticmethod\\n    def combined_score(reference, current, weights=None):\\n        \\\"\\\"\\\"\\n        Combined drift score ‡∏à‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢ metrics\\n        \\n        Default weights: PSI=0.3, Wasserstein=0.3, Mean Shift=0.2, Percentile=0.2\\n        \\\"\\\"\\\"\\n        if weights is None:\\n            weights = {\\n                'psi': 0.3,\\n                'wasserstein': 0.3,\\n                'mean_shift': 0.2,\\n                'percentile': 0.2\\n            }\\n        \\n        metrics = CustomDriftMetrics\\n        \\n        # Normalize each metric to 0-1 range\\n        psi = min(metrics.psi(reference, current), 1.0)  # Cap at 1\\n        wasserstein = min(metrics.normalized_wasserstein(reference, current) / 3, 1.0)  # 3 std = max\\n        mean_shift = min(metrics.mean_shift_ratio(reference, current) / 3, 1.0)\\n        percentile = min(metrics.percentile_shift(reference, current) / 3, 1.0)\\n        \\n        score = (\\n            weights['psi'] * psi +\\n            weights['wasserstein'] * wasserstein +\\n            weights['mean_shift'] * mean_shift +\\n            weights['percentile'] * percentile\\n        )\\n        \\n        return score\\n\\n#%%\\n# ‡∏ó‡∏î‡∏™‡∏≠‡∏ö custom metrics\\nprint(\\\"=\\\" * 60)\\nprint(\\\"üìä Testing Custom Drift Metrics\\\")\\nprint(\\\"=\\\" * 60)\\n\\n# ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å scenarios ‡∏ó‡∏µ‡πà‡∏°‡∏µ drift levels ‡∏ï‡πà‡∏≤‡∏á‡πÜ\\ntest_scenarios = [s for s in scenarios if s['drift_magnitude'] in [0, 0.5, 1.0, 2.0]][:8]\\n\\nmetrics_results = []\\nfor s in test_scenarios:\\n    result = {\\n        'id': s['id'],\\n        'has_drift': s['has_drift'],\\n        'magnitude': s['drift_magnitude'],\\n        'psi': CustomDriftMetrics.psi(s['reference'], s['current']),\\n        'wasserstein': CustomDriftMetrics.normalized_wasserstein(s['reference'], s['current']),\\n        'mean_shift': CustomDriftMetrics.mean_shift_ratio(s['reference'], s['current']),\\n        'percentile': CustomDriftMetrics.percentile_shift(s['reference'], s['current']),\\n        'js_div': CustomDriftMetrics.jensen_shannon_divergence(s['reference'], s['current']),\\n        'combined': CustomDriftMetrics.combined_score(s['reference'], s['current'])\\n    }\\n    metrics_results.append(result)\\n\\nmetrics_df = pd.DataFrame(metrics_results)\\nprint(metrics_df.to_string(index=False))\\n\\n#%% [markdown]\\n# ## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 4: Threshold Optimization\\n# \\n# ‡∏´‡∏≤ optimal threshold ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ labeled data\\n\\n#%%\\ndef calculate_metrics_for_threshold(scenarios, metric_func, threshold):\\n    \\\"\\\"\\\"\\n    ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì precision, recall, f1 ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö threshold ‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î\\n    \\\"\\\"\\\"\\n    y_true = []\\n    y_pred = []\\n    \\n    for s in scenarios:\\n        y_true.append(1 if s['has_drift'] else 0)\\n        \\n        score = metric_func(s['reference'], s['current'])\\n        y_pred.append(1 if score > threshold else 0)\\n    \\n    # Handle edge cases\\n    if sum(y_pred) == 0:\\n        return {'precision': 0, 'recall': 0, 'f1': 0}\\n    \\n    precision = precision_score(y_true, y_pred, zero_division=0)\\n    recall = recall_score(y_true, y_pred, zero_division=0)\\n    f1 = f1_score(y_true, y_pred, zero_division=0)\\n    \\n    return {'precision': precision, 'recall': recall, 'f1': f1}\\n\\ndef find_optimal_threshold(scenarios, metric_func, thresholds, optimize_for='f1'):\\n    \\\"\\\"\\\"\\n    ‡∏´‡∏≤ optimal threshold\\n    \\\"\\\"\\\"\\n    results = []\\n    \\n    for t in thresholds:\\n        metrics = calculate_metrics_for_threshold(scenarios, metric_func, t)\\n        metrics['threshold'] = t\\n        results.append(metrics)\\n    \\n    results_df = pd.DataFrame(results)\\n    \\n    # ‡∏´‡∏≤ optimal\\n    optimal_idx = results_df[optimize_for].idxmax()\\n    optimal = results_df.iloc[optimal_idx]\\n    \\n    return results_df, optimal\\n\\n#%%\\n# ‡∏´‡∏≤ optimal threshold ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö PSI\\nthresholds = np.linspace(0.01, 0.5, 50)\\n\\npsi_results, psi_optimal = find_optimal_threshold(\\n    scenarios, \\n    CustomDriftMetrics.psi, \\n    thresholds\\n)\\n\\nprint(\\\"=\\\" * 60)\\nprint(\\\"üìä PSI Threshold Optimization\\\")\\nprint(\\\"=\\\" * 60)\\nprint(f\\\"Optimal threshold: {psi_optimal['threshold']:.3f}\\\")\\nprint(f\\\"Precision: {psi_optimal['precision']:.3f}\\\")\\nprint(f\\\"Recall: {psi_optimal['recall']:.3f}\\\")\\nprint(f\\\"F1 Score: {psi_optimal['f1']:.3f}\\\")\\n\\n#%%\\n# Visualize threshold optimization\\nfig, axes = plt.subplots(1, 3, figsize=(15, 4))\\n\\nmetrics_to_plot = ['precision', 'recall', 'f1']\\ncolors = ['blue', 'green', 'red']\\n\\nfor ax, metric, color in zip(axes, metrics_to_plot, colors):\\n    ax.plot(psi_results['threshold'], psi_results[metric], f'{color}-', linewidth=2)\\n    ax.axvline(psi_optimal['threshold'], color='black', linestyle='--', \\n               label=f'Optimal ({psi_optimal[\\\"threshold\\\"]:.3f})')\\n    ax.set_xlabel('PSI Threshold')\\n    ax.set_ylabel(metric.capitalize())\\n    ax.set_title(f'{metric.capitalize()} vs Threshold')\\n    ax.legend()\\n    ax.grid(True, alpha=0.3)\\n\\nplt.suptitle('PSI Threshold Optimization', fontsize=14, fontweight='bold')\\nplt.tight_layout()\\nplt.savefig('threshold_optimization_psi.png', dpi=150, bbox_inches='tight')\\nplt.show()\\n\\n#%%\\n# ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö optimal thresholds ‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞ metric\\nprint(\\\"=\\\" * 60)\\nprint(\\\"üìä Comparing Optimal Thresholds for Different Metrics\\\")\\nprint(\\\"=\\\" * 60)\\n\\nmetrics_funcs = {\\n    'PSI': CustomDriftMetrics.psi,\\n    'Normalized Wasserstein': CustomDriftMetrics.normalized_wasserstein,\\n    'Mean Shift Ratio': CustomDriftMetrics.mean_shift_ratio,\\n    'Combined Score': CustomDriftMetrics.combined_score\\n}\\n\\nthreshold_ranges = {\\n    'PSI': np.linspace(0.01, 0.5, 50),\\n    'Normalized Wasserstein': np.linspace(0.01, 2.0, 50),\\n    'Mean Shift Ratio': np.linspace(0.01, 2.0, 50),\\n    'Combined Score': np.linspace(0.01, 0.5, 50)\\n}\\n\\noptimal_thresholds = {}\\nfor name, func in metrics_funcs.items():\\n    results, optimal = find_optimal_threshold(scenarios, func, threshold_ranges[name])\\n    optimal_thresholds[name] = optimal\\n    print(f\\\"\\\\n{name}:\\\")\\n    print(f\\\"  Optimal threshold: {optimal['threshold']:.3f}\\\")\\n    print(f\\\"  F1 Score: {optimal['f1']:.3f}\\\")\\n\\n#%% [markdown]\\n# ## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 5: Business-driven Threshold Setting\\n# \\n# ‡∏õ‡∏£‡∏±‡∏ö threshold ‡∏ï‡∏≤‡∏° business requirements\\n\\n#%%\\nclass BusinessDriftThreshold:\\n    \\\"\\\"\\\"\\n    Class ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≥‡∏´‡∏ô‡∏î threshold ‡∏ï‡∏≤‡∏° business context\\n    \\\"\\\"\\\"\\n    \\n    def __init__(self, false_positive_cost=1, false_negative_cost=10):\\n        \\\"\\\"\\\"\\n        Parameters:\\n        -----------\\n        false_positive_cost : float - cost ‡∏Ç‡∏≠‡∏á false alarm\\n        false_negative_cost : float - cost ‡∏Ç‡∏≠‡∏á missing drift\\n        \\\"\\\"\\\"\\n        self.fp_cost = false_positive_cost\\n        self.fn_cost = false_negative_cost\\n    \\n    def calculate_total_cost(self, y_true, y_pred):\\n        \\\"\\\"\\\"‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì total cost\\\"\\\"\\\"\\n        tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\\n        return fp * self.fp_cost + fn * self.fn_cost\\n    \\n    def find_cost_optimal_threshold(self, scenarios, metric_func, thresholds):\\n        \\\"\\\"\\\"‡∏´‡∏≤ threshold ‡∏ó‡∏µ‡πà minimize total cost\\\"\\\"\\\"\\n        costs = []\\n        \\n        for t in thresholds:\\n            y_true = [1 if s['has_drift'] else 0 for s in scenarios]\\n            y_pred = [1 if metric_func(s['reference'], s['current']) > t else 0 for s in scenarios]\\n            \\n            cost = self.calculate_total_cost(y_true, y_pred)\\n            costs.append({'threshold': t, 'cost': cost})\\n        \\n        cost_df = pd.DataFrame(costs)\\n        optimal_idx = cost_df['cost'].idxmin()\\n        return cost_df, cost_df.iloc[optimal_idx]\\n\\n#%%\\n# ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö threshold ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö scenarios ‡∏ï‡πà‡∏≤‡∏á‡πÜ\\nprint(\\\"=\\\" * 60)\\nprint(\\\"üìä Business-driven Threshold Optimization\\\")\\nprint(\\\"=\\\" * 60)\\n\\n# Scenario 1: High cost of missing drift (e.g., fraud detection)\\nprint(\\\"\\\\nüî¥ Scenario 1: High cost of missing drift (FN cost = 10x)\\\")\\nhigh_fn_cost = BusinessDriftThreshold(false_positive_cost=1, false_negative_cost=10)\\ncost_df_1, optimal_1 = high_fn_cost.find_cost_optimal_threshold(\\n    scenarios, CustomDriftMetrics.psi, thresholds\\n)\\nprint(f\\\"   Optimal threshold: {optimal_1['threshold']:.3f}\\\")\\nprint(f\\\"   Total cost: {optimal_1['cost']:.0f}\\\")\\n\\n# Scenario 2: High cost of false alarms (e.g., model retraining is expensive)\\nprint(\\\"\\\\nüü° Scenario 2: High cost of false alarms (FP cost = 10x)\\\")\\nhigh_fp_cost = BusinessDriftThreshold(false_positive_cost=10, false_negative_cost=1)\\ncost_df_2, optimal_2 = high_fp_cost.find_cost_optimal_threshold(\\n    scenarios, CustomDriftMetrics.psi, thresholds\\n)\\nprint(f\\\"   Optimal threshold: {optimal_2['threshold']:.3f}\\\")\\nprint(f\\\"   Total cost: {optimal_2['cost']:.0f}\\\")\\n\\n# Scenario 3: Balanced costs\\nprint(\\\"\\\\nüü¢ Scenario 3: Balanced costs (FP = FN)\\\")\\nbalanced_cost = BusinessDriftThreshold(false_positive_cost=1, false_negative_cost=1)\\ncost_df_3, optimal_3 = balanced_cost.find_cost_optimal_threshold(\\n    scenarios, CustomDriftMetrics.psi, thresholds\\n)\\nprint(f\\\"   Optimal threshold: {optimal_3['threshold']:.3f}\\\")\\nprint(f\\\"   Total cost: {optimal_3['cost']:.0f}\\\")\\n\\n#%%\\n# Visualize cost-based optimization\\nfig, axes = plt.subplots(1, 3, figsize=(15, 4))\\n\\nscenarios_data = [\\n    (cost_df_1, optimal_1, 'High FN Cost (10x)', 'red'),\\n    (cost_df_2, optimal_2, 'High FP Cost (10x)', 'orange'),\\n    (cost_df_3, optimal_3, 'Balanced Cost', 'green')\\n]\\n\\nfor ax, (cost_df, optimal, title, color) in zip(axes, scenarios_data):\\n    ax.plot(cost_df['threshold'], cost_df['cost'], f'{color[0]}-', linewidth=2)\\n    ax.axvline(optimal['threshold'], color='black', linestyle='--',\\n               label=f'Optimal ({optimal[\\\"threshold\\\"]:.3f})')\\n    ax.scatter([optimal['threshold']], [optimal['cost']], color='black', s=100, zorder=5)\\n    ax.set_xlabel('PSI Threshold')\\n    ax.set_ylabel('Total Cost')\\n    ax.set_title(title)\\n    ax.legend()\\n    ax.grid(True, alpha=0.3)\\n\\nplt.tight_layout()\\nplt.savefig('cost_based_optimization.png', dpi=150, bbox_inches='tight')\\nplt.show()\\n\\nprint(\\\"\\\\nüí° Insight:\\\")\\nprint(\\\"   - High FN cost ‚Üí Lower threshold (detect more, accept false alarms)\\\")\\nprint(\\\"   - High FP cost ‚Üí Higher threshold (be more conservative)\\\")\\nprint(\\\"   - Balanced ‚Üí Somewhere in between\\\")\\n\\n#%% [markdown]\\n# ## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 6: Handling False Positives/Negatives\\n\\n#%%\\nclass RobustDriftDetector:\\n    \\\"\\\"\\\"\\n    Drift detector ‡∏ó‡∏µ‡πà‡∏°‡∏µ mechanisms ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö handle FP/FN\\n    \\\"\\\"\\\"\\n    \\n    def __init__(self, primary_metric='psi', primary_threshold=0.1,\\n                 confirmation_window=3, use_ensemble=True):\\n        \\\"\\\"\\\"\\n        Parameters:\\n        -----------\\n        primary_metric : str - metric ‡∏´‡∏•‡∏±‡∏Å\\n        primary_threshold : float - threshold ‡∏´‡∏•‡∏±‡∏Å\\n        confirmation_window : int - ‡∏ï‡πâ‡∏≠‡∏á detect ‡∏Å‡∏µ‡πà‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ï‡∏¥‡∏î‡∏Å‡∏±‡∏ô\\n        use_ensemble : bool - ‡πÉ‡∏ä‡πâ‡∏´‡∏•‡∏≤‡∏¢ metrics ‡∏£‡πà‡∏ß‡∏°‡∏Å‡∏±‡∏ô\\n        \\\"\\\"\\\"\\n        self.primary_metric = primary_metric\\n        self.primary_threshold = primary_threshold\\n        self.confirmation_window = confirmation_window\\n        self.use_ensemble = use_ensemble\\n        \\n        self.metrics = CustomDriftMetrics()\\n        \\n        # Thresholds for ensemble\\n        self.thresholds = {\\n            'psi': 0.1,\\n            'wasserstein': 0.5,\\n            'mean_shift': 0.5,\\n            'percentile': 0.3\\n        }\\n        \\n        self.consecutive_count = 0\\n        self.history = []\\n    \\n    def _calculate_all_metrics(self, reference, current):\\n        \\\"\\\"\\\"‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏ó‡∏∏‡∏Å metrics\\\"\\\"\\\"\\n        return {\\n            'psi': self.metrics.psi(reference, current),\\n            'wasserstein': self.metrics.normalized_wasserstein(reference, current),\\n            'mean_shift': self.metrics.mean_shift_ratio(reference, current),\\n            'percentile': self.metrics.percentile_shift(reference, current)\\n        }\\n    \\n    def detect(self, reference, current):\\n        \\\"\\\"\\\"\\n        Detect drift with robustness mechanisms\\n        \\\"\\\"\\\"\\n        all_metrics = self._calculate_all_metrics(reference, current)\\n        \\n        # Single metric detection\\n        primary_value = all_metrics[self.primary_metric]\\n        primary_drift = primary_value > self.primary_threshold\\n        \\n        if self.use_ensemble:\\n            # Ensemble: majority voting\\n            drift_votes = sum(\\n                1 for m, v in all_metrics.items() \\n                if v > self.thresholds.get(m, 0.5)\\n            )\\n            ensemble_drift = drift_votes >= 3  # ‡∏ï‡πâ‡∏≠‡∏á 3/4 metrics agree\\n        else:\\n            ensemble_drift = primary_drift\\n        \\n        # Confirmation mechanism\\n        if ensemble_drift:\\n            self.consecutive_count += 1\\n        else:\\n            self.consecutive_count = 0\\n        \\n        confirmed_drift = self.consecutive_count >= self.confirmation_window\\n        \\n        result = {\\n            'metrics': all_metrics,\\n            'primary_drift': primary_drift,\\n            'ensemble_drift': ensemble_drift,\\n            'consecutive_count': self.consecutive_count,\\n            'confirmed_drift': confirmed_drift\\n        }\\n        \\n        self.history.append(result)\\n        \\n        return result\\n    \\n    def evaluate(self, scenarios):\\n        \\\"\\\"\\\"\\n        Evaluate detector performance on labeled scenarios\\n        \\\"\\\"\\\"\\n        y_true = []\\n        y_pred_primary = []\\n        y_pred_ensemble = []\\n        y_pred_confirmed = []\\n        \\n        for s in scenarios:\\n            y_true.append(1 if s['has_drift'] else 0)\\n            \\n            result = self.detect(s['reference'], s['current'])\\n            y_pred_primary.append(1 if result['primary_drift'] else 0)\\n            y_pred_ensemble.append(1 if result['ensemble_drift'] else 0)\\n            y_pred_confirmed.append(1 if result['confirmed_drift'] else 0)\\n            \\n            # Reset for next scenario\\n            self.consecutive_count = 0\\n        \\n        evaluations = {}\\n        for name, y_pred in [\\n            ('primary', y_pred_primary),\\n            ('ensemble', y_pred_ensemble),\\n            ('confirmed', y_pred_confirmed)\\n        ]:\\n            evaluations[name] = {\\n                'precision': precision_score(y_true, y_pred, zero_division=0),\\n                'recall': recall_score(y_true, y_pred, zero_division=0),\\n                'f1': f1_score(y_true, y_pred, zero_division=0)\\n            }\\n        \\n        return evaluations\\n\\n#%%\\n# ‡∏ó‡∏î‡∏™‡∏≠‡∏ö RobustDriftDetector\\nprint(\\\"=\\\" * 60)\\nprint(\\\"üìä Evaluating Robust Drift Detector\\\")\\nprint(\\\"=\\\" * 60)\\n\\ndetector = RobustDriftDetector(\\n    primary_metric='psi',\\n    primary_threshold=0.1,\\n    confirmation_window=1,  # ‡∏•‡∏î‡πÄ‡∏´‡∏•‡∏∑‡∏≠ 1 ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö single scenario testing\\n    use_ensemble=True\\n)\\n\\nevaluations = detector.evaluate(scenarios)\\n\\nfor method, metrics in evaluations.items():\\n    print(f\\\"\\\\n{method.upper()} Method:\\\")\\n    print(f\\\"  Precision: {metrics['precision']:.3f}\\\")\\n    print(f\\\"  Recall: {metrics['recall']:.3f}\\\")\\n    print(f\\\"  F1 Score: {metrics['f1']:.3f}\\\")\\n\\n#%%\\n# Visualize comparison\\nfig, ax = plt.subplots(figsize=(10, 6))\\n\\nmethods = list(evaluations.keys())\\nx = np.arange(len(methods))\\nwidth = 0.25\\n\\nmetrics_names = ['precision', 'recall', 'f1']\\ncolors = ['blue', 'green', 'red']\\n\\nfor i, (metric, color) in enumerate(zip(metrics_names, colors)):\\n    values = [evaluations[m][metric] for m in methods]\\n    ax.bar(x + i * width, values, width, label=metric.capitalize(), color=color, alpha=0.7)\\n\\nax.set_ylabel('Score')\\nax.set_xlabel('Detection Method')\\nax.set_title('Comparison of Detection Methods')\\nax.set_xticks(x + width)\\nax.set_xticklabels([m.capitalize() for m in methods])\\nax.legend()\\nax.set_ylim(0, 1)\\n\\nfor i, (metric, color) in enumerate(zip(metrics_names, colors)):\\n    values = [evaluations[m][metric] for m in methods]\\n    for j, v in enumerate(values):\\n        ax.text(x[j] + i * width, v + 0.02, f'{v:.2f}', ha='center', fontsize=9)\\n\\nplt.tight_layout()\\nplt.savefig('detection_methods_comparison.png', dpi=150, bbox_inches='tight')\\nplt.show()\\n\\n#%% [markdown]\\n# ## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 7: ‡∏™‡∏£‡∏∏‡∏õ‡πÅ‡∏•‡∏∞ Best Practices\\n\\n#%%\\nprint(\\\"=\\\" * 70)\\nprint(\\\"üìã THRESHOLD SETTING BEST PRACTICES\\\")\\nprint(\\\"=\\\" * 70)\\n\\nbest_practices = \\\"\\\"\\\"\\n1Ô∏è‚É£ DOMAIN-SPECIFIC THRESHOLDS\\n   - ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ default thresholds ‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà validate\\n   - ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏±‡∏ö labeled data ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ\\n   - ‡∏õ‡∏£‡∏∂‡∏Å‡∏©‡∏≤‡∏Å‡∏±‡∏ö domain experts\\n\\n2Ô∏è‚É£ COST-BASED OPTIMIZATION\\n   - ‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤ cost ‡∏Ç‡∏≠‡∏á FP vs FN\\n   - FN ‡πÅ‡∏û‡∏á ‚Üí Lower threshold (more sensitive)\\n   - FP ‡πÅ‡∏û‡∏á ‚Üí Higher threshold (more conservative)\\n\\n3Ô∏è‚É£ ENSEMBLE APPROACH\\n   - ‡πÉ‡∏ä‡πâ‡∏´‡∏•‡∏≤‡∏¢ metrics ‡∏£‡πà‡∏ß‡∏°‡∏Å‡∏±‡∏ô\\n   - Voting mechanism ‡∏•‡∏î false positives\\n   - ‡∏ñ‡πâ‡∏≤ metrics ‡πÑ‡∏°‡πà agree ‚Üí investigate further\\n\\n4Ô∏è‚É£ CONFIRMATION MECHANISM\\n   - Require consecutive detections\\n   - ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô temporary spikes\\n   - Trade-off: delay detection\\n\\n5Ô∏è‚É£ PERIODIC REVIEW\\n   - Review thresholds ‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏∞‡∏¢‡∏∞\\n   - Data patterns ‡∏≠‡∏≤‡∏à‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô\\n   - Business requirements ‡∏≠‡∏≤‡∏à‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô\\n\\n6Ô∏è‚É£ MONITORING & ALERTING\\n   - Different severity levels\\n   - Different thresholds for warning vs critical\\n   - Escalation procedures\\n\\\"\\\"\\\"\\n\\nprint(best_practices)\\n\\n#%% [markdown]\\n# ## ‡∏™‡∏£‡∏∏‡∏õ LAB 5\\n# \\n# ### ‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ:\\n# 1. **Custom Metrics**: PSI, Wasserstein, Mean Shift, Combined Score\\n# 2. **Threshold Optimization**: ‡πÉ‡∏ä‡πâ labeled data ‡∏´‡∏≤ optimal threshold\\n# 3. **Cost-based Approach**: ‡∏õ‡∏£‡∏±‡∏ö threshold ‡∏ï‡∏≤‡∏° FP/FN costs\\n# 4. **Robust Detection**: Ensemble + Confirmation mechanisms\\n\\n#%%\\nprint(\\\"=\\\" * 60)\\nprint(\\\"‚úÖ LAB 5 COMPLETED!\\\")\\nprint(\\\"=\\\" * 60)\\nprint(\\\"\\\"\\\"\\nüìö Key Takeaways:\\n1. Default thresholds rarely optimal for your use case\\n2. Use labeled data to optimize thresholds\\n3. Consider business costs of FP vs FN\\n4. Ensemble methods reduce false alarms\\n5. Confirmation window adds robustness\\n\\nüîú Next: LAB 6 - End-to-End Monitoring Pipeline\\n\\\"\\\"\\\")\\n```\\n\\n---\\n\\n## LAB 6: End-to-End Monitoring Pipeline\\n\\n```python\\n#%% [markdown]\\n# # LAB 6: End-to-End Monitoring Pipeline\\n# ## ‡∏™‡∏£‡πâ‡∏≤‡∏á Pipeline ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Drift Monitoring ‡πÅ‡∏ö‡∏ö‡∏Ñ‡∏£‡∏ö‡∏ß‡∏á‡∏à‡∏£\\n# \\n# ### ‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ:\\n# 1. ‡∏£‡∏ß‡∏°‡∏ó‡∏∏‡∏Å components ‡πÄ‡∏Ç‡πâ‡∏≤‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏±‡∏ô\\n# 2. ‡∏™‡∏£‡πâ‡∏≤‡∏á automated monitoring workflow\\n# 3. Integrate ‡∏Å‡∏±‡∏ö MLflow ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö experiment tracking\\n#\\n# ### ‡∏ó‡∏§‡∏©‡∏é‡∏µ:\\n# Production ML monitoring ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£:\\n# - Automated data ingestion\\n# - Real-time drift detection\\n# - Alerting mechanisms\\n# - Experiment tracking\\n# - Dashboard ‡πÅ‡∏•‡∏∞ reporting\\n\\n#%% [markdown]\\n# ## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 1: ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° Environment\\n\\n#%%\\nimport numpy as np\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\nfrom datetime import datetime, timedelta\\nfrom scipy import stats\\nimport json\\nimport os\\nimport logging\\nfrom collections import deque\\nimport warnings\\nwarnings.filterwarnings('ignore')\\n\\n# Setup logging\\nlogging.basicConfig(\\n    level=logging.INFO,\\n    format='%(asctime)s - %(levelname)s - %(message)s'\\n)\\nlogger = logging.getLogger(__name__)\\n\\nnp.random.seed(42)\\nplt.rcParams['figure.figsize'] = (14, 6)\\n\\nprint(\\\"‚úÖ Libraries imported successfully!\\\")\\n\\n# ‡∏™‡∏£‡πâ‡∏≤‡∏á directory ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö output\\nos.makedirs('monitoring_output', exist_ok=True)\\nos.makedirs('monitoring_output/reports', exist_ok=True)\\nos.makedirs('monitoring_output/alerts', exist_ok=True)\\n\\n#%% [markdown]\\n# ## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 2: ‡∏™‡∏£‡πâ‡∏≤‡∏á Data Classes ‡πÅ‡∏•‡∏∞ Utilities\\n\\n#%%\\nfrom dataclasses import dataclass, field\\nfrom typing import List, Dict, Optional, Callable\\nfrom enum import Enum\\n\\nclass AlertSeverity(Enum):\\n    INFO = \\\"info\\\"\\n    WARNING = \\\"warning\\\"\\n    CRITICAL = \\\"critical\\\"\\n\\nclass DriftType(Enum):\\n    NONE = \\\"none\\\"\\n    MILD = \\\"mild\\\"\\n    MODERATE = \\\"moderate\\\"\\n    SEVERE = \\\"severe\\\"\\n\\n@dataclass\\nclass DriftResult:\\n    \\\"\\\"\\\"‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö drift\\\"\\\"\\\"\\n    timestamp: datetime\\n    feature: str\\n    drift_detected: bool\\n    drift_type: DriftType\\n    psi: float\\n    ks_statistic: float\\n    ks_pvalue: float\\n    reference_mean: float\\n    current_mean: float\\n    reference_std: float\\n    current_std: float\\n    \\n    def to_dict(self):\\n        return {\\n            'timestamp': self.timestamp.isoformat(),\\n            'feature': self.feature,\\n            'drift_detected': self.drift_detected,\\n            'drift_type': self.drift_type.value,\\n            'psi': self.psi,\\n            'ks_statistic': self.ks_statistic,\\n            'ks_pvalue': self.ks_pvalue,\\n            'reference_mean': self.reference_mean,\\n            'current_mean': self.current_mean,\\n            'reference_std': self.reference_std,\\n            'current_std': self.current_std\\n        }\\n\\n@dataclass\\nclass Alert:\\n    \\\"\\\"\\\"Alert object\\\"\\\"\\\"\\n    timestamp: datetime\\n    severity: AlertSeverity\\n    message: str\\n    details: Dict\\n    acknowledged: bool = False\\n    \\n    def to_dict(self):\\n        return {\\n            'timestamp': self.timestamp.isoformat(),\\n            'severity': self.severity.value,\\n            'message': self.message,\\n            'details': self.details,\\n            'acknowledged': self.acknowledged\\n        }\\n\\n@dataclass\\nclass MonitoringConfig:\\n    \\\"\\\"\\\"Configuration ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö monitoring\\\"\\\"\\\"\\n    reference_window_size: int = 1000\\n    current_window_size: int = 200\\n    psi_mild_threshold: float = 0.1\\n    psi_moderate_threshold: float = 0.2\\n    psi_severe_threshold: float = 0.25\\n    ks_significance: float = 0.05\\n    check_interval_seconds: int = 60\\n    alert_cooldown_minutes: int = 30\\n    features_to_monitor: List[str] = field(default_factory=list)\\n\\n#%% [markdown]\\n# ## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 3: ‡∏™‡∏£‡πâ‡∏≤‡∏á Core Monitoring Components\\n\\n#%%\\nclass DriftCalculator:\\n    \\\"\\\"\\\"\\n    Component ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì drift metrics\\n    \\\"\\\"\\\"\\n    \\n    @staticmethod\\n    def calculate_psi(reference: np.ndarray, current: np.ndarray, bins: int = 10) -> float:\\n        \\\"\\\"\\\"Calculate Population Stability Index\\\"\\\"\\\"\\n        breakpoints = np.percentile(reference, np.linspace(0, 100, bins + 1))\\n        breakpoints = np.unique(breakpoints)\\n        \\n        if len(breakpoints) < 2:\\n            return 0.0\\n        \\n        ref_counts, _ = np.histogram(reference, bins=breakpoints)\\n        cur_counts, _ = np.histogram(current, bins=breakpoints)\\n        \\n        eps = 1e-6\\n        ref_props = ref_counts / len(reference) + eps\\n        cur_props = cur_counts / len(current) + eps\\n        \\n        psi = np.sum((cur_props - ref_props) * np.log(cur_props / ref_props))\\n        return float(psi)\\n    \\n    @staticmethod\\n    def calculate_ks_test(reference: np.ndarray, current: np.ndarray) -> tuple:\\n        \\\"\\\"\\\"Calculate Kolmogorov-Smirnov test\\\"\\\"\\\"\\n        statistic, pvalue = stats.ks_2samp(reference, current)\\n        return float(statistic), float(pvalue)\\n    \\n    @staticmethod\\n    def determine_drift_type(psi: float, config: MonitoringConfig) -> DriftType:\\n        \\\"\\\"\\\"Determine drift severity based on PSI\\\"\\\"\\\"\\n        if psi >= config.psi_severe_threshold:\\n            return DriftType.SEVERE\\n        elif psi >= config.psi_moderate_threshold:\\n            return DriftType.MODERATE\\n        elif psi >= config.psi_mild_threshold:\\n            return DriftType.MILD\\n        return DriftType.NONE\\n\\n#%%\\nclass DataBuffer:\\n    \\\"\\\"\\\"\\n    Buffer ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏Å‡πá‡∏ö reference ‡πÅ‡∏•‡∏∞ current data\\n    \\\"\\\"\\\"\\n    \\n    def __init__(self, config: MonitoringConfig):\\n        self.config = config\\n        self.reference_data: Dict[str, deque] = {}\\n        self.current_data: Dict[str, deque] = {}\\n        self.is_initialized = False\\n        \\n    def initialize(self, reference_df: pd.DataFrame):\\n        \\\"\\\"\\\"Initialize with reference data\\\"\\\"\\\"\\n        for feature in self.config.features_to_monitor:\\n            if feature in reference_df.columns:\\n                self.reference_data[feature] = deque(\\n                    reference_df[feature].values[-self.config.reference_window_size:],\\n                    maxlen=self.config.reference_window_size\\n                )\\n                self.current_data[feature] = deque(\\n                    maxlen=self.config.current_window_size\\n                )\\n        self.is_initialized = True\\n        logger.info(f\\\"DataBuffer initialized with {len(self.reference_data)} features\\\")\\n    \\n    def add_data(self, data: Dict[str, float]):\\n        \\\"\\\"\\\"Add new data point\\\"\\\"\\\"\\n        for feature, value in data.items():\\n            if feature in self.current_data:\\n                self.current_data[feature].append(value)\\n    \\n    def get_reference(self, feature: str) -> Optional[np.ndarray]:\\n        \\\"\\\"\\\"Get reference data for a feature\\\"\\\"\\\"\\n        if feature in self.reference_data:\\n            return np.array(self.reference_data[feature])\\n        return None\\n    \\n    def get_current(self, feature: str) -> Optional[np.ndarray]:\\n        \\\"\\\"\\\"Get current data for a feature\\\"\\\"\\\"\\n        if feature in self.current_data:\\n            return np.array(self.current_data[feature])\\n        return None\\n    \\n    def is_current_ready(self) -> bool:\\n        \\\"\\\"\\\"Check if current buffer has enough data\\\"\\\"\\\"\\n        for feature in self.config.features_to_monitor:\\n            if feature in self.current_data:\\n                if len(self.current_data[feature]) < self.config.current_window_size:\\n                    return False\\n        return True\\n    \\n    def update_reference(self):\\n        \\\"\\\"\\\"Update reference with current data\\\"\\\"\\\"\\n        for feature in self.config.features_to_monitor:\\n            if feature in self.current_data and len(self.current_data[feature]) > 0:\\n                # Add current data to reference\\n                for val in self.current_data[feature]:\\n                    self.reference_data[feature].append(val)\\n                self.current_data[feature].clear()\\n        logger.info(\\\"Reference data updated with current window\\\")\\n\\n#%%\\nclass AlertManager:\\n    \\\"\\\"\\\"\\n    Component ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ alerts\\n    \\\"\\\"\\\"\\n    \\n    def __init__(self, config: MonitoringConfig):\\n        self.config = config\\n        self.alerts: List[Alert] = []\\n        self.last_alert_time: Dict[str, datetime] = {}\\n    \\n    def should_alert(self, feature: str) -> bool:\\n        \\\"\\\"\\\"Check if we should send alert (cooldown)\\\"\\\"\\\"\\n        if feature not in self.last_alert_time:\\n            return True\\n        \\n        elapsed = datetime.now() - self.last_alert_time[feature]\\n        return elapsed > timedelta(minutes=self.config.alert_cooldown_minutes)\\n    \\n    def create_alert(self, drift_result: DriftResult) -> Optional[Alert]:\\n        \\\"\\\"\\\"Create alert based on drift result\\\"\\\"\\\"\\n        if not drift_result.drift_detected:\\n            return None\\n        \\n        if not self.should_alert(drift_result.feature):\\n            return None\\n        \\n        # Determine severity\\n        if drift_result.drift_type == DriftType.SEVERE:\\n            severity = AlertSeverity.CRITICAL\\n        elif drift_result.drift_type == DriftType.MODERATE:\\n            severity = AlertSeverity.WARNING\\n        else:\\n            severity = AlertSeverity.INFO\\n        \\n        message = (\\n            f\\\"Drift detected in feature '{drift_result.feature}': \\\"\\n            f\\\"PSI={drift_result.psi:.4f}, Type={drift_result.drift_type.value}\\\"\\n        )\\n        \\n        alert = Alert(\\n            timestamp=drift_result.timestamp,\\n            severity=severity,\\n            message=message,\\n            details=drift_result.to_dict()\\n        )\\n        \\n        self.alerts.append(alert)\\n        self.last_alert_time[drift_result.feature] = datetime.now()\\n        \\n        # Log based on severity\\n        if severity == AlertSeverity.CRITICAL:\\n            logger.critical(message)\\n        elif severity == AlertSeverity.WARNING:\\n            logger.warning(message)\\n        else:\\n            logger.info(message)\\n        \\n        return alert\\n    \\n    def get_active_alerts(self) -> List[Alert]:\\n        \\\"\\\"\\\"Get all unacknowledged alerts\\\"\\\"\\\"\\n        return [a for a in self.alerts if not a.acknowledged]\\n    \\n    def acknowledge_alert(self, index: int):\\n        \\\"\\\"\\\"Acknowledge an alert\\\"\\\"\\\"\\n        if 0 <= index < len(self.alerts):\\n            self.alerts[index].acknowledged = True\\n    \\n    def save_alerts(self, filepath: str):\\n        \\\"\\\"\\\"Save alerts to file\\\"\\\"\\\"\\n        alerts_data = [a.to_dict() for a in self.alerts]\\n        with open(filepath, 'w') as f:\\n            json.dump(alerts_data, f, indent=2)\\n\\n#%% [markdown]\\n# ## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 4: ‡∏™‡∏£‡πâ‡∏≤‡∏á Main Monitoring Pipeline\\n\\n#%%\\nclass DriftMonitoringPipeline:\\n    \\\"\\\"\\\"\\n    Main pipeline ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö drift monitoring\\n    \\\"\\\"\\\"\\n    \\n    def __init__(self, config: MonitoringConfig):\\n        self.config = config\\n        self.data_buffer = DataBuffer(config)\\n        self.alert_manager = AlertManager(config)\\n        self.drift_calculator = DriftCalculator()\\n        self.results_history: List[DriftResult] = []\\n        self.is_running = False\\n        \\n    def initialize(self, reference_data: pd.DataFrame):\\n        \\\"\\\"\\\"Initialize pipeline with reference data\\\"\\\"\\\"\\n        self.data_buffer.initialize(reference_data)\\n        logger.info(\\\"DriftMonitoringPipeline initialized\\\")\\n    \\n    def process_batch(self, batch_data: pd.DataFrame) -> List[DriftResult]:\\n        \\\"\\\"\\\"Process a batch of new data\\\"\\\"\\\"\\n        results = []\\n        \\n        # Add data to buffer\\n        for idx, row in batch_data.iterrows():\\n            data_point = {f: row[f] for f in self.config.features_to_monitor if f in row}\\n            self.data_buffer.add_data(data_point)\\n        \\n        # Check if ready for drift detection\\n        if not self.data_buffer.is_current_ready():\\n            logger.debug(\\\"Current buffer not ready yet\\\")\\n            return results\\n        \\n        # Perform drift detection for each feature\\n        for feature in self.config.features_to_monitor:\\n            result = self._detect_drift_for_feature(feature)\\n            if result:\\n                results.append(result)\\n                self.results_history.append(result)\\n                \\n                # Create alert if needed\\n                if result.drift_detected:\\n                    self.alert_manager.create_alert(result)\\n        \\n        return results\\n    \\n    def _detect_drift_for_feature(self, feature: str) -> Optional[DriftResult]:\\n        \\\"\\\"\\\"Detect drift for a single feature\\\"\\\"\\\"\\n        reference = self.data_buffer.get_reference(feature)\\n        current = self.data_buffer.get_current(feature)\\n        \\n        if reference is None or current is None:\\n            return None\\n        \\n        if len(current) < 10:  # Need minimum samples\\n            return None\\n        \\n        # Calculate metrics\\n        psi = self.drift_calculator.calculate_psi(reference, current)\\n        ks_stat, ks_pval = self.drift_calculator.calculate_ks_test(reference, current)\\n        \\n        # Determine drift type\\n        drift_type = self.drift_calculator.determine_drift_type(psi, self.config)\\n        drift_detected = drift_type != DriftType.NONE or ks_pval < self.config.ks_significance\\n        \\n        return DriftResult(\\n            timestamp=datetime.now(),\\n            feature=feature,\\n            drift_detected=drift_detected,\\n            drift_type=drift_type,\\n            psi=psi,\\n            ks_statistic=ks_stat,\\n            ks_pvalue=ks_pval,\\n            reference_mean=float(np.mean(reference)),\\n            current_mean=float(np.mean(current)),\\n            reference_std=float(np.std(reference)),\\n            current_std=float(np.std(current))\\n        )\\n    \\n    def get_summary_report(self) -> Dict:\\n        \\\"\\\"\\\"Generate summary report\\\"\\\"\\\"\\n        if not self.results_history:\\n            return {'status': 'no_data'}\\n        \\n        # Group by feature\\n        feature_summary = {}\\n        for feature in self.config.features_to_monitor:\\n            feature_results = [r for r in self.results_history if r.feature == feature]\\n            if feature_results:\\n                latest = feature_results[-1]\\n                drift_count = sum(1 for r in feature_results if r.drift_detected)\\n                feature_summary[feature] = {\\n                    'latest_psi': latest.psi,\\n                    'latest_drift_type': latest.drift_type.value,\\n                    'drift_count': drift_count,\\n                    'total_checks': len(feature_results),\\n                    'drift_rate': drift_count / len(feature_results)\\n                }\\n        \\n        return {\\n            'timestamp': datetime.now().isoformat(),\\n            'total_checks': len(self.results_history),\\n            'total_drifts_detected': sum(1 for r in self.results_history if r.drift_detected),\\n            'active_alerts': len(self.alert_manager.get_active_alerts()),\\n            'feature_summary': feature_summary\\n        }\\n    \\n    def save_results(self, output_dir: str = 'monitoring_output'):\\n        \\\"\\\"\\\"Save all results to files\\\"\\\"\\\"\\n        # Save drift results\\n        results_data = [r.to_dict() for r in self.results_history]\\n        with open(f'{output_dir}/drift_results.json', 'w') as f:\\n            json.dump(results_data, f, indent=2)\\n        \\n        # Save alerts\\n        self.alert_manager.save_alerts(f'{output_dir}/alerts/alerts.json')\\n        \\n        # Save summary\\n        summary = self.get_summary_report()\\n        with open(f'{output_dir}/reports/summary.json', 'w') as f:\\n            json.dump(summary, f, indent=2)\\n        \\n        logger.info(f\\\"Results saved to {output_dir}\\\")\\n\\n#%% [markdown]\\n# ## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 5: ‡∏™‡∏£‡πâ‡∏≤‡∏á Report Generator\\n\\n#%%\\nclass ReportGenerator:\\n    \\\"\\\"\\\"\\n    Component ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡∏£‡πâ‡∏≤‡∏á reports ‡πÅ‡∏•‡∏∞ visualizations\\n    \\\"\\\"\\\"\\n    \\n    def __init__(self, pipeline: DriftMonitoringPipeline):\\n        self.pipeline = pipeline\\n    \\n    def generate_dashboard_data(self) -> Dict:\\n        \\\"\\\"\\\"Generate data for dashboard\\\"\\\"\\\"\\n        summary = self.pipeline.get_summary_report()\\n        \\n        # Time series data for each feature\\n        time_series = {}\\n        for feature in self.pipeline.config.features_to_monitor:\\n            feature_results = [\\n                r for r in self.pipeline.results_history \\n                if r.feature == feature\\n            ]\\n            time_series[feature] = {\\n                'timestamps': [r.timestamp.isoformat() for r in feature_results],\\n                'psi_values': [r.psi for r in feature_results],\\n                'drift_types': [r.drift_type.value for r in feature_results]\\n            }\\n        \\n        return {\\n            'summary': summary,\\n            'time_series': time_series,\\n            'alerts': [a.to_dict() for a in self.pipeline.alert_manager.get_active_alerts()]\\n        }\\n    \\n    def plot_drift_trends(self, save_path: str = None):\\n        \\\"\\\"\\\"Plot drift trends for all features\\\"\\\"\\\"\\n        n_features = len(self.pipeline.config.features_to_monitor)\\n        \\n        if n_features == 0:\\n            logger.warning(\\\"No features to plot\\\")\\n            return\\n        \\n        fig, axes = plt.subplots(n_features, 1, figsize=(14, 4*n_features), sharex=True)\\n        \\n        if n_features == 1:\\n            axes = [axes]\\n        \\n        colors = {'none': 'green', 'mild': 'yellow', 'moderate': 'orange', 'severe': 'red'}\\n        \\n        for ax, feature in zip(axes, self.pipeline.config.features_to_monitor):\\n            feature_results = [\\n                r for r in self.pipeline.results_history \\n                if r.feature == feature\\n            ]\\n            \\n            if not feature_results:\\n                continue\\n            \\n            timestamps = [r.timestamp for r in feature_results]\\n            psi_values = [r.psi for r in feature_results]\\n            drift_types = [r.drift_type.value for r in feature_results]\\n            \\n            # Plot PSI\\n            scatter_colors = [colors.get(dt, 'gray') for dt in drift_types]\\n            ax.scatter(timestamps, psi_values, c=scatter_colors, s=30, alpha=0.7)\\n            ax.plot(timestamps, psi_values, 'b-', alpha=0.3)\\n            \\n            # Add thresholds\\n            ax.axhline(self.pipeline.config.psi_mild_threshold, \\n                      color='yellow', linestyle='--', alpha=0.7, label='Mild')\\n            ax.axhline(self.pipeline.config.psi_moderate_threshold, \\n                      color='orange', linestyle='--', alpha=0.7, label='Moderate')\\n            ax.axhline(self.pipeline.config.psi_severe_threshold, \\n                      color='red', linestyle='--', alpha=0.7, label='Severe')\\n            \\n            ax.set_ylabel(f'{feature}\\\\nPSI')\\n            ax.legend(loc='upper right')\\n            ax.grid(True, alpha=0.3)\\n        \\n        axes[-1].set_xlabel('Time')\\n        plt.suptitle('Drift Monitoring Dashboard', fontsize=14, fontweight='bold')\\n        plt.tight_layout()\\n        \\n        if save_path:\\n            plt.savefig(save_path, dpi=150, bbox_inches='tight')\\n            logger.info(f\\\"Plot saved to {save_path}\\\")\\n        \\n        plt.show()\\n    \\n    def generate_html_report(self, output_path: str):\\n        \\\"\\\"\\\"Generate HTML report\\\"\\\"\\\"\\n        summary = self.pipeline.get_summary_report()\\n        \\n        html_content = f\\\"\\\"\\\"\\n        <!DOCTYPE html>\\n        <html>\\n        <head>\\n            <title>Drift Monitoring Report</title>\\n            <style>\\n                body {{ font-family: Arial, sans-serif; margin: 20px; }}\\n                .header {{ background-color: #2196F3; color: white; padding: 20px; }}\\n                .summary {{ background-color: #f0f0f0; padding: 15px; margin: 10px 0; }}\\n                .alert-critical {{ background-color: #ffebee; border-left: 4px solid #f44336; padding: 10px; margin: 5px 0; }}\\n                .alert-warning {{ background-color: #fff3e0; border-left: 4px solid #ff9800; padding: 10px; margin: 5px 0; }}\\n                .alert-info {{ background-color: #e3f2fd; border-left: 4px solid #2196F3; padding: 10px; margin: 5px 0; }}\\n                table {{ border-collapse: collapse; width: 100%; }}\\n                th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}\\n                th {{ background-color: #2196F3; color: white; }}\\n            </style>\\n        </head>\\n        <body>\\n            <div class=\\\"header\\\">\\n                <h1>üîç Drift Monitoring Report</h1>\\n                <p>Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>\\n            </div>\\n            \\n            <div class=\\\"summary\\\">\\n                <h2>üìä Summary</h2>\\n                <p><strong>Total Checks:</strong> {summary.get('total_checks', 0)}</p>\\n                <p><strong>Total Drifts Detected:</strong> {summary.get('total_drifts_detected', 0)}</p>\\n                <p><strong>Active Alerts:</strong> {summary.get('active_alerts', 0)}</p>\\n            </div>\\n            \\n            <h2>üìà Feature Summary</h2>\\n            <table>\\n                <tr>\\n                    <th>Feature</th>\\n                    <th>Latest PSI</th>\\n                    <th>Drift Type</th>\\n                    <th>Drift Count</th>\\n                    <th>Drift Rate</th>\\n                </tr>\\n        \\\"\\\"\\\"\\n        \\n        for feature, data in summary.get('feature_summary', {}).items():\\n            html_content += f\\\"\\\"\\\"\\n                <tr>\\n                    <td>{feature}</td>\\n                    <td>{data['latest_psi']:.4f}</td>\\n                    <td>{data['latest_drift_type']}</td>\\n                    <td>{data['drift_count']}</td>\\n                    <td>{data['drift_rate']:.1%}</td>\\n                </tr>\\n            \\\"\\\"\\\"\\n        \\n        html_content += \\\"\\\"\\\"\\n            </table>\\n            \\n            <h2>‚ö†Ô∏è Active Alerts</h2>\\n        \\\"\\\"\\\"\\n        \\n        for alert in self.pipeline.alert_manager.get_active_alerts():\\n            alert_class = f\\\"alert-{alert.severity.value}\\\"\\n            html_content += f\\\"\\\"\\\"\\n            <div class=\\\"{alert_class}\\\">\\n                <strong>{alert.severity.value.upper()}</strong>: {alert.message}\\n                <br><small>{alert.timestamp.strftime('%Y-%m-%d %H:%M:%S')}</small>\\n            </div>\\n            \\\"\\\"\\\"\\n        \\n        html_content += \\\"\\\"\\\"\\n        </body>\\n        </html>\\n        \\\"\\\"\\\"\\n        \\n        with open(output_path, 'w') as f:\\n            f.write(html_content)\\n        \\n        logger.info(f\\\"HTML report saved to {output_path}\\\")\\n\\n#%% [markdown]\\n# ## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 6: ‡∏ó‡∏î‡∏™‡∏≠‡∏ö Full Pipeline\\n\\n#%%\\n# ‡∏™‡∏£‡πâ‡∏≤‡∏á test data\\ndef generate_test_data(n_samples=5000):\\n    \\\"\\\"\\\"‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡∏°‡∏µ drift\\\"\\\"\\\"\\n    np.random.seed(42)\\n    \\n    data = []\\n    base_means = {'feature_a': 50, 'feature_b': 100, 'feature_c': 75}\\n    \\n    for i in range(n_samples):\\n        # ‡∏™‡∏£‡πâ‡∏≤‡∏á drift ‡∏ó‡∏µ‡πà feature_a ‡∏´‡∏•‡∏±‡∏á sample 3000\\n        if i < 3000:\\n            feature_a_mean = base_means['feature_a']\\n        else:\\n            # Gradual drift\\n            progress = (i - 3000) / 2000\\n            feature_a_mean = base_means['feature_a'] + progress * 20\\n        \\n        # ‡∏™‡∏£‡πâ‡∏≤‡∏á sudden drift ‡∏ó‡∏µ‡πà feature_b ‡∏ó‡∏µ‡πà sample 2000\\n        if i < 2000:\\n            feature_b_mean = base_means['feature_b']\\n        else:\\n            feature_b_mean = base_means['feature_b'] + 30\\n        \\n        # feature_c ‡πÑ‡∏°‡πà‡∏°‡∏µ drift\\n        feature_c_mean = base_means['feature_c']\\n        \\n        data.append({\\n            'timestamp': datetime(2024, 1, 1) + timedelta(hours=i),\\n            'feature_a': np.random.normal(feature_a_mean, 10),\\n            'feature_b': np.random.normal(feature_b_mean, 15),\\n            'feature_c': np.random.normal(feature_c_mean, 12)\\n        })\\n    \\n    return pd.DataFrame(data)\\n\\n# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏î‡∏™‡∏≠‡∏ö\\ntest_data = generate_test_data(5000)\\nprint(f\\\"üìä Test data shape: {test_data.shape}\\\")\\nprint(test_data.head())\\n\\n#%%\\n# Visualize test data\\nfig, axes = plt.subplots(3, 1, figsize=(14, 8), sharex=True)\\n\\nfor ax, feature in zip(axes, ['feature_a', 'feature_b', 'feature_c']):\\n    ax.plot(test_data['timestamp'], test_data[feature], alpha=0.3)\\n    rolling_mean = test_data[feature].rolling(100).mean()\\n    ax.plot(test_data['timestamp'], rolling_mean, 'r-', linewidth=2, label='Rolling Mean')\\n    ax.set_ylabel(feature)\\n    ax.legend()\\n\\naxes[0].set_title('Test Data with Drift Patterns')\\naxes[-1].set_xlabel('Time')\\nplt.tight_layout()\\nplt.show()\\n\\n#%%\\n# Configure ‡πÅ‡∏•‡∏∞ run pipeline\\nconfig = MonitoringConfig(\\n    reference_window_size=1000,\\n    current_window_size=200,\\n    psi_mild_threshold=0.1,\\n    psi_moderate_threshold=0.2,\\n    psi_severe_threshold=0.25,\\n    ks_significance=0.05,\\n    features_to_monitor=['feature_a', 'feature_b', 'feature_c']\\n)\\n\\n# ‡∏™‡∏£‡πâ‡∏≤‡∏á pipeline\\npipeline = DriftMonitoringPipeline(config)\\n\\n# ‡πÉ‡∏ä‡πâ 1000 samples ‡πÅ‡∏£‡∏Å‡πÄ‡∏õ‡πá‡∏ô reference\\nreference_data = test_data.iloc[:1000]\\npipeline.initialize(reference_data)\\n\\nprint(\\\"=\\\" * 60)\\nprint(\\\"üöÄ Starting Drift Monitoring Pipeline\\\")\\nprint(\\\"=\\\" * 60)\\n\\n# Process data ‡πÄ‡∏õ‡πá‡∏ô batches\\nbatch_size = 200\\nremaining_data = test_data.iloc[1000:]\\n\\nfor i in range(0, len(remaining_data), batch_size):\\n    batch = remaining_data.iloc[i:i+batch_size]\\n    results = pipeline.process_batch(batch)\\n    \\n    if results and any(r.drift_detected for r in results):\\n        print(f\\\"\\\\nüìç Batch {i//batch_size + 1} results:\\\")\\n        for r in results:\\n            if r.drift_detected:\\n                print(f\\\"   ‚ö†Ô∏è {r.feature}: PSI={r.psi:.4f}, Type={r.drift_type.value}\\\")\\n\\n#%%\\n# Generate summary\\nprint(\\\"\\\\n\\\" + \\\"=\\\" * 60)\\nprint(\\\"üìä FINAL SUMMARY REPORT\\\")\\nprint(\\\"=\\\" * 60)\\n\\nsummary = pipeline.get_summary_report()\\nprint(f\\\"\\\\nTotal Checks: {summary['total_checks']}\\\")\\nprint(f\\\"Total Drifts Detected: {summary['total_drifts_detected']}\\\")\\nprint(f\\\"Active Alerts: {summary['active_alerts']}\\\")\\n\\nprint(\\\"\\\\nFeature Summary:\\\")\\nfor feature, data in summary['feature_summary'].items():\\n    print(f\\\"\\\\n  {feature}:\\\")\\n    print(f\\\"    Latest PSI: {data['latest_psi']:.4f}\\\")\\n    print(f\\\"    Drift Type: {data['latest_drift_type']}\\\")\\n    print(f\\\"    Drift Count: {data['drift_count']}/{data['total_checks']}\\\")\\n    print(f\\\"    Drift Rate: {data['drift_rate']:.1%}\\\")\\n\\n#%%\\n# Generate reports\\nreport_generator = ReportGenerator(pipeline)\\n\\n# Plot trends\\nreport_generator.plot_drift_trends(save_path='monitoring_output/drift_trends.png')\\n\\n# Generate HTML report\\nreport_generator.generate_html_report('monitoring_output/reports/drift_report.html')\\n\\n# Save all results\\npipeline.save_results()\\n\\nprint(\\\"\\\\n‚úÖ All reports generated and saved to monitoring_output/\\\")\\n\\n#%% [markdown]\\n# ## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 7: Integration ‡∏Å‡∏±‡∏ö MLflow (Optional)\\n\\n#%%\\n# Note: ‡∏™‡πà‡∏ß‡∏ô‡∏ô‡∏µ‡πâ‡∏ï‡πâ‡∏≠‡∏á install mlflow ‡∏Å‡πà‡∏≠‡∏ô: pip install mlflow\\n\\ntry:\\n    import mlflow\\n    MLFLOW_AVAILABLE = True\\nexcept ImportError:\\n    MLFLOW_AVAILABLE = False\\n    print(\\\"‚ö†Ô∏è MLflow not installed. Skipping MLflow integration.\\\")\\n    print(\\\"   Install with: pip install mlflow\\\")\\n\\nif MLFLOW_AVAILABLE:\\n    class MLflowDriftTracker:\\n        \\\"\\\"\\\"\\n        Integration ‡∏Å‡∏±‡∏ö MLflow ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö track drift experiments\\n        \\\"\\\"\\\"\\n        \\n        def __init__(self, experiment_name: str = \\\"drift_monitoring\\\"):\\n            mlflow.set_experiment(experiment_name)\\n            self.active_run = None\\n        \\n        def start_run(self, run_name: str = None):\\n            \\\"\\\"\\\"Start new MLflow run\\\"\\\"\\\"\\n            self.active_run = mlflow.start_run(run_name=run_name)\\n            return self.active_run\\n        \\n        def log_drift_result(self, result: DriftResult):\\n            \\\"\\\"\\\"Log drift result to MLflow\\\"\\\"\\\"\\n            if self.active_run is None:\\n                return\\n            \\n            # Log metrics\\n            mlflow.log_metric(f\\\"{result.feature}_psi\\\", result.psi)\\n            mlflow.log_metric(f\\\"{result.feature}_ks_stat\\\", result.ks_statistic)\\n            mlflow.log_metric(f\\\"{result.feature}_drift\\\", 1 if result.drift_detected else 0)\\n            \\n            # Log params\\n            mlflow.log_param(f\\\"{result.feature}_drift_type\\\", result.drift_type.value)\\n        \\n        def log_summary(self, summary: Dict):\\n            \\\"\\\"\\\"Log summary to MLflow\\\"\\\"\\\"\\n            if self.active_run is None:\\n                return\\n            \\n            mlflow.log_metric(\\\"total_drifts\\\", summary.get('total_drifts_detected', 0))\\n            mlflow.log_metric(\\\"total_checks\\\", summary.get('total_checks', 0))\\n            \\n            for feature, data in summary.get('feature_summary', {}).items():\\n                mlflow.log_metric(f\\\"{feature}_drift_rate\\\", data['drift_rate'])\\n        \\n        def log_artifact(self, artifact_path: str):\\n            \\\"\\\"\\\"Log artifact to MLflow\\\"\\\"\\\"\\n            if self.active_run is None:\\n                return\\n            mlflow.log_artifact(artifact_path)\\n        \\n        def end_run(self):\\n            \\\"\\\"\\\"End MLflow run\\\"\\\"\\\"\\n            if self.active_run:\\n                mlflow.end_run()\\n                self.active_run = None\\n    \\n    # Example usage\\n    print(\\\"\\\\n\\\" + \\\"=\\\" * 60)\\n    print(\\\"üìä Logging to MLflow\\\")\\n    print(\\\"=\\\" * 60)\\n    \\n    tracker = MLflowDriftTracker(\\\"drift_monitoring_lab\\\")\\n    tracker.start_run(run_name=\\\"pipeline_run_1\\\")\\n    \\n    # Log summary\\n    tracker.log_summary(summary)\\n    \\n    # Log artifacts\\n    tracker.log_artifact('monitoring_output/drift_trends.png')\\n    tracker.log_artifact('monitoring_output/reports/drift_report.html')\\n    \\n    tracker.end_run()\\n    print(\\\"‚úÖ Results logged to MLflow\\\")\\n\\n#%% [markdown]\\n# ## ‡∏™‡∏£‡∏∏‡∏õ LAB 6\\n# \\n# ### ‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ:\\n# 1. **Pipeline Architecture**: ‡πÅ‡∏¢‡∏Å components ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô\\n# 2. **Data Classes**: ‡πÉ‡∏ä‡πâ dataclasses ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö type safety\\n# 3. **Alert Management**: ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ alerts ‡∏û‡∏£‡πâ‡∏≠‡∏° cooldown\\n# 4. **Report Generation**: HTML reports ‡πÅ‡∏•‡∏∞ visualizations\\n# 5. **MLflow Integration**: Track experiments\\n\\n#%%\\nprint(\\\"=\\\" * 60)\\nprint(\\\"‚úÖ LAB 6 COMPLETED!\\\")\\nprint(\\\"=\\\" * 60)\\nprint(\\\"\\\"\\\"\\nüìö Key Takeaways:\\n1. Modular architecture ‡∏ó‡∏≥‡πÉ‡∏´‡πâ maintain ‡∏á‡πà‡∏≤‡∏¢\\n2. Data buffering ‡∏ä‡πà‡∏ß‡∏¢ handle streaming data\\n3. Alert management ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô alert fatigue\\n4. Automated reporting saves time\\n5. MLflow integration enables experiment tracking\\n\\nüéâ CONGRATULATIONS! You have completed all 6 labs!\\n\\nüìã What you've learned:\\n- LAB 1: Data Drift Concepts (Covariate/Concept Shift)\\n- LAB 2: Feature Drift Detection (per-feature analysis)\\n- LAB 3: Multivariate Drift Analysis (correlation/PCA)\\n- LAB 4: Production Simulation (streaming detection)\\n- LAB 5: Custom Metrics & Thresholds (optimization)\\n- LAB 6: End-to-End Pipeline (production-ready)\\n\\nüöÄ Next steps:\\n- Deploy pipeline to production\\n- Add more sophisticated alerting (email, Slack)\\n- Integrate with model retraining triggers\\n- Add A/B testing capabilities\\n\\\"\\\"\\\")\\n\\n#%%\\n# Final cleanup and summary\\nprint(\\\"\\\\nüìÅ Output files created:\\\")\\nfor root, dirs, files in os.walk('monitoring_output'):\\n    for file in files:\\n        filepath = os.path.join(root, file)\\n        print(f\\\"   {filepath}\\\")\\n```\\n\\n---\\n\\n## ‡∏™‡∏£‡∏∏‡∏õ‡∏£‡∏ß‡∏°\\n\\n‡∏ó‡∏±‡πâ‡∏á 6 Labs ‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏î‡∏±‡∏á‡∏ô‡∏µ‡πâ:\\n\\n| Lab | ‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠ | ‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ |\\n|-----|--------|----------------|\\n| 1 | Understanding Data Drift | Covariate/Concept Shift, KS/PSI/Wasserstein |\\n| 2 | Feature Drift Detection | Per-feature analysis, Numerical vs Categorical |\\n| 3 | Multivariate Drift | Correlation, PCA, Mahalanobis Distance |\\n| 4 | Production Simulation | Streaming, Sliding Window, Page-Hinkley |\\n| 5 | Custom Metrics & Thresholds | Optimization, Cost-based, Ensemble |\\n| 6 | End-to-End Pipeline | Architecture, Alerting, Reporting, MLflow |\"\n",
    "    }\n",
    "  ],\n",
    "}\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77f36907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Complete MLOps Labs: Understanding Data Drift Concepts with sklearn\n",
      "\n",
      "## LAB 1: Understanding Data Drift Concepts\n",
      "\n",
      "```python\n",
      "#%% [markdown]\n",
      "# # LAB 1: Understanding Data Drift Concepts\n",
      "# ## ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡πÅ‡∏ô‡∏ß‡∏Ñ‡∏¥‡∏î‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á Data Drift\n",
      "# \n",
      "# ### ‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ:\n",
      "# 1. ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á Covariate Shift ‡πÅ‡∏•‡∏∞ Concept Drift\n",
      "# 2. ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ Statistical tests ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö drift detection (KS, PSI, Wasserstein)\n",
      "# 3. ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å drift detection method ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡∏Å‡∏±‡∏ö‡∏™‡∏ñ‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏ì‡πå‡∏ï‡πà‡∏≤‡∏á‡πÜ\n",
      "#\n",
      "# ### ‡∏ó‡∏§‡∏©‡∏é‡∏µ‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô:\n",
      "# **Data Drift** ‡∏Ñ‡∏∑‡∏≠ ‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÄ‡∏ß‡∏•‡∏≤‡∏ú‡πà‡∏≤‡∏ô‡πÑ‡∏õ ‡∏ã‡∏∂‡πà‡∏á‡∏≠‡∏≤‡∏à‡∏™‡πà‡∏á‡∏ú‡∏•‡∏Å‡∏£‡∏∞‡∏ó‡∏ö‡∏ï‡πà‡∏≠‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡∏Ç‡∏≠‡∏á ML Model\n",
      "#\n",
      "# ‡∏°‡∏µ 2 ‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏´‡∏•‡∏±‡∏Å:\n",
      "# - **Covariate Shift**: ‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡∏≠‡∏á input features P(X) ‡πÇ‡∏î‡∏¢‡∏ó‡∏µ‡πà‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå P(Y|X) ‡∏¢‡∏±‡∏á‡∏Ñ‡∏á‡πÄ‡∏î‡∏¥‡∏°\n",
      "# - **Concept Drift**: ‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á input ‡πÅ‡∏•‡∏∞ output P(Y|X)\n",
      "\n",
      "#%% [markdown]\n",
      "# ## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 1: ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° Environment ‡πÅ‡∏•‡∏∞ Import Libraries\n",
      "# \n",
      "# ‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô ‡πÄ‡∏£‡∏≤‡∏à‡∏∞ import libraries ‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î\n",
      "\n",
      "#%%\n",
      "# Import libraries ‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from scipy import stats\n",
      "from sklearn.datasets import make_classification\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.metrics import accuracy_score, classification_report\n",
      "import warnings\n",
      "warnings.filterwarnings('ignore')\n",
      "\n",
      "# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ random seed ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ó‡∏≥‡∏ã‡πâ‡∏≥‡πÑ‡∏î‡πâ\n",
      "np.random.seed(42)\n",
      "\n",
      "# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ matplotlib ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢\n",
      "plt.rcParams['figure.figsize'] = (12, 6)\n",
      "plt.rcParams['font.size'] = 10\n",
      "\n",
      "print(\"‚úÖ Libraries imported successfully!\")\n",
      "print(f\"NumPy version: {np.__version__}\")\n",
      "print(f\"Pandas version: {pd.__version__}\")\n",
      "\n",
      "#%% [markdown]\n",
      "# ## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 2: ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à Covariate Shift\n",
      "# \n",
      "# ### ‡∏ó‡∏§‡∏©‡∏é‡∏µ Covariate Shift:\n",
      "# - ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡∏∂‡πâ‡∏ô‡πÄ‡∏°‡∏∑‡πà‡∏≠ distribution ‡∏Ç‡∏≠‡∏á input features ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á\n",
      "# - ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏£‡∏≤‡∏Ñ‡∏≤‡∏ö‡πâ‡∏≤‡∏ô‡∏ó‡∏µ‡πà train ‡∏Å‡∏±‡∏ö‡∏ö‡πâ‡∏≤‡∏ô‡πÉ‡∏ô‡πÄ‡∏°‡∏∑‡∏≠‡∏á ‡πÅ‡∏ï‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ö‡πâ‡∏≤‡∏ô‡πÉ‡∏ô‡∏ä‡∏ô‡∏ö‡∏ó\n",
      "# - ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á features ‡∏Å‡∏±‡∏ö target ‡∏¢‡∏±‡∏á‡∏Ñ‡∏á‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°\n",
      "#\n",
      "# ‡∏™‡∏π‡∏ï‡∏£‡∏ó‡∏≤‡∏á‡∏Ñ‡∏ì‡∏¥‡∏ï‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå:\n",
      "# - Training: P_train(X) ‚â† P_test(X)\n",
      "# - ‡πÅ‡∏ï‡πà: P(Y|X) ‡∏Ñ‡∏á‡∏ó‡∏µ‡πà\n",
      "\n",
      "#%%\n",
      "def generate_covariate_shift_data():\n",
      "    \"\"\"\n",
      "    ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÅ‡∏™‡∏î‡∏á Covariate Shift\n",
      "    \n",
      "    ‡πÉ‡∏ô‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡∏µ‡πâ:\n",
      "    - Training data: ‡∏≠‡∏≤‡∏¢‡∏∏‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤ 20-40 ‡∏õ‡∏µ\n",
      "    - Production data: ‡∏≠‡∏≤‡∏¢‡∏∏‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤ 40-60 ‡∏õ‡∏µ\n",
      "    - ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏≠‡∏≤‡∏¢‡∏∏‡∏Å‡∏±‡∏ö‡∏û‡∏§‡∏ï‡∏¥‡∏Å‡∏£‡∏£‡∏°‡∏Å‡∏≤‡∏£‡∏ã‡∏∑‡πâ‡∏≠‡∏¢‡∏±‡∏á‡∏Ñ‡∏á‡πÄ‡∏î‡∏¥‡∏°\n",
      "    \"\"\"\n",
      "    \n",
      "    # Training data: ‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤‡∏≠‡∏≤‡∏¢‡∏∏‡∏ô‡πâ‡∏≠‡∏¢ (20-40)\n",
      "    np.random.seed(42)\n",
      "    n_train = 1000\n",
      "    age_train = np.random.normal(30, 5, n_train)\n",
      "    income_train = age_train * 1500 + np.random.normal(0, 5000, n_train)\n",
      "    \n",
      "    # ‡∏Å‡∏é‡∏Å‡∏≤‡∏£‡∏ã‡∏∑‡πâ‡∏≠: ‡∏ã‡∏∑‡πâ‡∏≠‡∏ñ‡πâ‡∏≤ income > 40000 + age * 500\n",
      "    threshold_train = 40000 + age_train * 500\n",
      "    purchase_train = (income_train > threshold_train).astype(int)\n",
      "    \n",
      "    train_df = pd.DataFrame({\n",
      "        'age': age_train,\n",
      "        'income': income_train,\n",
      "        'purchase': purchase_train\n",
      "    })\n",
      "    \n",
      "    # Production data: ‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤‡∏≠‡∏≤‡∏¢‡∏∏‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô (40-60) - Covariate Shift!\n",
      "    n_prod = 1000\n",
      "    age_prod = np.random.normal(50, 5, n_prod)  # ‡∏≠‡∏≤‡∏¢‡∏∏‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô!\n",
      "    income_prod = age_prod * 1500 + np.random.normal(0, 5000, n_prod)\n",
      "    \n",
      "    # ‡∏Å‡∏é‡∏Å‡∏≤‡∏£‡∏ã‡∏∑‡πâ‡∏≠‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏° (‡πÑ‡∏°‡πà‡∏°‡∏µ Concept Drift)\n",
      "    threshold_prod = 40000 + age_prod * 500\n",
      "    purchase_prod = (income_prod > threshold_prod).astype(int)\n",
      "    \n",
      "    prod_df = pd.DataFrame({\n",
      "        'age': age_prod,\n",
      "        'income': income_prod,\n",
      "        'purchase': purchase_prod\n",
      "    })\n",
      "    \n",
      "    return train_df, prod_df\n",
      "\n",
      "# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•\n",
      "train_covariate, prod_covariate = generate_covariate_shift_data()\n",
      "\n",
      "print(\"üìä Training Data Summary:\")\n",
      "print(train_covariate.describe())\n",
      "print(\"\\nüìä Production Data Summary:\")\n",
      "print(prod_covariate.describe())\n",
      "\n",
      "#%%\n",
      "# Visualize Covariate Shift\n",
      "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
      "\n",
      "# Plot 1: Age Distribution\n",
      "axes[0].hist(train_covariate['age'], bins=30, alpha=0.7, label='Training', color='blue')\n",
      "axes[0].hist(prod_covariate['age'], bins=30, alpha=0.7, label='Production', color='red')\n",
      "axes[0].set_xlabel('Age')\n",
      "axes[0].set_ylabel('Frequency')\n",
      "axes[0].set_title('Covariate Shift: Age Distribution\\n(Training vs Production)')\n",
      "axes[0].legend()\n",
      "axes[0].axvline(train_covariate['age'].mean(), color='blue', linestyle='--', label='Train Mean')\n",
      "axes[0].axvline(prod_covariate['age'].mean(), color='red', linestyle='--', label='Prod Mean')\n",
      "\n",
      "# Plot 2: Income Distribution\n",
      "axes[1].hist(train_covariate['income'], bins=30, alpha=0.7, label='Training', color='blue')\n",
      "axes[1].hist(prod_covariate['income'], bins=30, alpha=0.7, label='Production', color='red')\n",
      "axes[1].set_xlabel('Income')\n",
      "axes[1].set_ylabel('Frequency')\n",
      "axes[1].set_title('Covariate Shift: Income Distribution\\n(Training vs Production)')\n",
      "axes[1].legend()\n",
      "\n",
      "# Plot 3: Relationship P(Y|X) ‡∏¢‡∏±‡∏á‡∏Ñ‡∏á‡πÄ‡∏î‡∏¥‡∏°\n",
      "axes[2].scatter(train_covariate['age'], train_covariate['income'], \n",
      "                c=train_covariate['purchase'], alpha=0.3, cmap='coolwarm', label='Training')\n",
      "axes[2].set_xlabel('Age')\n",
      "axes[2].set_ylabel('Income')\n",
      "axes[2].set_title('P(Y|X) Relationship\\n(Decision boundary ‡∏¢‡∏±‡∏á‡∏Ñ‡∏á‡πÄ‡∏î‡∏¥‡∏°)')\n",
      "\n",
      "plt.tight_layout()\n",
      "plt.savefig('covariate_shift_visualization.png', dpi=150, bbox_inches='tight')\n",
      "plt.show()\n",
      "\n",
      "print(\"\\nüí° ‡∏™‡∏±‡∏á‡πÄ‡∏Å‡∏ï: Distribution ‡∏Ç‡∏≠‡∏á Age ‡πÅ‡∏•‡∏∞ Income ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÑ‡∏õ ‡πÅ‡∏ï‡πà‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à‡∏ã‡∏∑‡πâ‡∏≠‡∏¢‡∏±‡∏á‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°\")\n",
      "\n",
      "#%% [markdown]\n",
      "# ## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 3: ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à Concept Drift\n",
      "# \n",
      "# ### ‡∏ó‡∏§‡∏©‡∏é‡∏µ Concept Drift:\n",
      "# - ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡∏∂‡πâ‡∏ô‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á input ‡πÅ‡∏•‡∏∞ output ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á\n",
      "# - ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: ‡∏û‡∏§‡∏ï‡∏¥‡∏Å‡∏£‡∏£‡∏°‡∏Å‡∏≤‡∏£‡∏ã‡∏∑‡πâ‡∏≠‡∏Ç‡∏≠‡∏á‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏´‡∏•‡∏±‡∏á COVID-19\n",
      "# - ‡πÅ‡∏°‡πâ input distribution ‡∏à‡∏∞‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏° ‡πÅ‡∏ï‡πà output ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô\n",
      "#\n",
      "# ‡∏™‡∏π‡∏ï‡∏£‡∏ó‡∏≤‡∏á‡∏Ñ‡∏ì‡∏¥‡∏ï‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå:\n",
      "# - P(X) ‡∏≠‡∏≤‡∏à‡∏Ñ‡∏á‡∏ó‡∏µ‡πà‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏Å‡πá‡πÑ‡∏î‡πâ\n",
      "# - ‡πÅ‡∏ï‡πà: P(Y|X) ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á\n",
      "\n",
      "#%%\n",
      "def generate_concept_drift_data():\n",
      "    \"\"\"\n",
      "    ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÅ‡∏™‡∏î‡∏á Concept Drift\n",
      "    \n",
      "    ‡πÉ‡∏ô‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡∏µ‡πâ:\n",
      "    - Training data: ‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤‡∏ã‡∏∑‡πâ‡∏≠‡πÄ‡∏°‡∏∑‡πà‡∏≠ income ‡∏™‡∏π‡∏á‡∏Å‡∏ß‡πà‡∏≤ threshold\n",
      "    - Production data: threshold ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô (‡∏û‡∏§‡∏ï‡∏¥‡∏Å‡∏£‡∏£‡∏°‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô)\n",
      "    \"\"\"\n",
      "    \n",
      "    np.random.seed(42)\n",
      "    n_samples = 1000\n",
      "    \n",
      "    # Training data\n",
      "    age_train = np.random.normal(35, 10, n_samples)\n",
      "    income_train = np.random.normal(50000, 15000, n_samples)\n",
      "    \n",
      "    # ‡∏Å‡∏é‡∏Å‡∏≤‡∏£‡∏ã‡∏∑‡πâ‡∏≠‡πÄ‡∏î‡∏¥‡∏°: ‡∏ã‡∏∑‡πâ‡∏≠‡∏ñ‡πâ‡∏≤ income > 45000\n",
      "    purchase_train = (income_train > 45000).astype(int)\n",
      "    \n",
      "    train_df = pd.DataFrame({\n",
      "        'age': age_train,\n",
      "        'income': income_train,\n",
      "        'purchase': purchase_train\n",
      "    })\n",
      "    \n",
      "    # Production data: distribution ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°\n",
      "    age_prod = np.random.normal(35, 10, n_samples)\n",
      "    income_prod = np.random.normal(50000, 15000, n_samples)\n",
      "    \n",
      "    # ‡∏Å‡∏é‡∏Å‡∏≤‡∏£‡∏ã‡∏∑‡πâ‡∏≠‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô: ‡∏ã‡∏∑‡πâ‡∏≠‡∏ñ‡πâ‡∏≤ income > 55000 (Concept Drift!)\n",
      "    # ‡∏≠‡∏≤‡∏à‡πÄ‡∏Å‡∏¥‡∏î‡∏à‡∏≤‡∏Å‡πÄ‡∏®‡∏£‡∏©‡∏ê‡∏Å‡∏¥‡∏à‡∏ï‡∏Å‡∏ï‡πà‡∏≥ ‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏£‡∏≤‡∏¢‡πÑ‡∏î‡πâ‡∏™‡∏π‡∏á‡∏Ç‡∏∂‡πâ‡∏ô‡∏ñ‡∏∂‡∏á‡∏à‡∏∞‡∏ã‡∏∑‡πâ‡∏≠\n",
      "    purchase_prod = (income_prod > 55000).astype(int)\n",
      "    \n",
      "    prod_df = pd.DataFrame({\n",
      "        'age': age_prod,\n",
      "        'income': income_prod,\n",
      "        'purchase': purchase_prod\n",
      "    })\n",
      "    \n",
      "    return train_df, prod_df\n",
      "\n",
      "# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•\n",
      "train_concept, prod_concept = generate_concept_drift_data()\n",
      "\n",
      "print(\"üìä Training Data Summary:\")\n",
      "print(f\"Purchase Rate: {train_concept['purchase'].mean():.2%}\")\n",
      "print(train_concept.describe())\n",
      "\n",
      "print(\"\\nüìä Production Data Summary:\")\n",
      "print(f\"Purchase Rate: {prod_concept['purchase'].mean():.2%}\")\n",
      "print(prod_concept.describe())\n",
      "\n",
      "#%%\n",
      "# Visualize Concept Drift\n",
      "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
      "\n",
      "# Plot 1: Income Distribution (‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Å‡∏±‡∏ô)\n",
      "axes[0].hist(train_concept['income'], bins=30, alpha=0.7, label='Training', color='blue')\n",
      "axes[0].hist(prod_concept['income'], bins=30, alpha=0.7, label='Production', color='red')\n",
      "axes[0].set_xlabel('Income')\n",
      "axes[0].set_ylabel('Frequency')\n",
      "axes[0].set_title('No Covariate Shift: Income Distribution\\n(Training ‚âà Production)')\n",
      "axes[0].legend()\n",
      "\n",
      "# Plot 2: Purchase Rate by Income Bin\n",
      "income_bins = np.linspace(20000, 80000, 10)\n",
      "\n",
      "train_purchase_rate = []\n",
      "prod_purchase_rate = []\n",
      "bin_centers = []\n",
      "\n",
      "for i in range(len(income_bins)-1):\n",
      "    mask_train = (train_concept['income'] >= income_bins[i]) & (train_concept['income'] < income_bins[i+1])\n",
      "    mask_prod = (prod_concept['income'] >= income_bins[i]) & (prod_concept['income'] < income_bins[i+1])\n",
      "    \n",
      "    if mask_train.sum() > 0:\n",
      "        train_purchase_rate.append(train_concept[mask_train]['purchase'].mean())\n",
      "    else:\n",
      "        train_purchase_rate.append(0)\n",
      "        \n",
      "    if mask_prod.sum() > 0:\n",
      "        prod_purchase_rate.append(prod_concept[mask_prod]['purchase'].mean())\n",
      "    else:\n",
      "        prod_purchase_rate.append(0)\n",
      "    \n",
      "    bin_centers.append((income_bins[i] + income_bins[i+1]) / 2)\n",
      "\n",
      "axes[1].plot(bin_centers, train_purchase_rate, 'b-o', label='Training P(Y|X)', linewidth=2)\n",
      "axes[1].plot(bin_centers, prod_purchase_rate, 'r-o', label='Production P(Y|X)', linewidth=2)\n",
      "axes[1].set_xlabel('Income')\n",
      "axes[1].set_ylabel('Purchase Probability')\n",
      "axes[1].set_title('Concept Drift: P(Y|X) Changed!\\n(Decision boundary moved)')\n",
      "axes[1].legend()\n",
      "axes[1].axhline(0.5, color='gray', linestyle='--', alpha=0.5)\n",
      "\n",
      "# Plot 3: Decision Boundary Comparison\n",
      "axes[2].scatter(train_concept['income'], train_concept['purchase'] + np.random.normal(0, 0.05, len(train_concept)), \n",
      "                alpha=0.3, color='blue', label='Training')\n",
      "axes[2].scatter(prod_concept['income'], prod_concept['purchase'] + np.random.normal(0, 0.05, len(prod_concept)), \n",
      "                alpha=0.3, color='red', label='Production')\n",
      "axes[2].axvline(45000, color='blue', linestyle='--', linewidth=2, label='Train Threshold (45k)')\n",
      "axes[2].axvline(55000, color='red', linestyle='--', linewidth=2, label='Prod Threshold (55k)')\n",
      "axes[2].set_xlabel('Income')\n",
      "axes[2].set_ylabel('Purchase (with jitter)')\n",
      "axes[2].set_title('Concept Drift Visualization\\n(Threshold shifted from 45k to 55k)')\n",
      "axes[2].legend()\n",
      "\n",
      "plt.tight_layout()\n",
      "plt.savefig('concept_drift_visualization.png', dpi=150, bbox_inches='tight')\n",
      "plt.show()\n",
      "\n",
      "print(\"\\nüí° ‡∏™‡∏±‡∏á‡πÄ‡∏Å‡∏ï: Distribution ‡∏Ç‡∏≠‡∏á Income ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏° ‡πÅ‡∏ï‡πà‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡∏Å‡∏±‡∏ö Purchase ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÑ‡∏õ!\")\n",
      "print(\"   - Training: ‡∏ã‡∏∑‡πâ‡∏≠‡πÄ‡∏°‡∏∑‡πà‡∏≠ income > 45,000\")\n",
      "print(\"   - Production: ‡∏ã‡∏∑‡πâ‡∏≠‡πÄ‡∏°‡∏∑‡πà‡∏≠ income > 55,000\")\n",
      "\n",
      "#%% [markdown]\n",
      "# ## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 4: Statistical Tests ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Drift Detection\n",
      "# \n",
      "# ### 4.1 Kolmogorov-Smirnov (KS) Test\n",
      "# \n",
      "# **‡∏ó‡∏§‡∏©‡∏é‡∏µ:**\n",
      "# - ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö cumulative distribution function (CDF) ‡∏Ç‡∏≠‡∏á 2 samples\n",
      "# - ‡∏ß‡∏±‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á 2 CDFs\n",
      "# - ‡∏Ç‡πâ‡∏≠‡∏î‡∏µ: ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏™‡∏°‡∏°‡∏ï‡∏¥ distribution, sensitive ‡∏ï‡πà‡∏≠‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á\n",
      "# - ‡∏Ç‡πâ‡∏≠‡πÄ‡∏™‡∏µ‡∏¢: ‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ‡∏Å‡∏±‡∏ö continuous variables ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô\n",
      "#\n",
      "# **‡∏Å‡∏≤‡∏£‡∏ï‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°:**\n",
      "# - KS Statistic: 0-1 (‡∏¢‡∏¥‡πà‡∏á‡∏™‡∏π‡∏á = ‡∏¢‡∏¥‡πà‡∏á‡∏ï‡πà‡∏≤‡∏á)\n",
      "# - p-value < 0.05: reject null hypothesis ‚Üí ‡∏°‡∏µ drift\n",
      "\n",
      "#%%\n",
      "def kolmogorov_smirnov_test(data1, data2, feature_name=\"feature\"):\n",
      "    \"\"\"\n",
      "    ‡∏ó‡∏≥ KS Test ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö drift\n",
      "    \n",
      "    Parameters:\n",
      "    -----------\n",
      "    data1 : array-like - ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ä‡∏∏‡∏î‡πÅ‡∏£‡∏Å (reference/training)\n",
      "    data2 : array-like - ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ä‡∏∏‡∏î‡∏ó‡∏µ‡πà‡∏™‡∏≠‡∏á (current/production)\n",
      "    feature_name : str - ‡∏ä‡∏∑‡πà‡∏≠ feature\n",
      "    \n",
      "    Returns:\n",
      "    --------\n",
      "    dict : ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Ç‡∏≠‡∏á KS test\n",
      "    \"\"\"\n",
      "    statistic, p_value = stats.ks_2samp(data1, data2)\n",
      "    \n",
      "    # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î threshold ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö drift\n",
      "    drift_detected = p_value < 0.05\n",
      "    \n",
      "    result = {\n",
      "        'feature': feature_name,\n",
      "        'test': 'Kolmogorov-Smirnov',\n",
      "        'statistic': statistic,\n",
      "        'p_value': p_value,\n",
      "        'drift_detected': drift_detected,\n",
      "        'interpretation': 'DRIFT DETECTED!' if drift_detected else 'No significant drift'\n",
      "    }\n",
      "    \n",
      "    return result\n",
      "\n",
      "# ‡∏ó‡∏î‡∏™‡∏≠‡∏ö KS Test ‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Covariate Shift\n",
      "print(\"=\" * 60)\n",
      "print(\"üîç KS Test Results for Covariate Shift Data\")\n",
      "print(\"=\" * 60)\n",
      "\n",
      "ks_age = kolmogorov_smirnov_test(train_covariate['age'], prod_covariate['age'], 'age')\n",
      "ks_income = kolmogorov_smirnov_test(train_covariate['income'], prod_covariate['income'], 'income')\n",
      "\n",
      "for result in [ks_age, ks_income]:\n",
      "    print(f\"\\nFeature: {result['feature']}\")\n",
      "    print(f\"  KS Statistic: {result['statistic']:.4f}\")\n",
      "    print(f\"  P-value: {result['p_value']:.6f}\")\n",
      "    print(f\"  Result: {result['interpretation']}\")\n",
      "\n",
      "#%%\n",
      "# Visualize KS Test\n",
      "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
      "\n",
      "for idx, (feature, ax) in enumerate(zip(['age', 'income'], axes)):\n",
      "    # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì CDF\n",
      "    sorted_train = np.sort(train_covariate[feature])\n",
      "    sorted_prod = np.sort(prod_covariate[feature])\n",
      "    \n",
      "    cdf_train = np.arange(1, len(sorted_train) + 1) / len(sorted_train)\n",
      "    cdf_prod = np.arange(1, len(sorted_prod) + 1) / len(sorted_prod)\n",
      "    \n",
      "    ax.plot(sorted_train, cdf_train, 'b-', linewidth=2, label='Training CDF')\n",
      "    ax.plot(sorted_prod, cdf_prod, 'r-', linewidth=2, label='Production CDF')\n",
      "    \n",
      "    # ‡πÅ‡∏™‡∏î‡∏á KS statistic (maximum distance)\n",
      "    ks_result = kolmogorov_smirnov_test(train_covariate[feature], prod_covariate[feature], feature)\n",
      "    \n",
      "    ax.set_xlabel(feature.capitalize())\n",
      "    ax.set_ylabel('Cumulative Probability')\n",
      "    ax.set_title(f'KS Test: {feature.capitalize()}\\nKS Statistic = {ks_result[\"statistic\"]:.4f}, p-value = {ks_result[\"p_value\"]:.4f}')\n",
      "    ax.legend()\n",
      "    ax.grid(True, alpha=0.3)\n",
      "\n",
      "plt.tight_layout()\n",
      "plt.savefig('ks_test_visualization.png', dpi=150, bbox_inches='tight')\n",
      "plt.show()\n",
      "\n",
      "#%% [markdown]\n",
      "# ### 4.2 Population Stability Index (PSI)\n",
      "# \n",
      "# **‡∏ó‡∏§‡∏©‡∏é‡∏µ:**\n",
      "# - ‡∏ß‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡∏≠‡∏á distribution ‡πÇ‡∏î‡∏¢‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö proportions ‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞ bin\n",
      "# - ‡∏ô‡∏¥‡∏¢‡∏°‡πÉ‡∏ä‡πâ‡πÉ‡∏ô credit scoring ‡πÅ‡∏•‡∏∞ financial models\n",
      "# - ‡∏™‡∏π‡∏ï‡∏£: PSI = Œ£ (Actual% - Expected%) √ó ln(Actual% / Expected%)\n",
      "#\n",
      "# **‡∏Å‡∏≤‡∏£‡∏ï‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏° PSI:**\n",
      "# - PSI < 0.1: ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏ó‡∏µ‡πà‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç\n",
      "# - 0.1 ‚â§ PSI < 0.25: ‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á ‡∏Ñ‡∏ß‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö\n",
      "# - PSI ‚â• 0.25: ‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏°‡∏≤‡∏Å ‡∏ï‡πâ‡∏≠‡∏á‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£\n",
      "\n",
      "#%%\n",
      "def calculate_psi(expected, actual, bins=10, eps=1e-6):\n",
      "    \"\"\"\n",
      "    ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Population Stability Index (PSI)\n",
      "    \n",
      "    Parameters:\n",
      "    -----------\n",
      "    expected : array-like - ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• reference (training)\n",
      "    actual : array-like - ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• current (production)\n",
      "    bins : int - ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô bins ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö discretize\n",
      "    eps : float - ‡∏Ñ‡πà‡∏≤‡πÄ‡∏•‡πá‡∏Å‡πÜ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô division by zero\n",
      "    \n",
      "    Returns:\n",
      "    --------\n",
      "    float : ‡∏Ñ‡πà‡∏≤ PSI\n",
      "    dict : ‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì\n",
      "    \"\"\"\n",
      "    # ‡∏™‡∏£‡πâ‡∏≤‡∏á bins ‡∏à‡∏≤‡∏Å expected data\n",
      "    breakpoints = np.percentile(expected, np.linspace(0, 100, bins + 1))\n",
      "    breakpoints = np.unique(breakpoints)  # ‡∏•‡∏ö duplicates\n",
      "    \n",
      "    # ‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞ bin\n",
      "    expected_counts, _ = np.histogram(expected, bins=breakpoints)\n",
      "    actual_counts, _ = np.histogram(actual, bins=breakpoints)\n",
      "    \n",
      "    # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì proportions\n",
      "    expected_props = expected_counts / len(expected) + eps\n",
      "    actual_props = actual_counts / len(actual) + eps\n",
      "    \n",
      "    # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì PSI\n",
      "    psi_values = (actual_props - expected_props) * np.log(actual_props / expected_props)\n",
      "    psi = np.sum(psi_values)\n",
      "    \n",
      "    # ‡∏ï‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå\n",
      "    if psi < 0.1:\n",
      "        interpretation = \"No significant change (PSI < 0.1)\"\n",
      "        severity = \"LOW\"\n",
      "    elif psi < 0.25:\n",
      "        interpretation = \"Moderate change - monitor closely (0.1 ‚â§ PSI < 0.25)\"\n",
      "        severity = \"MEDIUM\"\n",
      "    else:\n",
      "        interpretation = \"Significant change - action required (PSI ‚â• 0.25)\"\n",
      "        severity = \"HIGH\"\n",
      "    \n",
      "    return {\n",
      "        'psi': psi,\n",
      "        'interpretation': interpretation,\n",
      "        'severity': severity,\n",
      "        'bin_psi_values': psi_values,\n",
      "        'expected_props': expected_props,\n",
      "        'actual_props': actual_props,\n",
      "        'breakpoints': breakpoints\n",
      "    }\n",
      "\n",
      "# ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì PSI ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞ feature\n",
      "print(\"=\" * 60)\n",
      "print(\"üìä PSI Results for Covariate Shift Data\")\n",
      "print(\"=\" * 60)\n",
      "\n",
      "for feature in ['age', 'income']:\n",
      "    psi_result = calculate_psi(train_covariate[feature], prod_covariate[feature])\n",
      "    print(f\"\\nFeature: {feature}\")\n",
      "    print(f\"  PSI Value: {psi_result['psi']:.4f}\")\n",
      "    print(f\"  Severity: {psi_result['severity']}\")\n",
      "    print(f\"  Interpretation: {psi_result['interpretation']}\")\n",
      "\n",
      "#%%\n",
      "# Visualize PSI\n",
      "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
      "\n",
      "for idx, feature in enumerate(['age', 'income']):\n",
      "    psi_result = calculate_psi(train_covariate[feature], prod_covariate[feature])\n",
      "    \n",
      "    x = np.arange(len(psi_result['expected_props']))\n",
      "    width = 0.35\n",
      "    \n",
      "    axes[idx].bar(x - width/2, psi_result['expected_props'], width, label='Expected (Train)', color='blue', alpha=0.7)\n",
      "    axes[idx].bar(x + width/2, psi_result['actual_props'], width, label='Actual (Prod)', color='red', alpha=0.7)\n",
      "    \n",
      "    axes[idx].set_xlabel('Bin')\n",
      "    axes[idx].set_ylabel('Proportion')\n",
      "    axes[idx].set_title(f'PSI Analysis: {feature.capitalize()}\\nPSI = {psi_result[\"psi\"]:.4f} ({psi_result[\"severity\"]})')\n",
      "    axes[idx].legend()\n",
      "    axes[idx].set_xticks(x)\n",
      "\n",
      "plt.tight_layout()\n",
      "plt.savefig('psi_visualization.png', dpi=150, bbox_inches='tight')\n",
      "plt.show()\n",
      "\n",
      "#%% [markdown]\n",
      "# ### 4.3 Wasserstein Distance (Earth Mover's Distance)\n",
      "# \n",
      "# **‡∏ó‡∏§‡∏©‡∏é‡∏µ:**\n",
      "# - ‡∏ß‡∏±‡∏î \"‡∏á‡∏≤‡∏ô\" ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô distribution ‡∏´‡∏ô‡∏∂‡πà‡∏á‡πÑ‡∏õ‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏µ‡∏Å distribution\n",
      "# - ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏ï‡πâ‡∏ô‡∏ó‡∏∏‡∏ô‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏Ç‡∏ô‡∏¢‡πâ‡∏≤‡∏¢‡∏î‡∏¥‡∏ô (Earth Mover)\n",
      "# - ‡∏Ç‡πâ‡∏≠‡∏î‡∏µ: ‡∏Ñ‡∏≥‡∏ô‡∏∂‡∏á‡∏ñ‡∏∂‡∏á distance ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á bins, sensitive ‡∏ï‡πà‡∏≠ shift ‡πÉ‡∏ô‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á\n",
      "# - ‡∏Ç‡πâ‡∏≠‡πÄ‡∏™‡∏µ‡∏¢: ‡∏ï‡πâ‡∏≠‡∏á normalize ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡πÑ‡∏î‡πâ\n",
      "\n",
      "#%%\n",
      "def wasserstein_distance_test(data1, data2, feature_name=\"feature\"):\n",
      "    \"\"\"\n",
      "    ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Wasserstein Distance ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö drift detection\n",
      "    \n",
      "    Parameters:\n",
      "    -----------\n",
      "    data1 : array-like - ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ä‡∏∏‡∏î‡πÅ‡∏£‡∏Å (reference)\n",
      "    data2 : array-like - ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ä‡∏∏‡∏î‡∏ó‡∏µ‡πà‡∏™‡∏≠‡∏á (current)\n",
      "    feature_name : str - ‡∏ä‡∏∑‡πà‡∏≠ feature\n",
      "    \n",
      "    Returns:\n",
      "    --------\n",
      "    dict : ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Ç‡∏≠‡∏á Wasserstein distance\n",
      "    \"\"\"\n",
      "    # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Wasserstein distance\n",
      "    distance = stats.wasserstein_distance(data1, data2)\n",
      "    \n",
      "    # Normalize ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ standard deviation ‡∏Ç‡∏≠‡∏á reference data\n",
      "    std_ref = np.std(data1)\n",
      "    normalized_distance = distance / std_ref if std_ref > 0 else distance\n",
      "    \n",
      "    # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î threshold (‡∏õ‡∏£‡∏±‡∏ö‡πÑ‡∏î‡πâ‡∏ï‡∏≤‡∏° domain)\n",
      "    if normalized_distance < 0.1:\n",
      "        severity = \"LOW\"\n",
      "        interpretation = \"No significant drift\"\n",
      "    elif normalized_distance < 0.5:\n",
      "        severity = \"MEDIUM\"\n",
      "        interpretation = \"Moderate drift detected\"\n",
      "    else:\n",
      "        severity = \"HIGH\"\n",
      "        interpretation = \"Significant drift detected\"\n",
      "    \n",
      "    return {\n",
      "        'feature': feature_name,\n",
      "        'distance': distance,\n",
      "        'normalized_distance': normalized_distance,\n",
      "        'severity': severity,\n",
      "        'interpretation': interpretation\n",
      "    }\n",
      "\n",
      "# ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Wasserstein Distance\n",
      "print(\"=\" * 60)\n",
      "print(\"üìè Wasserstein Distance Results for Covariate Shift Data\")\n",
      "print(\"=\" * 60)\n",
      "\n",
      "for feature in ['age', 'income']:\n",
      "    wd_result = wasserstein_distance_test(train_covariate[feature], prod_covariate[feature], feature)\n",
      "    print(f\"\\nFeature: {feature}\")\n",
      "    print(f\"  Wasserstein Distance: {wd_result['distance']:.4f}\")\n",
      "    print(f\"  Normalized Distance: {wd_result['normalized_distance']:.4f}\")\n",
      "    print(f\"  Severity: {wd_result['severity']}\")\n",
      "    print(f\"  Interpretation: {wd_result['interpretation']}\")\n",
      "\n",
      "#%%\n",
      "# Visualize Wasserstein Distance\n",
      "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
      "\n",
      "for idx, feature in enumerate(['age', 'income']):\n",
      "    ax = axes[idx]\n",
      "    \n",
      "    # ‡πÅ‡∏™‡∏î‡∏á histogram ‡πÅ‡∏•‡∏∞ KDE\n",
      "    ax.hist(train_covariate[feature], bins=30, alpha=0.5, density=True, label='Training', color='blue')\n",
      "    ax.hist(prod_covariate[feature], bins=30, alpha=0.5, density=True, label='Production', color='red')\n",
      "    \n",
      "    # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Wasserstein distance\n",
      "    wd_result = wasserstein_distance_test(train_covariate[feature], prod_covariate[feature], feature)\n",
      "    \n",
      "    # ‡πÅ‡∏™‡∏î‡∏á mean ‡πÅ‡∏•‡∏∞ \"‡∏Å‡∏≤‡∏£‡∏Ç‡∏ô‡∏¢‡πâ‡∏≤‡∏¢\"\n",
      "    mean_train = train_covariate[feature].mean()\n",
      "    mean_prod = prod_covariate[feature].mean()\n",
      "    \n",
      "    ax.axvline(mean_train, color='blue', linestyle='--', linewidth=2, label=f'Train Mean: {mean_train:.1f}')\n",
      "    ax.axvline(mean_prod, color='red', linestyle='--', linewidth=2, label=f'Prod Mean: {mean_prod:.1f}')\n",
      "    \n",
      "    # ‡πÅ‡∏™‡∏î‡∏á arrow ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö \"earth moving\"\n",
      "    ax.annotate('', xy=(mean_prod, 0.01), xytext=(mean_train, 0.01),\n",
      "                arrowprops=dict(arrowstyle='->', color='green', lw=3))\n",
      "    \n",
      "    ax.set_xlabel(feature.capitalize())\n",
      "    ax.set_ylabel('Density')\n",
      "    ax.set_title(f'Wasserstein Distance: {feature.capitalize()}\\nDistance = {wd_result[\"distance\"]:.2f} ({wd_result[\"severity\"]})')\n",
      "    ax.legend(loc='upper right')\n",
      "\n",
      "plt.tight_layout()\n",
      "plt.savefig('wasserstein_visualization.png', dpi=150, bbox_inches='tight')\n",
      "plt.show()\n",
      "\n",
      "print(\"\\nüí° ‡∏•‡∏π‡∏Å‡∏®‡∏£‡∏™‡∏µ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ß‡πÅ‡∏™‡∏î‡∏á‡∏ó‡∏¥‡∏®‡∏ó‡∏≤‡∏á‡∏Ç‡∏≠‡∏á '‡∏Å‡∏≤‡∏£‡∏Ç‡∏ô‡∏¢‡πâ‡∏≤‡∏¢' ‡∏à‡∏≤‡∏Å Training ‡πÑ‡∏õ Production\")\n",
      "\n",
      "#%% [markdown]\n",
      "# ## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 5: ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡πÅ‡∏•‡∏∞‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Drift Detection Method\n",
      "# \n",
      "# ### ‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πâ‡∏≠‡∏î‡∏µ-‡∏Ç‡πâ‡∏≠‡πÄ‡∏™‡∏µ‡∏¢‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞ method:\n",
      "#\n",
      "# | Method | ‡∏Ç‡πâ‡∏≠‡∏î‡∏µ | ‡∏Ç‡πâ‡∏≠‡πÄ‡∏™‡∏µ‡∏¢ | ‡πÉ‡∏ä‡πâ‡πÄ‡∏°‡∏∑‡πà‡∏≠ |\n",
      "# |--------|-------|---------|---------|\n",
      "# | KS Test | ‡πÑ‡∏°‡πà‡∏™‡∏°‡∏°‡∏ï‡∏¥ distribution, sensitive | ‡πÉ‡∏ä‡πâ‡∏Å‡∏±‡∏ö continuous ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô | ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ statistical significance |\n",
      "# | PSI | ‡∏°‡∏µ threshold ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô, industry standard | ‡∏ï‡πâ‡∏≠‡∏á binning | Credit scoring, Risk models |\n",
      "# | Wasserstein | ‡∏Ñ‡∏≥‡∏ô‡∏∂‡∏á‡∏ñ‡∏∂‡∏á distance, ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏á‡πà‡∏≤‡∏¢ | ‡∏ï‡πâ‡∏≠‡∏á normalize | ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö shift ‡πÉ‡∏ô‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á |\n",
      "\n",
      "#%%\n",
      "def comprehensive_drift_analysis(reference_data, current_data, feature_name):\n",
      "    \"\"\"\n",
      "    ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå drift ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡∏ó‡∏∏‡∏Å methods ‡πÅ‡∏•‡∏∞‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå\n",
      "    \"\"\"\n",
      "    results = {\n",
      "        'feature': feature_name,\n",
      "        'ks': kolmogorov_smirnov_test(reference_data, current_data, feature_name),\n",
      "        'psi': calculate_psi(reference_data, current_data),\n",
      "        'wasserstein': wasserstein_distance_test(reference_data, current_data, feature_name)\n",
      "    }\n",
      "    \n",
      "    # ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•\n",
      "    drift_votes = 0\n",
      "    if results['ks']['drift_detected']:\n",
      "        drift_votes += 1\n",
      "    if results['psi']['severity'] in ['MEDIUM', 'HIGH']:\n",
      "        drift_votes += 1\n",
      "    if results['wasserstein']['severity'] in ['MEDIUM', 'HIGH']:\n",
      "        drift_votes += 1\n",
      "    \n",
      "    results['consensus'] = 'DRIFT' if drift_votes >= 2 else 'NO DRIFT'\n",
      "    results['drift_votes'] = drift_votes\n",
      "    \n",
      "    return results\n",
      "\n",
      "# ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå\n",
      "print(\"=\" * 80)\n",
      "print(\"üìä COMPREHENSIVE DRIFT ANALYSIS COMPARISON\")\n",
      "print(\"=\" * 80)\n",
      "\n",
      "comparison_results = []\n",
      "\n",
      "for feature in ['age', 'income']:\n",
      "    result = comprehensive_drift_analysis(\n",
      "        train_covariate[feature], \n",
      "        prod_covariate[feature], \n",
      "        feature\n",
      "    )\n",
      "    comparison_results.append(result)\n",
      "    \n",
      "    print(f\"\\nüîç Feature: {feature.upper()}\")\n",
      "    print(\"-\" * 40)\n",
      "    print(f\"  KS Test: stat={result['ks']['statistic']:.4f}, p={result['ks']['p_value']:.4f} ‚Üí {result['ks']['interpretation']}\")\n",
      "    print(f\"  PSI: {result['psi']['psi']:.4f} ‚Üí {result['psi']['severity']}\")\n",
      "    print(f\"  Wasserstein: {result['wasserstein']['normalized_distance']:.4f} ‚Üí {result['wasserstein']['severity']}\")\n",
      "    print(f\"  üìã CONSENSUS ({result['drift_votes']}/3 votes): {result['consensus']}\")\n",
      "\n",
      "#%%\n",
      "# ‡∏™‡∏£‡πâ‡∏≤‡∏á comparison table\n",
      "comparison_df = pd.DataFrame([\n",
      "    {\n",
      "        'Feature': r['feature'],\n",
      "        'KS Statistic': f\"{r['ks']['statistic']:.4f}\",\n",
      "        'KS p-value': f\"{r['ks']['p_value']:.4f}\",\n",
      "        'KS Drift': '‚úì' if r['ks']['drift_detected'] else '‚úó',\n",
      "        'PSI': f\"{r['psi']['psi']:.4f}\",\n",
      "        'PSI Severity': r['psi']['severity'],\n",
      "        'Wasserstein': f\"{r['wasserstein']['normalized_distance']:.4f}\",\n",
      "        'WD Severity': r['wasserstein']['severity'],\n",
      "        'Consensus': r['consensus']\n",
      "    }\n",
      "    for r in comparison_results\n",
      "])\n",
      "\n",
      "print(\"\\n\" + \"=\" * 80)\n",
      "print(\"üìä SUMMARY TABLE\")\n",
      "print(\"=\" * 80)\n",
      "print(comparison_df.to_string(index=False))\n",
      "\n",
      "#%% [markdown]\n",
      "# ## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 6: ‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Drift Detection Method\n",
      "# \n",
      "# ### Decision Tree ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Method:\n",
      "#\n",
      "# ```\n",
      "# 1. Data Type?\n",
      "#    ‚îú‚îÄ‚îÄ Continuous ‚Üí ‡πÑ‡∏õ‡∏Ç‡πâ‡∏≠ 2\n",
      "#    ‚îî‚îÄ‚îÄ Categorical ‚Üí ‡πÉ‡∏ä‡πâ Chi-squared test ‡∏´‡∏£‡∏∑‡∏≠ PSI\n",
      "#\n",
      "# 2. ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ Statistical Significance?\n",
      "#    ‚îú‚îÄ‚îÄ ‡πÉ‡∏ä‡πà ‚Üí KS Test ‡∏´‡∏£‡∏∑‡∏≠ Chi-squared\n",
      "#    ‚îî‚îÄ‚îÄ ‡πÑ‡∏°‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô ‚Üí PSI ‡∏´‡∏£‡∏∑‡∏≠ Wasserstein\n",
      "#\n",
      "# 3. Industry Requirement?\n",
      "#    ‚îú‚îÄ‚îÄ Finance/Credit ‚Üí PSI (‡∏°‡∏µ regulatory standards)\n",
      "#    ‚îî‚îÄ‚îÄ ‡∏≠‡∏∑‡πà‡∏ô‡πÜ ‚Üí ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ï‡∏≤‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°\n",
      "#\n",
      "# 4. ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏Ñ‡∏ß‡∏≤‡∏° sensitive ‡∏™‡∏π‡∏á?\n",
      "#    ‚îú‚îÄ‚îÄ ‡πÉ‡∏ä‡πà ‚Üí Wasserstein (detect small shifts)\n",
      "#    ‚îî‚îÄ‚îÄ ‡∏õ‡∏Å‡∏ï‡∏¥ ‚Üí KS ‡∏´‡∏£‡∏∑‡∏≠ PSI\n",
      "# ```\n",
      "\n",
      "#%%\n",
      "def recommend_drift_method(data_type, needs_significance, industry, high_sensitivity):\n",
      "    \"\"\"\n",
      "    ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥ drift detection method ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°\n",
      "    \n",
      "    Parameters:\n",
      "    -----------\n",
      "    data_type : str - 'continuous' ‡∏´‡∏£‡∏∑‡∏≠ 'categorical'\n",
      "    needs_significance : bool - ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ statistical significance ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà\n",
      "    industry : str - 'finance', 'healthcare', 'general'\n",
      "    high_sensitivity : bool - ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ sensitivity ‡∏™‡∏π‡∏á‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà\n",
      "    \n",
      "    Returns:\n",
      "    --------\n",
      "    str : recommended method ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•\n",
      "    \"\"\"\n",
      "    recommendations = []\n",
      "    \n",
      "    if data_type == 'categorical':\n",
      "        recommendations.append({\n",
      "            'method': 'Chi-squared Test',\n",
      "            'reason': '‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö categorical data',\n",
      "            'priority': 1\n",
      "        })\n",
      "        recommendations.append({\n",
      "            'method': 'PSI (binned)',\n",
      "            'reason': '‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏ä‡πâ‡∏Å‡∏±‡∏ö categorical ‡πÑ‡∏î‡πâ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ category ‡πÄ‡∏õ‡πá‡∏ô bins',\n",
      "            'priority': 2\n",
      "        })\n",
      "    else:  # continuous\n",
      "        if industry == 'finance':\n",
      "            recommendations.append({\n",
      "                'method': 'PSI',\n",
      "                'reason': 'Industry standard ‡πÉ‡∏ô finance, ‡∏°‡∏µ regulatory requirements',\n",
      "                'priority': 1\n",
      "            })\n",
      "        \n",
      "        if needs_significance:\n",
      "            recommendations.append({\n",
      "                'method': 'KS Test',\n",
      "                'reason': '‡πÉ‡∏´‡πâ p-value ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö statistical significance',\n",
      "                'priority': 1 if industry != 'finance' else 2\n",
      "            })\n",
      "        \n",
      "        if high_sensitivity:\n",
      "            recommendations.append({\n",
      "                'method': 'Wasserstein Distance',\n",
      "                'reason': 'Sensitive ‡∏ï‡πà‡∏≠ small shifts ‡πÅ‡∏•‡∏∞ location changes',\n",
      "                'priority': 2\n",
      "            })\n",
      "        \n",
      "        # Default recommendation\n",
      "        if not recommendations:\n",
      "            recommendations.append({\n",
      "                'method': 'PSI + KS Test',\n",
      "                'reason': '‡πÉ‡∏ä‡πâ‡∏ó‡∏±‡πâ‡∏á‡∏™‡∏≠‡∏á‡πÄ‡∏û‡∏∑‡πà‡∏≠ cross-validate',\n",
      "                'priority': 1\n",
      "            })\n",
      "    \n",
      "    # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏ï‡∏≤‡∏° priority\n",
      "    recommendations.sort(key=lambda x: x['priority'])\n",
      "    \n",
      "    return recommendations\n",
      "\n",
      "# ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô\n",
      "print(\"=\" * 60)\n",
      "print(\"üéØ DRIFT METHOD RECOMMENDATION EXAMPLES\")\n",
      "print(\"=\" * 60)\n",
      "\n",
      "scenarios = [\n",
      "    {'data_type': 'continuous', 'needs_significance': True, 'industry': 'finance', 'high_sensitivity': False},\n",
      "    {'data_type': 'continuous', 'needs_significance': False, 'industry': 'general', 'high_sensitivity': True},\n",
      "    {'data_type': 'categorical', 'needs_significance': True, 'industry': 'healthcare', 'high_sensitivity': False},\n",
      "]\n",
      "\n",
      "for i, scenario in enumerate(scenarios, 1):\n",
      "    print(f\"\\nüìã Scenario {i}:\")\n",
      "    print(f\"   Data Type: {scenario['data_type']}\")\n",
      "    print(f\"   Needs Significance: {scenario['needs_significance']}\")\n",
      "    print(f\"   Industry: {scenario['industry']}\")\n",
      "    print(f\"   High Sensitivity: {scenario['high_sensitivity']}\")\n",
      "    \n",
      "    recs = recommend_drift_method(**scenario)\n",
      "    print(\"   Recommendations:\")\n",
      "    for j, rec in enumerate(recs, 1):\n",
      "        print(f\"   {j}. {rec['method']} - {rec['reason']}\")\n",
      "\n",
      "#%% [markdown]\n",
      "# ## ‡∏™‡∏£‡∏∏‡∏õ LAB 1\n",
      "# \n",
      "# ### ‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ:\n",
      "# 1. **Covariate Shift**: P(X) ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô ‡πÅ‡∏ï‡πà P(Y|X) ‡∏Ñ‡∏á‡∏ó‡∏µ‡πà\n",
      "# 2. **Concept Drift**: P(Y|X) ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô (relationship ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô)\n",
      "# 3. **KS Test**: ‡∏ß‡∏±‡∏î maximum distance ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á CDFs, ‡πÉ‡∏´‡πâ p-value\n",
      "# 4. **PSI**: ‡∏ß‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡∏≠‡∏á proportions, ‡∏°‡∏µ threshold ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô\n",
      "# 5. **Wasserstein**: ‡∏ß‡∏±‡∏î \"‡∏á‡∏≤‡∏ô\" ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô distribution\n",
      "# \n",
      "# ### Best Practices:\n",
      "# - ‡πÉ‡∏ä‡πâ‡∏´‡∏•‡∏≤‡∏¢ methods ‡πÄ‡∏û‡∏∑‡πà‡∏≠ cross-validate\n",
      "# - ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å method ‡∏ï‡∏≤‡∏° data type ‡πÅ‡∏•‡∏∞ business requirements\n",
      "# - ‡∏ï‡∏±‡πâ‡∏á threshold ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡∏Å‡∏±‡∏ö context\n",
      "\n",
      "#%%\n",
      "print(\"=\" * 60)\n",
      "print(\"‚úÖ LAB 1 COMPLETED!\")\n",
      "print(\"=\" * 60)\n",
      "print(\"\"\"\n",
      "üìö Key Takeaways:\n",
      "1. Covariate Shift vs Concept Drift - ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á\n",
      "2. KS Test - ‡πÉ‡∏ä‡πâ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ statistical significance\n",
      "3. PSI - Industry standard ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö finance\n",
      "4. Wasserstein - Sensitive ‡∏ï‡πà‡∏≠ location shifts\n",
      "\n",
      "üîú Next: LAB 2 - Feature Drift Detection\n",
      "\"\"\")\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "## LAB 2: Feature Drift Detection\n",
      "\n",
      "```python\n",
      "#%% [markdown]\n",
      "# # LAB 2: Feature Drift Detection\n",
      "# ## ‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö Drift ‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞ Feature\n",
      "# \n",
      "# ### ‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ:\n",
      "# 1. ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö drift ‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞ feature ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏∞‡∏ö‡∏ö\n",
      "# 2. ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå numerical vs categorical feature drift\n",
      "# 3. ‡∏™‡∏£‡πâ‡∏≤‡∏á visualization ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö feature distributions over time\n",
      "#\n",
      "# ### ‡∏ó‡∏§‡∏©‡∏é‡∏µ:\n",
      "# ‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö drift ‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞ feature ‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡πÄ‡∏û‡∏£‡∏≤‡∏∞:\n",
      "# - ‡∏ä‡πà‡∏ß‡∏¢‡∏£‡∏∞‡∏ö‡∏∏ root cause ‡∏Ç‡∏≠‡∏á model performance degradation\n",
      "# - ‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏ß‡πà‡∏≤ feature ‡πÑ‡∏´‡∏ô‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î\n",
      "# - ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ prioritize ‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÑ‡∏î‡πâ\n",
      "\n",
      "#%% [markdown]\n",
      "# ## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 1: ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° Environment\n",
      "\n",
      "#%%\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from scipy import stats\n",
      "from sklearn.datasets import make_classification\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "import warnings\n",
      "warnings.filterwarnings('ignore')\n",
      "\n",
      "np.random.seed(42)\n",
      "plt.rcParams['figure.figsize'] = (14, 8)\n",
      "\n",
      "print(\"‚úÖ Libraries imported successfully!\")\n",
      "\n",
      "#%% [markdown]\n",
      "# ## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 2: ‡∏™‡∏£‡πâ‡∏≤‡∏á Dataset ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏´‡∏•‡∏≤‡∏¢ Features\n",
      "# \n",
      "# ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á dataset ‡∏ó‡∏µ‡πà‡∏à‡∏≥‡∏•‡∏≠‡∏á‡∏™‡∏ñ‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏ì‡πå‡∏à‡∏£‡∏¥‡∏á:\n",
      "# - ‡∏°‡∏µ‡∏ó‡∏±‡πâ‡∏á numerical ‡πÅ‡∏•‡∏∞ categorical features\n",
      "# - ‡∏ö‡∏≤‡∏á features ‡∏°‡∏µ drift ‡∏ö‡∏≤‡∏á features ‡πÑ‡∏°‡πà‡∏°‡∏µ\n",
      "# - ‡∏°‡∏µ drift ‡πÉ‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ï‡πà‡∏≤‡∏á‡πÜ\n",
      "\n",
      "#%%\n",
      "def create_multi_feature_dataset(n_samples=2000, drift_level='mixed'):\n",
      "    \"\"\"\n",
      "    ‡∏™‡∏£‡πâ‡∏≤‡∏á dataset ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏´‡∏•‡∏≤‡∏¢ features ‡∏û‡∏£‡πâ‡∏≠‡∏° simulated drift\n",
      "    \n",
      "    Parameters:\n",
      "    -----------\n",
      "    n_samples : int - ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô samples ‡∏ï‡πà‡∏≠ dataset\n",
      "    drift_level : str - 'none', 'mild', 'severe', 'mixed'\n",
      "    \n",
      "    Returns:\n",
      "    --------\n",
      "    tuple : (reference_df, current_df, feature_info)\n",
      "    \"\"\"\n",
      "    \n",
      "    np.random.seed(42)\n",
      "    \n",
      "    # === Reference Data (Training) ===\n",
      "    reference_data = {\n",
      "        # Numerical Features\n",
      "        'age': np.random.normal(35, 10, n_samples),\n",
      "        'income': np.random.normal(50000, 15000, n_samples),\n",
      "        'credit_score': np.random.normal(650, 80, n_samples),\n",
      "        'account_balance': np.random.exponential(10000, n_samples),\n",
      "        'transaction_count': np.random.poisson(20, n_samples),\n",
      "        \n",
      "        # Categorical Features (encoded as integers)\n",
      "        'region': np.random.choice([0, 1, 2, 3], n_samples, p=[0.3, 0.3, 0.25, 0.15]),\n",
      "        'customer_type': np.random.choice([0, 1, 2], n_samples, p=[0.5, 0.35, 0.15]),\n",
      "        'product_category': np.random.choice([0, 1, 2, 3, 4], n_samples, p=[0.2, 0.2, 0.2, 0.2, 0.2])\n",
      "    }\n",
      "    \n",
      "    # === Current Data (Production) with various drift levels ===\n",
      "    if drift_level == 'none':\n",
      "        current_data = {k: v.copy() for k, v in reference_data.items()}\n",
      "        # ‡πÄ‡∏û‡∏¥‡πà‡∏° random noise ‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢\n",
      "        for key in ['age', 'income', 'credit_score']:\n",
      "            current_data[key] = np.random.normal(\n",
      "                reference_data[key].mean(),\n",
      "                reference_data[key].std(),\n",
      "                n_samples\n",
      "            )\n",
      "    \n",
      "    elif drift_level == 'mixed':\n",
      "        current_data = {\n",
      "            # No drift\n",
      "            'age': np.random.normal(35, 10, n_samples),  # ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°\n",
      "            'credit_score': np.random.normal(650, 80, n_samples),  # ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°\n",
      "            \n",
      "            # Mild drift\n",
      "            'income': np.random.normal(55000, 15000, n_samples),  # mean shift ‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢\n",
      "            'transaction_count': np.random.poisson(25, n_samples),  # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡∏∂‡πâ‡∏ô‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢\n",
      "            \n",
      "            # Severe drift\n",
      "            'account_balance': np.random.exponential(20000, n_samples),  # scale ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏°‡∏≤‡∏Å\n",
      "            \n",
      "            # Categorical drift\n",
      "            'region': np.random.choice([0, 1, 2, 3], n_samples, p=[0.15, 0.15, 0.35, 0.35]),  # distribution ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô\n",
      "            'customer_type': np.random.choice([0, 1, 2], n_samples, p=[0.5, 0.35, 0.15]),  # ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°\n",
      "            'product_category': np.random.choice([0, 1, 2, 3, 4], n_samples, p=[0.4, 0.1, 0.1, 0.2, 0.2])  # ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏ö‡πâ‡∏≤‡∏á\n",
      "        }\n",
      "    \n",
      "    reference_df = pd.DataFrame(reference_data)\n",
      "    current_df = pd.DataFrame(current_data)\n",
      "    \n",
      "    # Feature info\n",
      "    feature_info = {\n",
      "        'numerical': ['age', 'income', 'credit_score', 'account_balance', 'transaction_count'],\n",
      "        'categorical': ['region', 'customer_type', 'product_category'],\n",
      "        'expected_drift': {\n",
      "            'age': 'none',\n",
      "            'income': 'mild',\n",
      "            'credit_score': 'none',\n",
      "            'account_balance': 'severe',\n",
      "            'transaction_count': 'mild',\n",
      "            'region': 'severe',\n",
      "            'customer_type': 'none',\n",
      "            'product_category': 'mild'\n",
      "        }\n",
      "    }\n",
      "    \n",
      "    return reference_df, current_df, feature_info\n",
      "\n",
      "# ‡∏™‡∏£‡πâ‡∏≤‡∏á dataset\n",
      "reference_df, current_df, feature_info = create_multi_feature_dataset(drift_level='mixed')\n",
      "\n",
      "print(\"üìä Reference Data Shape:\", reference_df.shape)\n",
      "print(\"üìä Current Data Shape:\", current_df.shape)\n",
      "print(\"\\nüìã Feature Types:\")\n",
      "print(f\"  Numerical: {feature_info['numerical']}\")\n",
      "print(f\"  Categorical: {feature_info['categorical']}\")\n",
      "\n",
      "print(\"\\nüìã Expected Drift Levels:\")\n",
      "for feature, level in feature_info['expected_drift'].items():\n",
      "    print(f\"  {feature}: {level}\")\n",
      "\n",
      "#%%\n",
      "# ‡πÅ‡∏™‡∏î‡∏á summary statistics\n",
      "print(\"\\n\" + \"=\" * 60)\n",
      "print(\"REFERENCE DATA SUMMARY\")\n",
      "print(\"=\" * 60)\n",
      "print(reference_df.describe())\n",
      "\n",
      "print(\"\\n\" + \"=\" * 60)\n",
      "print(\"CURRENT DATA SUMMARY\")\n",
      "print(\"=\" * 60)\n",
      "print(current_df.describe())\n",
      "\n",
      "#%% [markdown]\n",
      "# ## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 3: ‡∏™‡∏£‡πâ‡∏≤‡∏á Feature Drift Detector Class\n",
      "# \n",
      "# ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á class ‡∏ó‡∏µ‡πà‡∏£‡∏ß‡∏°‡∏ó‡∏∏‡∏Å drift detection methods ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡∏™‡∏∞‡∏î‡∏ß‡∏Å\n",
      "\n",
      "#%%\n",
      "class FeatureDriftDetector:\n",
      "    \"\"\"\n",
      "    Class ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö drift ‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞ feature\n",
      "    \n",
      "    ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á numerical ‡πÅ‡∏•‡∏∞ categorical features\n",
      "    ‡πÉ‡∏ä‡πâ‡∏´‡∏•‡∏≤‡∏¢ statistical tests ‡πÄ‡∏û‡∏∑‡πà‡∏≠ comprehensive analysis\n",
      "    \"\"\"\n",
      "    \n",
      "    def __init__(self, reference_data, current_data, numerical_features=None, categorical_features=None):\n",
      "        \"\"\"\n",
      "        Initialize detector\n",
      "        \n",
      "        Parameters:\n",
      "        -----------\n",
      "        reference_data : pd.DataFrame - ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• reference (training)\n",
      "        current_data : pd.DataFrame - ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• current (production)\n",
      "        numerical_features : list - ‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠ numerical features\n",
      "        categorical_features : list - ‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠ categorical features\n",
      "        \"\"\"\n",
      "        self.reference = reference_data\n",
      "        self.current = current_data\n",
      "        \n",
      "        # Auto-detect feature types if not provided\n",
      "        if numerical_features is None:\n",
      "            self.numerical_features = reference_data.select_dtypes(include=[np.number]).columns.tolist()\n",
      "        else:\n",
      "            self.numerical_features = numerical_features\n",
      "            \n",
      "        if categorical_features is None:\n",
      "            self.categorical_features = []\n",
      "        else:\n",
      "            self.categorical_features = categorical_features\n",
      "        \n",
      "        self.results = {}\n",
      "    \n",
      "    def ks_test(self, feature):\n",
      "        \"\"\"Kolmogorov-Smirnov test ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö numerical features\"\"\"\n",
      "        stat, p_value = stats.ks_2samp(\n",
      "            self.reference[feature].dropna(),\n",
      "            self.current[feature].dropna()\n",
      "        )\n",
      "        return {'statistic': stat, 'p_value': p_value}\n",
      "    \n",
      "    def calculate_psi(self, feature, bins=10):\n",
      "        \"\"\"Population Stability Index\"\"\"\n",
      "        ref_data = self.reference[feature].dropna()\n",
      "        cur_data = self.current[feature].dropna()\n",
      "        \n",
      "        # ‡∏™‡∏£‡πâ‡∏≤‡∏á bins\n",
      "        breakpoints = np.percentile(ref_data, np.linspace(0, 100, bins + 1))\n",
      "        breakpoints = np.unique(breakpoints)\n",
      "        \n",
      "        ref_counts, _ = np.histogram(ref_data, bins=breakpoints)\n",
      "        cur_counts, _ = np.histogram(cur_data, bins=breakpoints)\n",
      "        \n",
      "        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì proportions\n",
      "        eps = 1e-6\n",
      "        ref_props = ref_counts / len(ref_data) + eps\n",
      "        cur_props = cur_counts / len(cur_data) + eps\n",
      "        \n",
      "        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì PSI\n",
      "        psi = np.sum((cur_props - ref_props) * np.log(cur_props / ref_props))\n",
      "        \n",
      "        return {'psi': psi}\n",
      "    \n",
      "    def wasserstein_test(self, feature):\n",
      "        \"\"\"Wasserstein Distance\"\"\"\n",
      "        ref_data = self.reference[feature].dropna()\n",
      "        cur_data = self.current[feature].dropna()\n",
      "        \n",
      "        distance = stats.wasserstein_distance(ref_data, cur_data)\n",
      "        \n",
      "        # Normalize by std of reference\n",
      "        std = ref_data.std()\n",
      "        normalized = distance / std if std > 0 else distance\n",
      "        \n",
      "        return {'distance': distance, 'normalized_distance': normalized}\n",
      "    \n",
      "    def chi_squared_test(self, feature):\n",
      "        \"\"\"Chi-squared test ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö categorical features\"\"\"\n",
      "        # ‡∏ô‡∏±‡∏ö frequency ‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞ category\n",
      "        ref_counts = self.reference[feature].value_counts()\n",
      "        cur_counts = self.current[feature].value_counts()\n",
      "        \n",
      "        # ‡∏£‡∏ß‡∏° categories ‡πÉ‡∏´‡πâ‡πÄ‡∏ó‡πà‡∏≤‡∏Å‡∏±‡∏ô\n",
      "        all_categories = set(ref_counts.index) | set(cur_counts.index)\n",
      "        ref_freq = [ref_counts.get(cat, 0) for cat in all_categories]\n",
      "        cur_freq = [cur_counts.get(cat, 0) for cat in all_categories]\n",
      "        \n",
      "        # Normalize to expected frequencies\n",
      "        total_ref = sum(ref_freq)\n",
      "        total_cur = sum(cur_freq)\n",
      "        expected = [(r / total_ref) * total_cur for r in ref_freq]\n",
      "        \n",
      "        # Chi-squared test\n",
      "        try:\n",
      "            stat, p_value = stats.chisquare(cur_freq, expected)\n",
      "        except:\n",
      "            stat, p_value = 0, 1.0\n",
      "        \n",
      "        return {'statistic': stat, 'p_value': p_value}\n",
      "    \n",
      "    def analyze_numerical_feature(self, feature):\n",
      "        \"\"\"‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå drift ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö numerical feature\"\"\"\n",
      "        results = {\n",
      "            'feature': feature,\n",
      "            'type': 'numerical',\n",
      "            'ks_test': self.ks_test(feature),\n",
      "            'psi': self.calculate_psi(feature),\n",
      "            'wasserstein': self.wasserstein_test(feature)\n",
      "        }\n",
      "        \n",
      "        # ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•\n",
      "        ks_drift = results['ks_test']['p_value'] < 0.05\n",
      "        psi_value = results['psi']['psi']\n",
      "        \n",
      "        if psi_value < 0.1:\n",
      "            psi_severity = 'none'\n",
      "        elif psi_value < 0.25:\n",
      "            psi_severity = 'mild'\n",
      "        else:\n",
      "            psi_severity = 'severe'\n",
      "        \n",
      "        results['drift_detected'] = ks_drift or psi_severity != 'none'\n",
      "        results['severity'] = psi_severity\n",
      "        \n",
      "        return results\n",
      "    \n",
      "    def analyze_categorical_feature(self, feature):\n",
      "        \"\"\"‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå drift ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö categorical feature\"\"\"\n",
      "        chi2_result = self.chi_squared_test(feature)\n",
      "        \n",
      "        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì PSI ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö categorical\n",
      "        ref_props = self.reference[feature].value_counts(normalize=True)\n",
      "        cur_props = self.current[feature].value_counts(normalize=True)\n",
      "        \n",
      "        all_cats = set(ref_props.index) | set(cur_props.index)\n",
      "        eps = 1e-6\n",
      "        psi = 0\n",
      "        for cat in all_cats:\n",
      "            ref_p = ref_props.get(cat, 0) + eps\n",
      "            cur_p = cur_props.get(cat, 0) + eps\n",
      "            psi += (cur_p - ref_p) * np.log(cur_p / ref_p)\n",
      "        \n",
      "        results = {\n",
      "            'feature': feature,\n",
      "            'type': 'categorical',\n",
      "            'chi_squared': chi2_result,\n",
      "            'psi': {'psi': psi}\n",
      "        }\n",
      "        \n",
      "        # ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•\n",
      "        chi2_drift = chi2_result['p_value'] < 0.05\n",
      "        \n",
      "        if psi < 0.1:\n",
      "            psi_severity = 'none'\n",
      "        elif psi < 0.25:\n",
      "            psi_severity = 'mild'\n",
      "        else:\n",
      "            psi_severity = 'severe'\n",
      "        \n",
      "        results['drift_detected'] = chi2_drift or psi_severity != 'none'\n",
      "        results['severity'] = psi_severity\n",
      "        \n",
      "        return results\n",
      "    \n",
      "    def analyze_all_features(self):\n",
      "        \"\"\"‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå drift ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏∏‡∏Å features\"\"\"\n",
      "        all_results = {}\n",
      "        \n",
      "        # Numerical features\n",
      "        for feature in self.numerical_features:\n",
      "            all_results[feature] = self.analyze_numerical_feature(feature)\n",
      "        \n",
      "        # Categorical features\n",
      "        for feature in self.categorical_features:\n",
      "            all_results[feature] = self.analyze_categorical_feature(feature)\n",
      "        \n",
      "        self.results = all_results\n",
      "        return all_results\n",
      "    \n",
      "    def get_summary_report(self):\n",
      "        \"\"\"‡∏™‡∏£‡πâ‡∏≤‡∏á summary report\"\"\"\n",
      "        if not self.results:\n",
      "            self.analyze_all_features()\n",
      "        \n",
      "        summary = []\n",
      "        for feature, result in self.results.items():\n",
      "            row = {\n",
      "                'Feature': feature,\n",
      "                'Type': result['type'],\n",
      "                'Drift Detected': '‚úì' if result['drift_detected'] else '‚úó',\n",
      "                'Severity': result['severity'].upper(),\n",
      "                'PSI': f\"{result['psi']['psi']:.4f}\"\n",
      "            }\n",
      "            \n",
      "            if result['type'] == 'numerical':\n",
      "                row['KS p-value'] = f\"{result['ks_test']['p_value']:.4f}\"\n",
      "            else:\n",
      "                row['Chi2 p-value'] = f\"{result['chi_squared']['p_value']:.4f}\"\n",
      "            \n",
      "            summary.append(row)\n",
      "        \n",
      "        return pd.DataFrame(summary)\n",
      "\n",
      "#%%\n",
      "# ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô FeatureDriftDetector\n",
      "detector = FeatureDriftDetector(\n",
      "    reference_data=reference_df,\n",
      "    current_data=current_df,\n",
      "    numerical_features=feature_info['numerical'],\n",
      "    categorical_features=feature_info['categorical']\n",
      ")\n",
      "\n",
      "# ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ó‡∏∏‡∏Å features\n",
      "all_results = detector.analyze_all_features()\n",
      "\n",
      "# ‡πÅ‡∏™‡∏î‡∏á summary report\n",
      "print(\"=\" * 80)\n",
      "print(\"üìä FEATURE DRIFT DETECTION REPORT\")\n",
      "print(\"=\" * 80)\n",
      "summary_df = detector.get_summary_report()\n",
      "print(summary_df.to_string(index=False))\n",
      "\n",
      "#%% [markdown]\n",
      "# ## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 4: Visualize Feature Distributions\n",
      "# \n",
      "# ‡∏Å‡∏≤‡∏£ visualize ‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏´‡πâ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à drift ‡πÑ‡∏î‡πâ‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô\n",
      "\n",
      "#%%\n",
      "def plot_numerical_feature_drift(reference, current, feature_name, ax=None):\n",
      "    \"\"\"\n",
      "    ‡∏™‡∏£‡πâ‡∏≤‡∏á visualization ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö numerical feature drift\n",
      "    \"\"\"\n",
      "    if ax is None:\n",
      "        fig, ax = plt.subplots(figsize=(10, 4))\n",
      "    \n",
      "    # Histogram\n",
      "    ax.hist(reference[feature_name], bins=30, alpha=0.5, label='Reference', color='blue', density=True)\n",
      "    ax.hist(current[feature_name], bins=30, alpha=0.5, label='Current', color='red', density=True)\n",
      "    \n",
      "    # Statistics\n",
      "    ref_mean = reference[feature_name].mean()\n",
      "    cur_mean = current[feature_name].mean()\n",
      "    \n",
      "    ax.axvline(ref_mean, color='blue', linestyle='--', linewidth=2)\n",
      "    ax.axvline(cur_mean, color='red', linestyle='--', linewidth=2)\n",
      "    \n",
      "    # Calculate PSI for title\n",
      "    detector_temp = FeatureDriftDetector(reference, current, [feature_name], [])\n",
      "    result = detector_temp.analyze_numerical_feature(feature_name)\n",
      "    \n",
      "    ax.set_title(f'{feature_name}\\nPSI: {result[\"psi\"][\"psi\"]:.4f} | Severity: {result[\"severity\"].upper()}')\n",
      "    ax.set_xlabel(feature_name)\n",
      "    ax.set_ylabel('Density')\n",
      "    ax.legend()\n",
      "    \n",
      "    return ax\n",
      "\n",
      "def plot_categorical_feature_drift(reference, current, feature_name, ax=None):\n",
      "    \"\"\"\n",
      "    ‡∏™‡∏£‡πâ‡∏≤‡∏á visualization ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö categorical feature drift\n",
      "    \"\"\"\n",
      "    if ax is None:\n",
      "        fig, ax = plt.subplots(figsize=(10, 4))\n",
      "    \n",
      "    # Calculate proportions\n",
      "    ref_props = reference[feature_name].value_counts(normalize=True).sort_index()\n",
      "    cur_props = current[feature_name].value_counts(normalize=True).sort_index()\n",
      "    \n",
      "    # Align categories\n",
      "    all_cats = sorted(set(ref_props.index) | set(cur_props.index))\n",
      "    ref_values = [ref_props.get(cat, 0) for cat in all_cats]\n",
      "    cur_values = [cur_props.get(cat, 0) for cat in all_cats]\n",
      "    \n",
      "    x = np.arange(len(all_cats))\n",
      "    width = 0.35\n",
      "    \n",
      "    ax.bar(x - width/2, ref_values, width, label='Reference', color='blue', alpha=0.7)\n",
      "    ax.bar(x + width/2, cur_values, width, label='Current', color='red', alpha=0.7)\n",
      "    \n",
      "    ax.set_xticks(x)\n",
      "    ax.set_xticklabels([f'Cat {c}' for c in all_cats])\n",
      "    \n",
      "    # Calculate PSI for title\n",
      "    detector_temp = FeatureDriftDetector(reference, current, [], [feature_name])\n",
      "    result = detector_temp.analyze_categorical_feature(feature_name)\n",
      "    \n",
      "    ax.set_title(f'{feature_name}\\nPSI: {result[\"psi\"][\"psi\"]:.4f} | Severity: {result[\"severity\"].upper()}')\n",
      "    ax.set_xlabel('Category')\n",
      "    ax.set_ylabel('Proportion')\n",
      "    ax.legend()\n",
      "    \n",
      "    return ax\n",
      "\n",
      "#%%\n",
      "# Plot all numerical features\n",
      "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
      "axes = axes.flatten()\n",
      "\n",
      "for idx, feature in enumerate(feature_info['numerical']):\n",
      "    plot_numerical_feature_drift(reference_df, current_df, feature, axes[idx])\n",
      "\n",
      "# Remove empty subplot\n",
      "if len(feature_info['numerical']) < 6:\n",
      "    axes[-1].set_visible(False)\n",
      "\n",
      "plt.suptitle('Numerical Features: Distribution Comparison\\n(Reference vs Current)', fontsize=14, fontweight='bold')\n",
      "plt.tight_layout()\n",
      "plt.savefig('numerical_features_drift.png', dpi=150, bbox_inches='tight')\n",
      "plt.show()\n",
      "\n",
      "#%%\n",
      "# Plot all categorical features\n",
      "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
      "\n",
      "for idx, feature in enumerate(feature_info['categorical']):\n",
      "    plot_categorical_feature_drift(reference_df, current_df, feature, axes[idx])\n",
      "\n",
      "plt.suptitle('Categorical Features: Distribution Comparison\\n(Reference vs Current)', fontsize=14, fontweight='bold')\n",
      "plt.tight_layout()\n",
      "plt.savefig('categorical_features_drift.png', dpi=150, bbox_inches='tight')\n",
      "plt.show()\n",
      "\n",
      "#%% [markdown]\n",
      "# ## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 5: Feature Drift Ranking ‡πÅ‡∏•‡∏∞ Prioritization\n",
      "# \n",
      "# ‡∏à‡∏±‡∏î‡∏•‡∏≥‡∏î‡∏±‡∏ö features ‡∏ï‡∏≤‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏∏‡∏ô‡πÅ‡∏£‡∏á‡∏Ç‡∏≠‡∏á drift\n",
      "\n",
      "#%%\n",
      "def rank_features_by_drift(detector):\n",
      "    \"\"\"\n",
      "    ‡∏à‡∏±‡∏î‡∏•‡∏≥‡∏î‡∏±‡∏ö features ‡∏ï‡∏≤‡∏° drift severity\n",
      "    \"\"\"\n",
      "    if not detector.results:\n",
      "        detector.analyze_all_features()\n",
      "    \n",
      "    rankings = []\n",
      "    for feature, result in detector.results.items():\n",
      "        rankings.append({\n",
      "            'feature': feature,\n",
      "            'psi': result['psi']['psi'],\n",
      "            'drift_detected': result['drift_detected'],\n",
      "            'severity': result['severity'],\n",
      "            'type': result['type']\n",
      "        })\n",
      "    \n",
      "    # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏ï‡∏≤‡∏° PSI (‡∏°‡∏≤‡∏Å‡πÑ‡∏õ‡∏ô‡πâ‡∏≠‡∏¢)\n",
      "    rankings.sort(key=lambda x: x['psi'], reverse=True)\n",
      "    \n",
      "    return rankings\n",
      "\n",
      "# ‡∏à‡∏±‡∏î‡∏•‡∏≥‡∏î‡∏±‡∏ö features\n",
      "rankings = rank_features_by_drift(detector)\n",
      "\n",
      "print(\"=\" * 60)\n",
      "print(\"üìä FEATURE DRIFT RANKING (Sorted by PSI)\")\n",
      "print(\"=\" * 60)\n",
      "\n",
      "for rank, item in enumerate(rankings, 1):\n",
      "    severity_emoji = {'none': 'üü¢', 'mild': 'üü°', 'severe': 'üî¥'}[item['severity']]\n",
      "    print(f\"{rank}. {item['feature']:<20} | PSI: {item['psi']:.4f} | {severity_emoji} {item['severity'].upper()}\")\n",
      "\n",
      "#%%\n",
      "# ‡∏™‡∏£‡πâ‡∏≤‡∏á Drift Ranking Visualization\n",
      "fig, ax = plt.subplots(figsize=(12, 6))\n",
      "\n",
      "features = [r['feature'] for r in rankings]\n",
      "psi_values = [r['psi'] for r in rankings]\n",
      "colors = ['red' if r['severity'] == 'severe' else 'orange' if r['severity'] == 'mild' else 'green' for r in rankings]\n",
      "\n",
      "bars = ax.barh(features, psi_values, color=colors, alpha=0.7, edgecolor='black')\n",
      "\n",
      "# Add threshold lines\n",
      "ax.axvline(0.1, color='orange', linestyle='--', label='Mild Threshold (0.1)')\n",
      "ax.axvline(0.25, color='red', linestyle='--', label='Severe Threshold (0.25)')\n",
      "\n",
      "ax.set_xlabel('PSI Value')\n",
      "ax.set_ylabel('Feature')\n",
      "ax.set_title('Feature Drift Ranking by PSI\\n(Red=Severe, Orange=Mild, Green=None)')\n",
      "ax.legend()\n",
      "\n",
      "# Add value labels\n",
      "for bar, psi in zip(bars, psi_values):\n",
      "    ax.text(psi + 0.01, bar.get_y() + bar.get_height()/2, f'{psi:.3f}', va='center')\n",
      "\n",
      "plt.tight_layout()\n",
      "plt.savefig('feature_drift_ranking.png', dpi=150, bbox_inches='tight')\n",
      "plt.show()\n",
      "\n",
      "#%% [markdown]\n",
      "# ## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 6: Time-based Distribution Analysis\n",
      "# \n",
      "# ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ß‡πà‡∏≤ distribution ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÄ‡∏ß‡∏•‡∏≤‡∏ú‡πà‡∏≤‡∏ô‡πÑ‡∏õ\n",
      "\n",
      "#%%\n",
      "def simulate_time_series_data(n_periods=6, samples_per_period=500):\n",
      "    \"\"\"\n",
      "    ‡∏à‡∏≥‡∏•‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏ï‡∏≤‡∏°‡πÄ‡∏ß‡∏•‡∏≤\n",
      "    \"\"\"\n",
      "    all_data = []\n",
      "    \n",
      "    for period in range(n_periods):\n",
      "        np.random.seed(42 + period)\n",
      "        \n",
      "        # Gradual drift: mean ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡∏∂‡πâ‡∏ô‡∏ó‡∏µ‡∏•‡∏∞‡∏ô‡πâ‡∏≠‡∏¢\n",
      "        age_mean = 35 + period * 2  # drift ‡πÉ‡∏ô age\n",
      "        income_mean = 50000  # ‡πÑ‡∏°‡πà‡∏°‡∏µ drift\n",
      "        \n",
      "        data = pd.DataFrame({\n",
      "            'period': period,\n",
      "            'age': np.random.normal(age_mean, 10, samples_per_period),\n",
      "            'income': np.random.normal(income_mean, 15000, samples_per_period),\n",
      "            'credit_score': np.random.normal(650, 80, samples_per_period)\n",
      "        })\n",
      "        all_data.append(data)\n",
      "    \n",
      "    return pd.concat(all_data, ignore_index=True)\n",
      "\n",
      "# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• time series\n",
      "time_series_data = simulate_time_series_data()\n",
      "print(f\"üìä Time Series Data Shape: {time_series_data.shape}\")\n",
      "print(f\"üìã Periods: {time_series_data['period'].unique()}\")\n",
      "\n",
      "#%%\n",
      "# Visualize distribution over time\n",
      "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
      "\n",
      "features_to_plot = ['age', 'income', 'credit_score']\n",
      "colors = plt.cm.viridis(np.linspace(0, 1, 6))\n",
      "\n",
      "for idx, feature in enumerate(features_to_plot):\n",
      "    ax = axes[idx]\n",
      "    \n",
      "    for period in range(6):\n",
      "        period_data = time_series_data[time_series_data['period'] == period][feature]\n",
      "        ax.hist(period_data, bins=20, alpha=0.3, color=colors[period], label=f'Period {period}')\n",
      "        ax.axvline(period_data.mean(), color=colors[period], linestyle='--', alpha=0.8)\n",
      "    \n",
      "    ax.set_title(f'{feature.capitalize()} Distribution Over Time')\n",
      "    ax.set_xlabel(feature)\n",
      "    ax.set_ylabel('Frequency')\n",
      "    ax.legend(fontsize=8)\n",
      "\n",
      "plt.suptitle('Feature Distributions Across Time Periods\\n(Dashed lines = Mean per period)', fontsize=12, fontweight='bold')\n",
      "plt.tight_layout()\n",
      "plt.savefig('time_series_distributions.png', dpi=150, bbox_inches='tight')\n",
      "plt.show()\n",
      "\n",
      "#%%\n",
      "# Calculate PSI over time (comparing each period to Period 0)\n",
      "def calculate_psi_over_time(data, feature, reference_period=0):\n",
      "    \"\"\"\n",
      "    ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì PSI ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Å‡∏±‡∏ö reference period\n",
      "    \"\"\"\n",
      "    ref_data = data[data['period'] == reference_period][feature]\n",
      "    \n",
      "    results = []\n",
      "    for period in data['period'].unique():\n",
      "        if period == reference_period:\n",
      "            results.append({'period': period, 'psi': 0})\n",
      "        else:\n",
      "            cur_data = data[data['period'] == period][feature]\n",
      "            \n",
      "            # Calculate PSI\n",
      "            breakpoints = np.percentile(ref_data, np.linspace(0, 100, 11))\n",
      "            breakpoints = np.unique(breakpoints)\n",
      "            \n",
      "            ref_counts, _ = np.histogram(ref_data, bins=breakpoints)\n",
      "            cur_counts, _ = np.histogram(cur_data, bins=breakpoints)\n",
      "            \n",
      "            eps = 1e-6\n",
      "            ref_props = ref_counts / len(ref_data) + eps\n",
      "            cur_props = cur_counts / len(cur_data) + eps\n",
      "            \n",
      "            psi = np.sum((cur_props - ref_props) * np.log(cur_props / ref_props))\n",
      "            results.append({'period': period, 'psi': psi})\n",
      "    \n",
      "    return pd.DataFrame(results)\n",
      "\n",
      "# ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì PSI ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞ feature\n",
      "fig, ax = plt.subplots(figsize=(10, 6))\n",
      "\n",
      "for feature in ['age', 'income', 'credit_score']:\n",
      "    psi_over_time = calculate_psi_over_time(time_series_data, feature)\n",
      "    ax.plot(psi_over_time['period'], psi_over_time['psi'], marker='o', label=feature, linewidth=2)\n",
      "\n",
      "ax.axhline(0.1, color='orange', linestyle='--', alpha=0.7, label='Mild Threshold')\n",
      "ax.axhline(0.25, color='red', linestyle='--', alpha=0.7, label='Severe Threshold')\n",
      "\n",
      "ax.set_xlabel('Time Period')\n",
      "ax.set_ylabel('PSI (compared to Period 0)')\n",
      "ax.set_title('PSI Trend Over Time\\n(Reference: Period 0)')\n",
      "ax.legend()\n",
      "ax.grid(True, alpha=0.3)\n",
      "\n",
      "plt.tight_layout()\n",
      "plt.savefig('psi_over_time.png', dpi=150, bbox_inches='tight')\n",
      "plt.show()\n",
      "\n",
      "print(\"\\nüí° ‡∏™‡∏±‡∏á‡πÄ‡∏Å‡∏ï: Age ‡πÅ‡∏™‡∏î‡∏á gradual drift (PSI ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡∏∂‡πâ‡∏ô‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏¢‡πÜ)\")\n",
      "print(\"   ‡πÉ‡∏ô‡∏Ç‡∏ì‡∏∞‡∏ó‡∏µ‡πà Income ‡πÅ‡∏•‡∏∞ Credit Score ‡∏Ñ‡πà‡∏≠‡∏ô‡∏Ç‡πâ‡∏≤‡∏á stable\")\n",
      "\n",
      "#%% [markdown]\n",
      "# ## ‡∏™‡∏£‡∏∏‡∏õ LAB 2\n",
      "# \n",
      "# ### ‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ:\n",
      "# 1. **FeatureDriftDetector Class**: ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏°‡∏∑‡∏≠‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå drift ‡∏ó‡∏∏‡∏Å features\n",
      "# 2. **Numerical vs Categorical**: ‡πÉ‡∏ä‡πâ methods ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡∏Å‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞ type\n",
      "# 3. **Visualization**: ‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏´‡πâ‡πÄ‡∏´‡πá‡∏ô drift ‡πÑ‡∏î‡πâ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô\n",
      "# 4. **Ranking**: ‡∏à‡∏±‡∏î‡∏•‡∏≥‡∏î‡∏±‡∏ö features ‡∏ï‡∏≤‡∏° severity ‡πÄ‡∏û‡∏∑‡πà‡∏≠ prioritization\n",
      "# 5. **Time-based Analysis**: ‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏° drift ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÄ‡∏ß‡∏•‡∏≤‡∏ú‡πà‡∏≤‡∏ô‡πÑ‡∏õ\n",
      "\n",
      "#%%\n",
      "print(\"=\" * 60)\n",
      "print(\"‚úÖ LAB 2 COMPLETED!\")\n",
      "print(\"=\" * 60)\n",
      "print(\"\"\"\n",
      "üìö Key Takeaways:\n",
      "1. Numerical features: ‡πÉ‡∏ä‡πâ KS Test + PSI + Wasserstein\n",
      "2. Categorical features: ‡πÉ‡∏ä‡πâ Chi-squared + PSI\n",
      "3. Visualization ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à drift\n",
      "4. Track PSI over time ‡πÄ‡∏û‡∏∑‡πà‡∏≠ detect gradual drift\n",
      "\n",
      "üîú Next: LAB 3 - Multivariate Drift Analysis\n",
      "\"\"\")\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "## LAB 3: Multivariate Drift Analysis\n",
      "\n",
      "```python\n",
      "#%% [markdown]\n",
      "# # LAB 3: Multivariate Drift Analysis\n",
      "# ## ‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå Drift ‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏¥‡∏î‡∏à‡∏≤‡∏Å‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á Features\n",
      "# \n",
      "# ### ‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ:\n",
      "# 1. ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö drift ‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏¥‡∏î‡∏à‡∏≤‡∏Å‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á features\n",
      "# 2. ‡πÉ‡∏ä‡πâ Dataset-level drift detection\n",
      "# 3. ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå Correlation changes ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á features\n",
      "#\n",
      "# ### ‡∏ó‡∏§‡∏©‡∏é‡∏µ:\n",
      "# **Multivariate Drift** ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡∏∂‡πâ‡∏ô‡πÄ‡∏°‡∏∑‡πà‡∏≠:\n",
      "# - ‡πÅ‡∏ï‡πà‡∏•‡∏∞ feature ‡∏î‡∏π‡∏õ‡∏Å‡∏ï‡∏¥‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÅ‡∏¢‡∏Å\n",
      "# - ‡πÅ‡∏ï‡πà‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á features ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÑ‡∏õ\n",
      "# - ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: correlation ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á age ‡πÅ‡∏•‡∏∞ income ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô\n",
      "\n",
      "#%% [markdown]\n",
      "# ## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 1: ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° Environment\n",
      "\n",
      "#%%\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from scipy import stats\n",
      "from sklearn.decomposition import PCA\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.covariance import EmpiricalCovariance\n",
      "import warnings\n",
      "warnings.filterwarnings('ignore')\n",
      "\n",
      "np.random.seed(42)\n",
      "plt.rcParams['figure.figsize'] = (12, 8)\n",
      "\n",
      "print(\"‚úÖ Libraries imported successfully!\")\n",
      "\n",
      "#%% [markdown]\n",
      "# ## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 2: ‡∏™‡∏£‡πâ‡∏≤‡∏á Dataset ‡∏ó‡∏µ‡πà‡∏°‡∏µ Multivariate Drift\n",
      "# \n",
      "# ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà:\n",
      "# - Marginal distributions ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Å‡∏±‡∏ô (‡πÑ‡∏°‡πà‡∏°‡∏µ univariate drift)\n",
      "# - ‡πÅ‡∏ï‡πà correlation structure ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô (multivariate drift)\n",
      "\n",
      "#%%\n",
      "def create_multivariate_drift_data(n_samples=2000):\n",
      "    \"\"\"\n",
      "    ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ multivariate drift\n",
      "    - Marginal distributions ‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢‡∏Å‡∏±‡∏ô\n",
      "    - Correlation structure ‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô\n",
      "    \"\"\"\n",
      "    np.random.seed(42)\n",
      "    \n",
      "    # === Reference Data ===\n",
      "    # Correlated features: age ‡πÅ‡∏•‡∏∞ income ‡∏°‡∏µ positive correlation ‡∏™‡∏π‡∏á\n",
      "    mean_ref = [35, 50000, 650]  # age, income, credit_score\n",
      "    cov_ref = [\n",
      "        [100, 3000, 50],      # age variance ‡πÅ‡∏•‡∏∞ covariance\n",
      "        [3000, 225000000, 100000],  # income variance ‡πÅ‡∏•‡∏∞ covariance\n",
      "        [50, 100000, 6400]    # credit_score variance ‡πÅ‡∏•‡∏∞ covariance\n",
      "    ]\n",
      "    \n",
      "    ref_data = np.random.multivariate_normal(mean_ref, cov_ref, n_samples)\n",
      "    reference_df = pd.DataFrame(ref_data, columns=['age', 'income', 'credit_score'])\n",
      "    \n",
      "    # === Current Data (Multivariate Drift) ===\n",
      "    # Same marginals but different correlation structure\n",
      "    mean_cur = [35, 50000, 650]  # means ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°\n",
      "    \n",
      "    # Correlation structure ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô: age ‡πÅ‡∏•‡∏∞ income correlation ‡∏•‡∏î‡∏•‡∏á\n",
      "    cov_cur = [\n",
      "        [100, 500, 50],       # correlation ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á age-income ‡∏•‡∏î‡∏•‡∏á‡∏°‡∏≤‡∏Å!\n",
      "        [500, 225000000, 100000],\n",
      "        [50, 100000, 6400]\n",
      "    ]\n",
      "    \n",
      "    cur_data = np.random.multivariate_normal(mean_cur, cov_cur, n_samples)\n",
      "    current_df = pd.DataFrame(cur_data, columns=['age', 'income', 'credit_score'])\n",
      "    \n",
      "    return reference_df, current_df\n",
      "\n",
      "# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•\n",
      "ref_multi, cur_multi = create_multivariate_drift_data()\n",
      "\n",
      "print(\"üìä Reference Data:\")\n",
      "print(ref_multi.describe())\n",
      "print(\"\\nüìä Current Data:\")\n",
      "print(cur_multi.describe())\n",
      "\n",
      "#%%\n",
      "# ‡πÅ‡∏™‡∏î‡∏á correlation matrix comparison\n",
      "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
      "\n",
      "# Reference correlation matrix\n",
      "corr_ref = ref_multi.corr()\n",
      "sns.heatmap(corr_ref, annot=True, cmap='coolwarm', center=0, ax=axes[0], \n",
      "            vmin=-1, vmax=1, fmt='.3f')\n",
      "axes[0].set_title('Reference Correlation Matrix')\n",
      "\n",
      "# Current correlation matrix\n",
      "corr_cur = cur_multi.corr()\n",
      "sns.heatmap(corr_cur, annot=True, cmap='coolwarm', center=0, ax=axes[1],\n",
      "            vmin=-1, vmax=1, fmt='.3f')\n",
      "axes[1].set_title('Current Correlation Matrix')\n",
      "\n",
      "# Difference\n",
      "corr_diff = corr_cur - corr_ref\n",
      "sns.heatmap(corr_diff, annot=True, cmap='RdBu', center=0, ax=axes[2],\n",
      "            vmin=-1, vmax=1, fmt='.3f')\n",
      "axes[2].set_title('Correlation Difference\\n(Current - Reference)')\n",
      "\n",
      "plt.tight_layout()\n",
      "plt.savefig('correlation_comparison.png', dpi=150, bbox_inches='tight')\n",
      "plt.show()\n",
      "\n",
      "print(\"\\nüí° ‡∏™‡∏±‡∏á‡πÄ‡∏Å‡∏ï: Correlation ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á Age-Income ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏à‡∏≤‡∏Å ~0.6 ‡πÄ‡∏õ‡πá‡∏ô ~0.03\")\n",
      "\n",
      "#%% [markdown]\n",
      "# ## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 3: Univariate vs Multivariate Drift Detection\n",
      "# \n",
      "# ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏ß‡πà‡∏≤ univariate methods ‡∏û‡∏•‡∏≤‡∏î‡∏≠‡∏∞‡πÑ‡∏£‡πÑ‡∏õ\n",
      "\n",
      "#%%\n",
      "from scipy import stats\n",
      "\n",
      "def univariate_drift_check(ref_df, cur_df):\n",
      "    \"\"\"‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö drift ‡∏î‡πâ‡∏ß‡∏¢ univariate methods\"\"\"\n",
      "    results = []\n",
      "    \n",
      "    for col in ref_df.columns:\n",
      "        # KS Test\n",
      "        ks_stat, ks_pval = stats.ks_2samp(ref_df[col], cur_df[col])\n",
      "        \n",
      "        # Mean comparison\n",
      "        ref_mean = ref_df[col].mean()\n",
      "        cur_mean = cur_df[col].mean()\n",
      "        mean_diff_pct = abs(cur_mean - ref_mean) / ref_mean * 100\n",
      "        \n",
      "        # Std comparison\n",
      "        ref_std = ref_df[col].std()\n",
      "        cur_std = cur_df[col].std()\n",
      "        std_diff_pct = abs(cur_std - ref_std) / ref_std * 100\n",
      "        \n",
      "        results.append({\n",
      "            'feature': col,\n",
      "            'ks_statistic': ks_stat,\n",
      "            'ks_pvalue': ks_pval,\n",
      "            'drift_detected': ks_pval < 0.05,\n",
      "            'mean_diff_%': mean_diff_pct,\n",
      "            'std_diff_%': std_diff_pct\n",
      "        })\n",
      "    \n",
      "    return pd.DataFrame(results)\n",
      "\n",
      "# ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö univariate drift\n",
      "univariate_results = univariate_drift_check(ref_multi, cur_multi)\n",
      "print(\"=\" * 60)\n",
      "print(\"üìä UNIVARIATE DRIFT DETECTION RESULTS\")\n",
      "print(\"=\" * 60)\n",
      "print(univariate_results.to_string(index=False))\n",
      "\n",
      "print(\"\\nüí° ‡∏™‡∏±‡∏á‡πÄ‡∏Å‡∏ï: Univariate methods ‡πÑ‡∏°‡πà‡∏û‡∏ö drift ‡∏ó‡∏µ‡πà‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç!\")\n",
      "print(\"   ‡πÅ‡∏ï‡πà‡πÄ‡∏£‡∏≤‡∏£‡∏π‡πâ‡∏ß‡πà‡∏≤ correlation structure ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÑ‡∏õ\")\n",
      "\n",
      "#%% [markdown]\n",
      "# ## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 4: Correlation-based Drift Detection\n",
      "# \n",
      "# ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡∏≠‡∏á correlation structure\n",
      "\n",
      "#%%\n",
      "def correlation_drift_test(ref_df, cur_df, significance_level=0.05):\n",
      "    \"\"\"\n",
      "    ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö drift ‡πÉ‡∏ô correlation structure\n",
      "    \n",
      "    ‡πÉ‡∏ä‡πâ Fisher's Z transformation ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö correlations\n",
      "    \"\"\"\n",
      "    results = []\n",
      "    \n",
      "    # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì correlation matrices\n",
      "    ref_corr = ref_df.corr()\n",
      "    cur_corr = cur_df.corr()\n",
      "    \n",
      "    n_ref = len(ref_df)\n",
      "    n_cur = len(cur_df)\n",
      "    \n",
      "    # ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞ pair\n",
      "    for i, col1 in enumerate(ref_df.columns):\n",
      "        for j, col2 in enumerate(ref_df.columns):\n",
      "            if i >= j:  # ‡∏Ç‡πâ‡∏≤‡∏° diagonal ‡πÅ‡∏•‡∏∞ lower triangle\n",
      "                continue\n",
      "            \n",
      "            r_ref = ref_corr.loc[col1, col2]\n",
      "            r_cur = cur_corr.loc[col1, col2]\n",
      "            \n",
      "            # Fisher's Z transformation\n",
      "            z_ref = np.arctanh(r_ref)\n",
      "            z_cur = np.arctanh(r_cur)\n",
      "            \n",
      "            # Standard error\n",
      "            se = np.sqrt(1/(n_ref-3) + 1/(n_cur-3))\n",
      "            \n",
      "            # Z-test statistic\n",
      "            z_stat = (z_ref - z_cur) / se\n",
      "            p_value = 2 * (1 - stats.norm.cdf(abs(z_stat)))\n",
      "            \n",
      "            results.append({\n",
      "                'feature_pair': f'{col1} - {col2}',\n",
      "                'ref_correlation': r_ref,\n",
      "                'cur_correlation': r_cur,\n",
      "                'correlation_change': r_cur - r_ref,\n",
      "                'z_statistic': z_stat,\n",
      "                'p_value': p_value,\n",
      "                'significant_change': p_value < significance_level\n",
      "            })\n",
      "    \n",
      "    return pd.DataFrame(results)\n",
      "\n",
      "# ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö correlation drift\n",
      "corr_drift_results = correlation_drift_test(ref_multi, cur_multi)\n",
      "print(\"=\" * 60)\n",
      "print(\"üìä CORRELATION DRIFT DETECTION RESULTS\")\n",
      "print(\"=\" * 60)\n",
      "print(corr_drift_results.to_string(index=False))\n",
      "\n",
      "#%%\n",
      "# Visualize correlation changes\n",
      "fig, ax = plt.subplots(figsize=(10, 6))\n",
      "\n",
      "x = range(len(corr_drift_results))\n",
      "colors = ['red' if sig else 'green' for sig in corr_drift_results['significant_change']]\n",
      "\n",
      "bars = ax.bar(x, corr_drift_results['correlation_change'], color=colors, alpha=0.7, edgecolor='black')\n",
      "\n",
      "ax.set_xticks(x)\n",
      "ax.set_xticklabels(corr_drift_results['feature_pair'], rotation=45, ha='right')\n",
      "ax.set_ylabel('Correlation Change')\n",
      "ax.set_title('Correlation Changes Between Reference and Current Data\\n(Red = Statistically Significant)')\n",
      "ax.axhline(0, color='black', linestyle='-', linewidth=0.5)\n",
      "\n",
      "# Add value labels\n",
      "for bar, val in zip(bars, corr_drift_results['correlation_change']):\n",
      "    height = bar.get_height()\n",
      "    ax.text(bar.get_x() + bar.get_width()/2., height, f'{val:.3f}',\n",
      "            ha='center', va='bottom' if height > 0 else 'top')\n",
      "\n",
      "plt.tight_layout()\n",
      "plt.savefig('correlation_drift.png', dpi=150, bbox_inches='tight')\n",
      "plt.show()\n",
      "\n",
      "#%% [markdown]\n",
      "# ## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 5: PCA-based Multivariate Drift Detection\n",
      "# \n",
      "# ‡πÉ‡∏ä‡πâ PCA ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö drift ‡πÉ‡∏ô multivariate structure\n",
      "\n",
      "#%%\n",
      "def pca_drift_detection(ref_df, cur_df, n_components=None):\n",
      "    \"\"\"\n",
      "    ‡πÉ‡∏ä‡πâ PCA ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö multivariate drift\n",
      "    \n",
      "    ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö:\n",
      "    1. Explained variance ratios\n",
      "    2. Principal component directions\n",
      "    3. Reconstruction errors\n",
      "    \"\"\"\n",
      "    if n_components is None:\n",
      "        n_components = min(len(ref_df.columns), 3)\n",
      "    \n",
      "    # Standardize data\n",
      "    scaler = StandardScaler()\n",
      "    ref_scaled = scaler.fit_transform(ref_df)\n",
      "    cur_scaled = scaler.transform(cur_df)\n",
      "    \n",
      "    # Fit PCA on reference data\n",
      "    pca_ref = PCA(n_components=n_components)\n",
      "    pca_ref.fit(ref_scaled)\n",
      "    \n",
      "    # Transform both datasets using reference PCA\n",
      "    ref_transformed = pca_ref.transform(ref_scaled)\n",
      "    cur_transformed = pca_ref.transform(cur_scaled)\n",
      "    \n",
      "    # 1. Compare explained variance\n",
      "    ref_explained_var = pca_ref.explained_variance_ratio_\n",
      "    \n",
      "    # Fit PCA on current data for comparison\n",
      "    pca_cur = PCA(n_components=n_components)\n",
      "    pca_cur.fit(cur_scaled)\n",
      "    cur_explained_var = pca_cur.explained_variance_ratio_\n",
      "    \n",
      "    # 2. Compare principal components (using cosine similarity)\n",
      "    component_similarities = []\n",
      "    for i in range(n_components):\n",
      "        cos_sim = np.dot(pca_ref.components_[i], pca_cur.components_[i])\n",
      "        cos_sim = abs(cos_sim)  # Absolute value ‡πÄ‡∏û‡∏£‡∏≤‡∏∞ sign ‡∏≠‡∏≤‡∏à flip\n",
      "        component_similarities.append(cos_sim)\n",
      "    \n",
      "    # 3. Reconstruction error on current data using reference PCA\n",
      "    ref_reconstructed = pca_ref.inverse_transform(ref_transformed)\n",
      "    cur_reconstructed = pca_ref.inverse_transform(cur_transformed)\n",
      "    \n",
      "    ref_recon_error = np.mean((ref_scaled - ref_reconstructed) ** 2)\n",
      "    cur_recon_error = np.mean((cur_scaled - cur_reconstructed) ** 2)\n",
      "    \n",
      "    results = {\n",
      "        'ref_explained_variance': ref_explained_var,\n",
      "        'cur_explained_variance': cur_explained_var,\n",
      "        'component_similarities': component_similarities,\n",
      "        'ref_reconstruction_error': ref_recon_error,\n",
      "        'cur_reconstruction_error': cur_recon_error,\n",
      "        'reconstruction_error_ratio': cur_recon_error / ref_recon_error if ref_recon_error > 0 else 1,\n",
      "        'ref_components': pca_ref.components_,\n",
      "        'cur_components': pca_cur.components_,\n",
      "        'pca_ref': pca_ref,\n",
      "        'ref_transformed': ref_transformed,\n",
      "        'cur_transformed': cur_transformed\n",
      "    }\n",
      "    \n",
      "    return results\n",
      "\n",
      "# ‡∏ó‡∏≥ PCA drift detection\n",
      "pca_results = pca_drift_detection(ref_multi, cur_multi)\n",
      "\n",
      "print(\"=\" * 60)\n",
      "print(\"üìä PCA-BASED DRIFT DETECTION RESULTS\")\n",
      "print(\"=\" * 60)\n",
      "\n",
      "print(\"\\n1. Explained Variance Comparison:\")\n",
      "for i in range(len(pca_results['ref_explained_variance'])):\n",
      "    ref_ev = pca_results['ref_explained_variance'][i]\n",
      "    cur_ev = pca_results['cur_explained_variance'][i]\n",
      "    print(f\"   PC{i+1}: Reference = {ref_ev:.4f}, Current = {cur_ev:.4f}, Diff = {cur_ev - ref_ev:.4f}\")\n",
      "\n",
      "print(\"\\n2. Component Similarities (1.0 = identical):\")\n",
      "for i, sim in enumerate(pca_results['component_similarities']):\n",
      "    status = \"‚úì Similar\" if sim > 0.9 else \"‚ö†Ô∏è Changed!\"\n",
      "    print(f\"   PC{i+1}: Cosine Similarity = {sim:.4f} {status}\")\n",
      "\n",
      "print(\"\\n3. Reconstruction Error:\")\n",
      "print(f\"   Reference: {pca_results['ref_reconstruction_error']:.6f}\")\n",
      "print(f\"   Current: {pca_results['cur_reconstruction_error']:.6f}\")\n",
      "print(f\"   Ratio: {pca_results['reconstruction_error_ratio']:.4f}x\")\n",
      "\n",
      "#%%\n",
      "# Visualize PCA results\n",
      "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
      "\n",
      "# Plot 1: Explained variance comparison\n",
      "ax1 = axes[0, 0]\n",
      "x = range(1, len(pca_results['ref_explained_variance']) + 1)\n",
      "width = 0.35\n",
      "ax1.bar([i - width/2 for i in x], pca_results['ref_explained_variance'], \n",
      "        width, label='Reference', color='blue', alpha=0.7)\n",
      "ax1.bar([i + width/2 for i in x], pca_results['cur_explained_variance'], \n",
      "        width, label='Current', color='red', alpha=0.7)\n",
      "ax1.set_xlabel('Principal Component')\n",
      "ax1.set_ylabel('Explained Variance Ratio')\n",
      "ax1.set_title('Explained Variance Comparison')\n",
      "ax1.legend()\n",
      "ax1.set_xticks(x)\n",
      "\n",
      "# Plot 2: Component similarities\n",
      "ax2 = axes[0, 1]\n",
      "colors = ['green' if s > 0.9 else 'red' for s in pca_results['component_similarities']]\n",
      "ax2.bar(x, pca_results['component_similarities'], color=colors, alpha=0.7, edgecolor='black')\n",
      "ax2.axhline(0.9, color='orange', linestyle='--', label='Similarity Threshold (0.9)')\n",
      "ax2.set_xlabel('Principal Component')\n",
      "ax2.set_ylabel('Cosine Similarity')\n",
      "ax2.set_title('Principal Component Similarity\\n(Reference vs Current)')\n",
      "ax2.set_xticks(x)\n",
      "ax2.legend()\n",
      "\n",
      "# Plot 3: Scatter plot in PC space (Reference)\n",
      "ax3 = axes[1, 0]\n",
      "ax3.scatter(pca_results['ref_transformed'][:, 0], pca_results['ref_transformed'][:, 1], \n",
      "            alpha=0.3, label='Reference', color='blue')\n",
      "ax3.scatter(pca_results['cur_transformed'][:, 0], pca_results['cur_transformed'][:, 1], \n",
      "            alpha=0.3, label='Current', color='red')\n",
      "ax3.set_xlabel('PC1')\n",
      "ax3.set_ylabel('PC2')\n",
      "ax3.set_title('Data in PCA Space\\n(Using Reference PCA)')\n",
      "ax3.legend()\n",
      "\n",
      "# Plot 4: Component loadings comparison\n",
      "ax4 = axes[1, 1]\n",
      "features = ref_multi.columns.tolist()\n",
      "x = np.arange(len(features))\n",
      "width = 0.35\n",
      "\n",
      "for i in range(2):  # ‡πÅ‡∏™‡∏î‡∏á PC1 ‡πÅ‡∏•‡∏∞ PC2\n",
      "    ref_loadings = pca_results['ref_components'][i]\n",
      "    cur_loadings = pca_results['cur_components'][i]\n",
      "    \n",
      "    ax4.barh([f'PC{i+1} Ref' for f in features] if i == 0 else [f'PC{i+1} Cur' for f in features], \n",
      "             ref_loadings if i == 0 else cur_loadings)\n",
      "\n",
      "ax4.set_xlabel('Loading')\n",
      "ax4.set_title('PC Loadings (First 2 Components)')\n",
      "\n",
      "plt.tight_layout()\n",
      "plt.savefig('pca_drift_analysis.png', dpi=150, bbox_inches='tight')\n",
      "plt.show()\n",
      "\n",
      "#%% [markdown]\n",
      "# ## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 6: Mahalanobis Distance for Dataset-level Drift\n",
      "# \n",
      "# ‡πÉ‡∏ä‡πâ Mahalanobis distance ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ß‡∏±‡∏î multivariate drift\n",
      "\n",
      "#%%\n",
      "def mahalanobis_drift_detection(ref_df, cur_df, threshold_percentile=95):\n",
      "    \"\"\"\n",
      "    ‡πÉ‡∏ä‡πâ Mahalanobis distance ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö multivariate drift\n",
      "    \n",
      "    ‡∏ß‡∏±‡∏î‡∏ß‡πà‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡∏°‡πà‡∏≠‡∏¢‡∏π‡πà‡∏´‡πà‡∏≤‡∏á‡∏à‡∏≤‡∏Å distribution ‡∏Ç‡∏≠‡∏á reference data ‡πÄ‡∏ó‡πà‡∏≤‡πÑ‡∏£\n",
      "    \"\"\"\n",
      "    # Standardize features\n",
      "    scaler = StandardScaler()\n",
      "    ref_scaled = scaler.fit_transform(ref_df)\n",
      "    cur_scaled = scaler.transform(cur_df)\n",
      "    \n",
      "    # Fit covariance on reference data\n",
      "    cov = EmpiricalCovariance().fit(ref_scaled)\n",
      "    \n",
      "    # Calculate Mahalanobis distances\n",
      "    ref_distances = cov.mahalanobis(ref_scaled)\n",
      "    cur_distances = cov.mahalanobis(cur_scaled)\n",
      "    \n",
      "    # Determine threshold from reference data\n",
      "    threshold = np.percentile(ref_distances, threshold_percentile)\n",
      "    \n",
      "    # Count outliers\n",
      "    ref_outliers = np.sum(ref_distances > threshold) / len(ref_distances) * 100\n",
      "    cur_outliers = np.sum(cur_distances > threshold) / len(cur_distances) * 100\n",
      "    \n",
      "    # Statistical comparison\n",
      "    ks_stat, ks_pval = stats.ks_2samp(ref_distances, cur_distances)\n",
      "    \n",
      "    results = {\n",
      "        'ref_distances': ref_distances,\n",
      "        'cur_distances': cur_distances,\n",
      "        'threshold': threshold,\n",
      "        'ref_mean_distance': np.mean(ref_distances),\n",
      "        'cur_mean_distance': np.mean(cur_distances),\n",
      "        'ref_outlier_pct': ref_outliers,\n",
      "        'cur_outlier_pct': cur_outliers,\n",
      "        'ks_statistic': ks_stat,\n",
      "        'ks_pvalue': ks_pval,\n",
      "        'drift_detected': ks_pval < 0.05\n",
      "    }\n",
      "    \n",
      "    return results\n",
      "\n",
      "# ‡∏ó‡∏≥ Mahalanobis drift detection\n",
      "maha_results = mahalanobis_drift_detection(ref_multi, cur_multi)\n",
      "\n",
      "print(\"=\" * 60)\n",
      "print(\"üìä MAHALANOBIS DISTANCE DRIFT DETECTION\")\n",
      "print(\"=\" * 60)\n",
      "print(f\"\\nMean Mahalanobis Distance:\")\n",
      "print(f\"  Reference: {maha_results['ref_mean_distance']:.4f}\")\n",
      "print(f\"  Current: {maha_results['cur_mean_distance']:.4f}\")\n",
      "print(f\"\\nOutlier Percentage (beyond {maha_results['threshold']:.2f} threshold):\")\n",
      "print(f\"  Reference: {maha_results['ref_outlier_pct']:.2f}%\")\n",
      "print(f\"  Current: {maha_results['cur_outlier_pct']:.2f}%\")\n",
      "print(f\"\\nKS Test on Distances:\")\n",
      "print(f\"  Statistic: {maha_results['ks_statistic']:.4f}\")\n",
      "print(f\"  P-value: {maha_results['ks_pvalue']:.4f}\")\n",
      "print(f\"  Drift Detected: {'Yes ‚úì' if maha_results['drift_detected'] else 'No'}\")\n",
      "\n",
      "#%%\n",
      "# Visualize Mahalanobis distances\n",
      "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
      "\n",
      "# Plot 1: Distribution of Mahalanobis distances\n",
      "ax1 = axes[0]\n",
      "ax1.hist(maha_results['ref_distances'], bins=50, alpha=0.5, label='Reference', color='blue', density=True)\n",
      "ax1.hist(maha_results['cur_distances'], bins=50, alpha=0.5, label='Current', color='red', density=True)\n",
      "ax1.axvline(maha_results['threshold'], color='orange', linestyle='--', linewidth=2, \n",
      "            label=f'95th percentile threshold ({maha_results[\"threshold\"]:.2f})')\n",
      "ax1.set_xlabel('Mahalanobis Distance')\n",
      "ax1.set_ylabel('Density')\n",
      "ax1.set_title('Distribution of Mahalanobis Distances')\n",
      "ax1.legend()\n",
      "\n",
      "# Plot 2: CDF comparison\n",
      "ax2 = axes[1]\n",
      "sorted_ref = np.sort(maha_results['ref_distances'])\n",
      "sorted_cur = np.sort(maha_results['cur_distances'])\n",
      "\n",
      "ax2.plot(sorted_ref, np.arange(1, len(sorted_ref)+1)/len(sorted_ref), \n",
      "         'b-', label='Reference CDF', linewidth=2)\n",
      "ax2.plot(sorted_cur, np.arange(1, len(sorted_cur)+1)/len(sorted_cur), \n",
      "         'r-', label='Current CDF', linewidth=2)\n",
      "ax2.axvline(maha_results['threshold'], color='orange', linestyle='--', linewidth=2)\n",
      "ax2.set_xlabel('Mahalanobis Distance')\n",
      "ax2.set_ylabel('Cumulative Probability')\n",
      "ax2.set_title(f'CDF Comparison\\nKS Statistic = {maha_results[\"ks_statistic\"]:.4f}')\n",
      "ax2.legend()\n",
      "\n",
      "plt.tight_layout()\n",
      "plt.savefig('mahalanobis_drift.png', dpi=150, bbox_inches='tight')\n",
      "plt.show()\n",
      "\n",
      "#%% [markdown]\n",
      "# ## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 7: Comprehensive Multivariate Drift Report\n",
      "\n",
      "#%%\n",
      "class MultivariateriftDetector:\n",
      "    \"\"\"\n",
      "    Class ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö comprehensive multivariate drift detection\n",
      "    \"\"\"\n",
      "    \n",
      "    def __init__(self, reference_df, current_df):\n",
      "        self.reference = reference_df\n",
      "        self.current = current_df\n",
      "        self.results = {}\n",
      "    \n",
      "    def analyze(self):\n",
      "        \"\"\"‡∏ó‡∏≥ comprehensive analysis\"\"\"\n",
      "        \n",
      "        # 1. Correlation analysis\n",
      "        corr_results = correlation_drift_test(self.reference, self.current)\n",
      "        self.results['correlation'] = corr_results\n",
      "        \n",
      "        # 2. PCA analysis\n",
      "        pca_results = pca_drift_detection(self.reference, self.current)\n",
      "        self.results['pca'] = pca_results\n",
      "        \n",
      "        # 3. Mahalanobis analysis\n",
      "        maha_results = mahalanobis_drift_detection(self.reference, self.current)\n",
      "        self.results['mahalanobis'] = maha_results\n",
      "        \n",
      "        # 4. Overall assessment\n",
      "        drift_indicators = {\n",
      "            'correlation_drift': any(corr_results['significant_change']),\n",
      "            'pca_structure_change': any(sim < 0.9 for sim in pca_results['component_similarities']),\n",
      "            'mahalanobis_drift': maha_results['drift_detected']\n",
      "        }\n",
      "        \n",
      "        drift_count = sum(drift_indicators.values())\n",
      "        \n",
      "        self.results['summary'] = {\n",
      "            'drift_indicators': drift_indicators,\n",
      "            'drift_count': drift_count,\n",
      "            'overall_assessment': 'HIGH' if drift_count >= 2 else 'MEDIUM' if drift_count == 1 else 'LOW'\n",
      "        }\n",
      "        \n",
      "        return self.results\n",
      "    \n",
      "    def print_report(self):\n",
      "        \"\"\"‡∏û‡∏¥‡∏°‡∏û‡πå report ‡∏™‡∏£‡∏∏‡∏õ\"\"\"\n",
      "        if not self.results:\n",
      "            self.analyze()\n",
      "        \n",
      "        print(\"=\" * 70)\n",
      "        print(\"üìä COMPREHENSIVE MULTIVARIATE DRIFT REPORT\")\n",
      "        print(\"=\" * 70)\n",
      "        \n",
      "        print(\"\\n1Ô∏è‚É£ CORRELATION DRIFT:\")\n",
      "        print(\"-\" * 40)\n",
      "        corr_df = self.results['correlation']\n",
      "        significant_pairs = corr_df[corr_df['significant_change']]\n",
      "        if len(significant_pairs) > 0:\n",
      "            print(\"   ‚ö†Ô∏è Significant correlation changes detected:\")\n",
      "            for _, row in significant_pairs.iterrows():\n",
      "                print(f\"      {row['feature_pair']}: {row['ref_correlation']:.3f} ‚Üí {row['cur_correlation']:.3f}\")\n",
      "        else:\n",
      "            print(\"   ‚úì No significant correlation changes\")\n",
      "        \n",
      "        print(\"\\n2Ô∏è‚É£ PCA STRUCTURE ANALYSIS:\")\n",
      "        print(\"-\" * 40)\n",
      "        pca_res = self.results['pca']\n",
      "        for i, sim in enumerate(pca_res['component_similarities']):\n",
      "            status = \"‚úì\" if sim > 0.9 else \"‚ö†Ô∏è Changed\"\n",
      "            print(f\"   PC{i+1} similarity: {sim:.4f} {status}\")\n",
      "        print(f\"   Reconstruction error ratio: {pca_res['reconstruction_error_ratio']:.4f}x\")\n",
      "        \n",
      "        print(\"\\n3Ô∏è‚É£ MAHALANOBIS DISTANCE ANALYSIS:\")\n",
      "        print(\"-\" * 40)\n",
      "        maha_res = self.results['mahalanobis']\n",
      "        print(f\"   Mean distance ratio: {maha_res['cur_mean_distance']/maha_res['ref_mean_distance']:.4f}x\")\n",
      "        print(f\"   Outlier % (Reference): {maha_res['ref_outlier_pct']:.2f}%\")\n",
      "        print(f\"   Outlier % (Current): {maha_res['cur_outlier_pct']:.2f}%\")\n",
      "        print(f\"   KS Test p-value: {maha_res['ks_pvalue']:.4f}\")\n",
      "        \n",
      "        print(\"\\n\" + \"=\" * 70)\n",
      "        print(\"üìã OVERALL ASSESSMENT\")\n",
      "        print(\"=\" * 70)\n",
      "        summary = self.results['summary']\n",
      "        print(f\"   Drift Indicators Triggered: {summary['drift_count']}/3\")\n",
      "        for indicator, triggered in summary['drift_indicators'].items():\n",
      "            status = \"‚ö†Ô∏è\" if triggered else \"‚úì\"\n",
      "            print(f\"      {status} {indicator}: {'Yes' if triggered else 'No'}\")\n",
      "        \n",
      "        severity_emoji = {'LOW': 'üü¢', 'MEDIUM': 'üü°', 'HIGH': 'üî¥'}\n",
      "        print(f\"\\n   üéØ OVERALL MULTIVARIATE DRIFT: {severity_emoji[summary['overall_assessment']]} {summary['overall_assessment']}\")\n",
      "\n",
      "# ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô\n",
      "detector = MultivariateriftDetector(ref_multi, cur_multi)\n",
      "detector.analyze()\n",
      "detector.print_report()\n",
      "\n",
      "#%% [markdown]\n",
      "# ## ‡∏™‡∏£‡∏∏‡∏õ LAB 3\n",
      "# \n",
      "# ### ‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ:\n",
      "# 1. **Multivariate Drift**: ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡∏∂‡πâ‡∏ô‡πÄ‡∏°‡∏∑‡πà‡∏≠ relationship ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á features ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô\n",
      "# 2. **Correlation Analysis**: ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡∏≠‡∏á pairwise correlations\n",
      "# 3. **PCA Analysis**: ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡∏≠‡∏á multivariate structure\n",
      "# 4. **Mahalanobis Distance**: ‡∏ß‡∏±‡∏î dataset-level drift\n",
      "# \n",
      "# ### Key Insights:\n",
      "# - Univariate methods ‡∏≠‡∏≤‡∏à‡∏û‡∏•‡∏≤‡∏î multivariate drift\n",
      "# - ‡πÉ‡∏ä‡πâ‡∏´‡∏•‡∏≤‡∏¢ methods ‡∏£‡πà‡∏ß‡∏°‡∏Å‡∏±‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°\n",
      "# - Monitor ‡∏ó‡∏±‡πâ‡∏á individual features ‡πÅ‡∏•‡∏∞ relationships\n",
      "\n",
      "#%%\n",
      "print(\"=\" * 60)\n",
      "print(\"‚úÖ LAB 3 COMPLETED!\")\n",
      "print(\"=\" * 60)\n",
      "print(\"\"\"\n",
      "üìö Key Takeaways:\n",
      "1. Multivariate drift ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏î‡πâ‡∏ß‡∏¢ univariate methods ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ\n",
      "2. Correlation drift analysis ‡πÉ‡∏ä‡πâ Fisher's Z transformation\n",
      "3. PCA structure comparison ‡∏î‡∏π component similarities\n",
      "4. Mahalanobis distance ‡∏ß‡∏±‡∏î overall distribution shift\n",
      "\n",
      "üîú Next: LAB 4 - Drift Detection in Production Simulation\n",
      "\"\"\")\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "## LAB 4: Drift Detection in Production Simulation\n",
      "\n",
      "```python\n",
      "#%% [markdown]\n",
      "# # LAB 4: Drift Detection in Production Simulation\n",
      "# ## ‡∏Å‡∏≤‡∏£‡∏à‡∏≥‡∏•‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö Drift ‡πÉ‡∏ô Production Environment\n",
      "# \n",
      "# ### ‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ:\n",
      "# 1. ‡∏™‡∏£‡πâ‡∏≤‡∏á simulated data stream ‡∏ó‡∏µ‡πà‡∏°‡∏µ gradual drift\n",
      "# 2. ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö sudden vs gradual drift\n",
      "# 3. Implement sliding window monitoring\n",
      "#\n",
      "# ### ‡∏ó‡∏§‡∏©‡∏é‡∏µ:\n",
      "# ‡πÉ‡∏ô production environment:\n",
      "# - ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏°‡∏≤‡πÄ‡∏õ‡πá‡∏ô stream ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà batch\n",
      "# - Drift ‡∏≠‡∏≤‡∏à‡πÄ‡∏Å‡∏¥‡∏î‡πÅ‡∏ö‡∏ö sudden ‡∏´‡∏£‡∏∑‡∏≠ gradual\n",
      "# - ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ monitoring strategy ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°\n",
      "\n",
      "#%% [markdown]\n",
      "# ## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 1: ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° Environment\n",
      "\n",
      "#%%\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy import stats\n",
      "from collections import deque\n",
      "from datetime import datetime, timedelta\n",
      "import warnings\n",
      "warnings.filterwarnings('ignore')\n",
      "\n",
      "np.random.seed(42)\n",
      "plt.rcParams['figure.figsize'] = (14, 6)\n",
      "\n",
      "print(\"‚úÖ Libraries imported successfully!\")\n",
      "\n",
      "#%% [markdown]\n",
      "# ## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 2: ‡∏™‡∏£‡πâ‡∏≤‡∏á Data Stream Simulator\n",
      "# \n",
      "# ‡∏à‡∏≥‡∏•‡∏≠‡∏á data stream ‡∏ó‡∏µ‡πà‡∏°‡∏µ drift ‡∏´‡∏•‡∏≤‡∏¢‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö\n",
      "\n",
      "#%%\n",
      "class DataStreamSimulator:\n",
      "    \"\"\"\n",
      "    Simulator ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡∏£‡πâ‡∏≤‡∏á data stream ‡∏ó‡∏µ‡πà‡∏°‡∏µ drift patterns ‡∏ï‡πà‡∏≤‡∏á‡πÜ\n",
      "    \n",
      "    Drift Types:\n",
      "    - sudden: ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏ó‡∏±‡∏ô‡∏ó‡∏µ‡∏ó‡∏µ‡πà‡∏à‡∏∏‡∏î‡πÉ‡∏î‡∏à‡∏∏‡∏î‡∏´‡∏ô‡∏∂‡πà‡∏á\n",
      "    - gradual: ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏ä‡πâ‡∏≤‡πÜ ‡∏ï‡∏≤‡∏°‡πÄ‡∏ß‡∏•‡∏≤\n",
      "    - incremental: ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡∏±‡πâ‡∏ô‡∏ö‡∏±‡∏ô‡πÑ‡∏î\n",
      "    - seasonal: ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏ï‡∏≤‡∏° pattern ‡∏ã‡πâ‡∏≥\n",
      "    - no_drift: ‡πÑ‡∏°‡πà‡∏°‡∏µ drift\n",
      "    \"\"\"\n",
      "    \n",
      "    def __init__(self, base_mean=50, base_std=10, random_seed=42):\n",
      "        self.base_mean = base_mean\n",
      "        self.base_std = base_std\n",
      "        self.random_seed = random_seed\n",
      "        np.random.seed(random_seed)\n",
      "    \n",
      "    def generate_stream(self, n_samples, drift_type='no_drift', drift_params=None):\n",
      "        \"\"\"\n",
      "        ‡∏™‡∏£‡πâ‡∏≤‡∏á data stream\n",
      "        \n",
      "        Parameters:\n",
      "        -----------\n",
      "        n_samples : int - ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô samples\n",
      "        drift_type : str - ‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Ç‡∏≠‡∏á drift\n",
      "        drift_params : dict - parameters ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö drift\n",
      "        \"\"\"\n",
      "        if drift_params is None:\n",
      "            drift_params = {}\n",
      "        \n",
      "        data = np.zeros(n_samples)\n",
      "        timestamps = [datetime(2024, 1, 1) + timedelta(hours=i) for i in range(n_samples)]\n",
      "        \n",
      "        if drift_type == 'no_drift':\n",
      "            data = np.random.normal(self.base_mean, self.base_std, n_samples)\n",
      "        \n",
      "        elif drift_type == 'sudden':\n",
      "            drift_point = drift_params.get('drift_point', n_samples // 2)\n",
      "            new_mean = drift_params.get('new_mean', self.base_mean + 2 * self.base_std)\n",
      "            \n",
      "            data[:drift_point] = np.random.normal(self.base_mean, self.base_std, drift_point)\n",
      "            data[drift_point:] = np.random.normal(new_mean, self.base_std, n_samples - drift_point)\n",
      "        \n",
      "        elif drift_type == 'gradual':\n",
      "            drift_start = drift_params.get('drift_start', n_samples // 4)\n",
      "            drift_end = drift_params.get('drift_end', 3 * n_samples // 4)\n",
      "            final_mean = drift_params.get('final_mean', self.base_mean + 2 * self.base_std)\n",
      "            \n",
      "            for i in range(n_samples):\n",
      "                if i < drift_start:\n",
      "                    current_mean = self.base_mean\n",
      "                elif i > drift_end:\n",
      "                    current_mean = final_mean\n",
      "                else:\n",
      "                    # Linear interpolation\n",
      "                    progress = (i - drift_start) / (drift_end - drift_start)\n",
      "                    current_mean = self.base_mean + progress * (final_mean - self.base_mean)\n",
      "                \n",
      "                data[i] = np.random.normal(current_mean, self.base_std)\n",
      "        \n",
      "        elif drift_type == 'incremental':\n",
      "            step_size = drift_params.get('step_size', n_samples // 5)\n",
      "            step_increase = drift_params.get('step_increase', self.base_std * 0.5)\n",
      "            \n",
      "            for i in range(n_samples):\n",
      "                step = i // step_size\n",
      "                current_mean = self.base_mean + step * step_increase\n",
      "                data[i] = np.random.normal(current_mean, self.base_std)\n",
      "        \n",
      "        elif drift_type == 'seasonal':\n",
      "            period = drift_params.get('period', 168)  # 1 week in hours\n",
      "            amplitude = drift_params.get('amplitude', self.base_std)\n",
      "            \n",
      "            for i in range(n_samples):\n",
      "                seasonal_effect = amplitude * np.sin(2 * np.pi * i / period)\n",
      "                data[i] = np.random.normal(self.base_mean + seasonal_effect, self.base_std)\n",
      "        \n",
      "        return pd.DataFrame({\n",
      "            'timestamp': timestamps,\n",
      "            'value': data,\n",
      "            'index': range(n_samples)\n",
      "        })\n",
      "\n",
      "# ‡∏™‡∏£‡πâ‡∏≤‡∏á simulator\n",
      "simulator = DataStreamSimulator(base_mean=50, base_std=10)\n",
      "\n",
      "print(\"‚úÖ DataStreamSimulator created\")\n",
      "\n",
      "#%%\n",
      "# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á drift patterns\n",
      "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
      "axes = axes.flatten()\n",
      "\n",
      "drift_types = ['no_drift', 'sudden', 'gradual', 'incremental', 'seasonal']\n",
      "drift_params_list = [\n",
      "    {},\n",
      "    {'drift_point': 500, 'new_mean': 70},\n",
      "    {'drift_start': 200, 'drift_end': 800, 'final_mean': 70},\n",
      "    {'step_size': 200, 'step_increase': 5},\n",
      "    {'period': 200, 'amplitude': 15}\n",
      "]\n",
      "\n",
      "streams = {}\n",
      "for i, (dtype, params) in enumerate(zip(drift_types, drift_params_list)):\n",
      "    stream = simulator.generate_stream(1000, drift_type=dtype, drift_params=params)\n",
      "    streams[dtype] = stream\n",
      "    \n",
      "    ax = axes[i]\n",
      "    ax.plot(stream['index'], stream['value'], alpha=0.7, linewidth=0.5)\n",
      "    \n",
      "    # Add rolling mean\n",
      "    rolling_mean = stream['value'].rolling(window=50).mean()\n",
      "    ax.plot(stream['index'], rolling_mean, 'r-', linewidth=2, label='Rolling Mean (50)')\n",
      "    \n",
      "    ax.axhline(50, color='green', linestyle='--', alpha=0.5, label='Original Mean')\n",
      "    ax.set_title(f'{dtype.replace(\"_\", \" \").title()}')\n",
      "    ax.set_xlabel('Time Index')\n",
      "    ax.set_ylabel('Value')\n",
      "    ax.legend(fontsize=8)\n",
      "\n",
      "# Hide empty subplot\n",
      "axes[-1].set_visible(False)\n",
      "\n",
      "plt.suptitle('Different Types of Data Drift', fontsize=14, fontweight='bold')\n",
      "plt.tight_layout()\n",
      "plt.savefig('drift_types.png', dpi=150, bbox_inches='tight')\n",
      "plt.show()\n",
      "\n",
      "#%% [markdown]\n",
      "# ## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 3: Sliding Window Drift Detector\n",
      "# \n",
      "# Implement sliding window ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö real-time drift detection\n",
      "\n",
      "#%%\n",
      "class SlidingWindowDriftDetector:\n",
      "    \"\"\"\n",
      "    Drift detector ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ sliding window approach\n",
      "    \n",
      "    ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö:\n",
      "    - Real-time monitoring\n",
      "    - Streaming data\n",
      "    - Memory-efficient processing\n",
      "    \"\"\"\n",
      "    \n",
      "    def __init__(self, reference_window_size=200, test_window_size=100, \n",
      "                 ks_threshold=0.05, psi_threshold=0.1):\n",
      "        \"\"\"\n",
      "        Parameters:\n",
      "        -----------\n",
      "        reference_window_size : int - ‡∏Ç‡∏ô‡∏≤‡∏î‡∏Ç‡∏≠‡∏á reference window\n",
      "        test_window_size : int - ‡∏Ç‡∏ô‡∏≤‡∏î‡∏Ç‡∏≠‡∏á test window\n",
      "        ks_threshold : float - threshold ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö KS test p-value\n",
      "        psi_threshold : float - threshold ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö PSI\n",
      "        \"\"\"\n",
      "        self.reference_window_size = reference_window_size\n",
      "        self.test_window_size = test_window_size\n",
      "        self.ks_threshold = ks_threshold\n",
      "        self.psi_threshold = psi_threshold\n",
      "        \n",
      "        # ‡πÉ‡∏ä‡πâ deque ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö efficient sliding window\n",
      "        self.reference_buffer = deque(maxlen=reference_window_size)\n",
      "        self.test_buffer = deque(maxlen=test_window_size)\n",
      "        \n",
      "        self.history = []\n",
      "        self.drift_points = []\n",
      "        self.is_initialized = False\n",
      "    \n",
      "    def calculate_psi(self, reference, test, bins=10):\n",
      "        \"\"\"‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì PSI\"\"\"\n",
      "        breakpoints = np.percentile(reference, np.linspace(0, 100, bins + 1))\n",
      "        breakpoints = np.unique(breakpoints)\n",
      "        \n",
      "        ref_counts, _ = np.histogram(reference, bins=breakpoints)\n",
      "        test_counts, _ = np.histogram(test, bins=breakpoints)\n",
      "        \n",
      "        eps = 1e-6\n",
      "        ref_props = ref_counts / len(reference) + eps\n",
      "        test_props = test_counts / len(test) + eps\n",
      "        \n",
      "        psi = np.sum((test_props - ref_props) * np.log(test_props / ref_props))\n",
      "        return psi\n",
      "    \n",
      "    def update(self, value, timestamp=None):\n",
      "        \"\"\"\n",
      "        Update detector ‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡πà‡∏≤‡πÉ‡∏´‡∏°‡πà\n",
      "        \n",
      "        Returns:\n",
      "        --------\n",
      "        dict : detection result\n",
      "        \"\"\"\n",
      "        # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡πà‡∏≤‡πÉ‡∏ô buffers\n",
      "        if not self.is_initialized:\n",
      "            self.reference_buffer.append(value)\n",
      "            if len(self.reference_buffer) >= self.reference_window_size:\n",
      "                self.is_initialized = True\n",
      "            return {'drift_detected': False, 'status': 'initializing'}\n",
      "        \n",
      "        self.test_buffer.append(value)\n",
      "        \n",
      "        # ‡∏£‡∏≠‡∏à‡∏ô‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏≠‡πÉ‡∏ô test buffer\n",
      "        if len(self.test_buffer) < self.test_window_size:\n",
      "            return {'drift_detected': False, 'status': 'collecting'}\n",
      "        \n",
      "        # ‡∏ó‡∏≥ drift detection\n",
      "        ref_array = np.array(self.reference_buffer)\n",
      "        test_array = np.array(self.test_buffer)\n",
      "        \n",
      "        # KS Test\n",
      "        ks_stat, ks_pval = stats.ks_2samp(ref_array, test_array)\n",
      "        \n",
      "        # PSI\n",
      "        psi = self.calculate_psi(ref_array, test_array)\n",
      "        \n",
      "        # Detection logic\n",
      "        ks_drift = ks_pval < self.ks_threshold\n",
      "        psi_drift = psi > self.psi_threshold\n",
      "        drift_detected = ks_drift or psi_drift\n",
      "        \n",
      "        result = {\n",
      "            'timestamp': timestamp,\n",
      "            'drift_detected': drift_detected,\n",
      "            'ks_statistic': ks_stat,\n",
      "            'ks_pvalue': ks_pval,\n",
      "            'psi': psi,\n",
      "            'ref_mean': np.mean(ref_array),\n",
      "            'test_mean': np.mean(test_array),\n",
      "            'status': 'DRIFT' if drift_detected else 'normal'\n",
      "        }\n",
      "        \n",
      "        self.history.append(result)\n",
      "        \n",
      "        if drift_detected:\n",
      "            self.drift_points.append(len(self.history) - 1)\n",
      "        \n",
      "        return result\n",
      "    \n",
      "    def reset_reference(self):\n",
      "        \"\"\"Reset reference window ‡∏î‡πâ‡∏ß‡∏¢ test window ‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô\"\"\"\n",
      "        self.reference_buffer.clear()\n",
      "        for val in self.test_buffer:\n",
      "            self.reference_buffer.append(val)\n",
      "        self.test_buffer.clear()\n",
      "        print(\"üìã Reference window reset with current data\")\n",
      "    \n",
      "    def get_history_df(self):\n",
      "        \"\"\"‡πÅ‡∏õ‡∏•‡∏á history ‡πÄ‡∏õ‡πá‡∏ô DataFrame\"\"\"\n",
      "        return pd.DataFrame(self.history)\n",
      "\n",
      "#%%\n",
      "# ‡∏ó‡∏î‡∏™‡∏≠‡∏ö SlidingWindowDriftDetector ‡∏Å‡∏±‡∏ö sudden drift\n",
      "print(\"=\" * 60)\n",
      "print(\"üîç Testing Sliding Window Detector with SUDDEN DRIFT\")\n",
      "print(\"=\" * 60)\n",
      "\n",
      "detector = SlidingWindowDriftDetector(\n",
      "    reference_window_size=200,\n",
      "    test_window_size=100,\n",
      "    ks_threshold=0.05,\n",
      "    psi_threshold=0.1\n",
      ")\n",
      "\n",
      "sudden_stream = streams['sudden']\n",
      "\n",
      "for idx, row in sudden_stream.iterrows():\n",
      "    result = detector.update(row['value'], timestamp=row['timestamp'])\n",
      "    \n",
      "    # Print only drift events\n",
      "    if result.get('drift_detected', False):\n",
      "        print(f\"‚ö†Ô∏è DRIFT at index {idx}: KS p-value={result['ks_pvalue']:.4f}, PSI={result['psi']:.4f}\")\n",
      "\n",
      "print(f\"\\nüìä Total drift points detected: {len(detector.drift_points)}\")\n",
      "\n",
      "#%%\n",
      "# Visualize detection results\n",
      "history_df = detector.get_history_df()\n",
      "\n",
      "fig, axes = plt.subplots(3, 1, figsize=(14, 10), sharex=True)\n",
      "\n",
      "# Plot 1: Original data with drift points\n",
      "ax1 = axes[0]\n",
      "ax1.plot(sudden_stream['index'], sudden_stream['value'], alpha=0.5, label='Data')\n",
      "ax1.plot(sudden_stream['index'], sudden_stream['value'].rolling(50).mean(), \n",
      "         'b-', linewidth=2, label='Rolling Mean')\n",
      "\n",
      "# Mark drift points\n",
      "for dp in detector.drift_points:\n",
      "    ax1.axvline(dp + 200 + 100, color='red', alpha=0.5, linestyle='--')  # Offset for initialization\n",
      "\n",
      "ax1.axvline(500, color='green', linestyle='--', linewidth=2, label='Actual Drift Point')\n",
      "ax1.set_ylabel('Value')\n",
      "ax1.set_title('Data Stream with Detected Drift Points')\n",
      "ax1.legend()\n",
      "\n",
      "# Plot 2: KS p-value over time\n",
      "ax2 = axes[1]\n",
      "ax2.plot(history_df.index + 200 + 100, history_df['ks_pvalue'], 'b-', linewidth=1)\n",
      "ax2.axhline(0.05, color='red', linestyle='--', label='Threshold (0.05)')\n",
      "ax2.set_ylabel('KS p-value')\n",
      "ax2.set_title('KS Test p-value Over Time')\n",
      "ax2.legend()\n",
      "ax2.set_yscale('log')\n",
      "\n",
      "# Plot 3: PSI over time\n",
      "ax3 = axes[2]\n",
      "ax3.plot(history_df.index + 200 + 100, history_df['psi'], 'g-', linewidth=1)\n",
      "ax3.axhline(0.1, color='red', linestyle='--', label='Threshold (0.1)')\n",
      "ax3.set_ylabel('PSI')\n",
      "ax3.set_xlabel('Time Index')\n",
      "ax3.set_title('PSI Over Time')\n",
      "ax3.legend()\n",
      "\n",
      "plt.tight_layout()\n",
      "plt.savefig('sliding_window_detection.png', dpi=150, bbox_inches='tight')\n",
      "plt.show()\n",
      "\n",
      "#%% [markdown]\n",
      "# ## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 4: Detecting Gradual Drift\n",
      "# \n",
      "# Gradual drift ‡∏¢‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤ sudden drift ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏ä‡πâ‡∏≤‡πÜ\n",
      "\n",
      "#%%\n",
      "print(\"=\" * 60)\n",
      "print(\"üîç Testing Sliding Window Detector with GRADUAL DRIFT\")\n",
      "print(\"=\" * 60)\n",
      "\n",
      "detector_gradual = SlidingWindowDriftDetector(\n",
      "    reference_window_size=200,\n",
      "    test_window_size=100\n",
      ")\n",
      "\n",
      "gradual_stream = streams['gradual']\n",
      "\n",
      "for idx, row in gradual_stream.iterrows():\n",
      "    result = detector_gradual.update(row['value'], timestamp=row['timestamp'])\n",
      "\n",
      "history_gradual = detector_gradual.get_history_df()\n",
      "\n",
      "print(f\"\\nüìä Total drift points detected: {len(detector_gradual.drift_points)}\")\n",
      "print(f\"   First detection at index: {detector_gradual.drift_points[0] + 300 if detector_gradual.drift_points else 'N/A'}\")\n",
      "\n",
      "#%%\n",
      "# Compare detection of sudden vs gradual drift\n",
      "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
      "\n",
      "# Sudden drift\n",
      "ax1 = axes[0, 0]\n",
      "ax1.plot(sudden_stream['index'], sudden_stream['value'], alpha=0.3)\n",
      "ax1.plot(sudden_stream['index'], sudden_stream['value'].rolling(50).mean(), 'b-', linewidth=2)\n",
      "for dp in detector.drift_points:\n",
      "    ax1.axvline(dp + 300, color='red', alpha=0.3)\n",
      "ax1.axvline(500, color='green', linestyle='--', linewidth=2, label='Actual Drift')\n",
      "ax1.set_title('Sudden Drift Detection')\n",
      "ax1.legend()\n",
      "\n",
      "ax2 = axes[0, 1]\n",
      "history_df = detector.get_history_df()\n",
      "ax2.plot(history_df.index + 300, history_df['psi'], 'g-')\n",
      "ax2.axhline(0.1, color='red', linestyle='--')\n",
      "ax2.set_title('Sudden Drift: PSI')\n",
      "\n",
      "# Gradual drift\n",
      "ax3 = axes[1, 0]\n",
      "ax3.plot(gradual_stream['index'], gradual_stream['value'], alpha=0.3)\n",
      "ax3.plot(gradual_stream['index'], gradual_stream['value'].rolling(50).mean(), 'b-', linewidth=2)\n",
      "for dp in detector_gradual.drift_points:\n",
      "    ax3.axvline(dp + 300, color='red', alpha=0.3)\n",
      "ax3.axvspan(200, 800, alpha=0.2, color='green', label='Drift Period')\n",
      "ax3.set_title('Gradual Drift Detection')\n",
      "ax3.legend()\n",
      "\n",
      "ax4 = axes[1, 1]\n",
      "ax4.plot(history_gradual.index + 300, history_gradual['psi'], 'g-')\n",
      "ax4.axhline(0.1, color='red', linestyle='--')\n",
      "ax4.set_title('Gradual Drift: PSI')\n",
      "\n",
      "plt.tight_layout()\n",
      "plt.savefig('sudden_vs_gradual_detection.png', dpi=150, bbox_inches='tight')\n",
      "plt.show()\n",
      "\n",
      "print(\"\\nüí° Observation:\")\n",
      "print(\"   - Sudden drift: ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡πÑ‡∏î‡πâ‡πÄ‡∏£‡πá‡∏ß‡πÅ‡∏•‡∏∞‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô\")\n",
      "print(\"   - Gradual drift: ‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤‡∏ô‡∏≤‡∏ô‡∏Å‡∏ß‡πà‡∏≤‡∏à‡∏∞ detect ‡πÑ‡∏î‡πâ\")\n",
      "\n",
      "#%% [markdown]\n",
      "# ## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 5: Adaptive Reference Window\n",
      "# \n",
      "# ‡∏õ‡∏£‡∏±‡∏ö reference window ‡πÄ‡∏û‡∏∑‡πà‡∏≠ handle gradual drift\n",
      "\n",
      "#%%\n",
      "class AdaptiveDriftDetector:\n",
      "    \"\"\"\n",
      "    Drift detector ‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏±‡∏ö reference window ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥\n",
      "    \n",
      "    Features:\n",
      "    - ‡∏õ‡∏£‡∏±‡∏ö reference ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö drift\n",
      "    - ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô false alarms ‡∏à‡∏≤‡∏Å temporary spikes\n",
      "    - Track both short-term ‡πÅ‡∏•‡∏∞ long-term drift\n",
      "    \"\"\"\n",
      "    \n",
      "    def __init__(self, reference_window_size=200, test_window_size=50,\n",
      "                 confirmation_window=3, psi_threshold=0.1):\n",
      "        \"\"\"\n",
      "        Parameters:\n",
      "        -----------\n",
      "        confirmation_window : int - ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ï‡∏¥‡∏î‡∏ï‡πà‡∏≠‡∏Å‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á detect ‡∏Å‡πà‡∏≠‡∏ô‡∏¢‡∏∑‡∏ô‡∏¢‡∏±‡∏ô drift\n",
      "        \"\"\"\n",
      "        self.reference_window_size = reference_window_size\n",
      "        self.test_window_size = test_window_size\n",
      "        self.confirmation_window = confirmation_window\n",
      "        self.psi_threshold = psi_threshold\n",
      "        \n",
      "        self.reference_buffer = deque(maxlen=reference_window_size)\n",
      "        self.test_buffer = deque(maxlen=test_window_size)\n",
      "        \n",
      "        self.consecutive_drift_count = 0\n",
      "        self.confirmed_drifts = []\n",
      "        self.history = []\n",
      "        self.adaptation_count = 0\n",
      "    \n",
      "    def calculate_psi(self, reference, test, bins=10):\n",
      "        \"\"\"‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì PSI\"\"\"\n",
      "        breakpoints = np.percentile(reference, np.linspace(0, 100, bins + 1))\n",
      "        breakpoints = np.unique(breakpoints)\n",
      "        \n",
      "        ref_counts, _ = np.histogram(reference, bins=breakpoints)\n",
      "        test_counts, _ = np.histogram(test, bins=breakpoints)\n",
      "        \n",
      "        eps = 1e-6\n",
      "        ref_props = ref_counts / len(reference) + eps\n",
      "        test_props = test_counts / len(test) + eps\n",
      "        \n",
      "        psi = np.sum((test_props - ref_props) * np.log(test_props / ref_props))\n",
      "        return psi\n",
      "    \n",
      "    def adapt_reference(self):\n",
      "        \"\"\"‡∏õ‡∏£‡∏±‡∏ö reference window\"\"\"\n",
      "        # ‡∏ú‡∏™‡∏° old reference ‡∏Å‡∏±‡∏ö new data\n",
      "        old_weight = 0.5\n",
      "        old_ref = list(self.reference_buffer)\n",
      "        new_data = list(self.test_buffer)\n",
      "        \n",
      "        # ‡∏™‡∏£‡πâ‡∏≤‡∏á new reference\n",
      "        self.reference_buffer.clear()\n",
      "        \n",
      "        # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ö‡∏≤‡∏á‡∏™‡πà‡∏ß‡∏ô‡∏à‡∏≤‡∏Å old reference\n",
      "        n_old = int(len(old_ref) * old_weight)\n",
      "        for val in old_ref[-n_old:]:\n",
      "            self.reference_buffer.append(val)\n",
      "        \n",
      "        # ‡πÄ‡∏û‡∏¥‡πà‡∏° new data\n",
      "        for val in new_data:\n",
      "            self.reference_buffer.append(val)\n",
      "        \n",
      "        self.adaptation_count += 1\n",
      "        self.consecutive_drift_count = 0\n",
      "    \n",
      "    def update(self, value, timestamp=None):\n",
      "        \"\"\"Update detector\"\"\"\n",
      "        # Initialize\n",
      "        if len(self.reference_buffer) < self.reference_window_size:\n",
      "            self.reference_buffer.append(value)\n",
      "            return {'status': 'initializing', 'drift_detected': False}\n",
      "        \n",
      "        self.test_buffer.append(value)\n",
      "        \n",
      "        if len(self.test_buffer) < self.test_window_size:\n",
      "            return {'status': 'collecting', 'drift_detected': False}\n",
      "        \n",
      "        # Drift detection\n",
      "        ref_array = np.array(self.reference_buffer)\n",
      "        test_array = np.array(self.test_buffer)\n",
      "        \n",
      "        psi = self.calculate_psi(ref_array, test_array)\n",
      "        potential_drift = psi > self.psi_threshold\n",
      "        \n",
      "        if potential_drift:\n",
      "            self.consecutive_drift_count += 1\n",
      "        else:\n",
      "            self.consecutive_drift_count = 0\n",
      "        \n",
      "        # Confirmed drift (multiple consecutive detections)\n",
      "        confirmed = self.consecutive_drift_count >= self.confirmation_window\n",
      "        \n",
      "        result = {\n",
      "            'timestamp': timestamp,\n",
      "            'psi': psi,\n",
      "            'potential_drift': potential_drift,\n",
      "            'confirmed_drift': confirmed,\n",
      "            'consecutive_count': self.consecutive_drift_count,\n",
      "            'adaptation_count': self.adaptation_count,\n",
      "            'ref_mean': np.mean(ref_array),\n",
      "            'test_mean': np.mean(test_array)\n",
      "        }\n",
      "        \n",
      "        self.history.append(result)\n",
      "        \n",
      "        # Adapt if confirmed\n",
      "        if confirmed:\n",
      "            self.confirmed_drifts.append(len(self.history) - 1)\n",
      "            self.adapt_reference()\n",
      "            result['adapted'] = True\n",
      "        \n",
      "        return result\n",
      "    \n",
      "    def get_history_df(self):\n",
      "        return pd.DataFrame(self.history)\n",
      "\n",
      "#%%\n",
      "# ‡∏ó‡∏î‡∏™‡∏≠‡∏ö Adaptive Detector\n",
      "print(\"=\" * 60)\n",
      "print(\"üîç Testing ADAPTIVE Drift Detector with GRADUAL DRIFT\")\n",
      "print(\"=\" * 60)\n",
      "\n",
      "adaptive_detector = AdaptiveDriftDetector(\n",
      "    reference_window_size=200,\n",
      "    test_window_size=50,\n",
      "    confirmation_window=3,\n",
      "    psi_threshold=0.1\n",
      ")\n",
      "\n",
      "for idx, row in gradual_stream.iterrows():\n",
      "    result = adaptive_detector.update(row['value'], timestamp=row['timestamp'])\n",
      "    \n",
      "    if result.get('confirmed_drift', False):\n",
      "        print(f\"‚úÖ CONFIRMED DRIFT at index {idx}: PSI={result['psi']:.4f}, Adapted!\")\n",
      "\n",
      "print(f\"\\nüìä Total confirmed drifts: {len(adaptive_detector.confirmed_drifts)}\")\n",
      "print(f\"üìä Total adaptations: {adaptive_detector.adaptation_count}\")\n",
      "\n",
      "#%%\n",
      "# Visualize adaptive detection\n",
      "adaptive_history = adaptive_detector.get_history_df()\n",
      "\n",
      "fig, axes = plt.subplots(3, 1, figsize=(14, 10), sharex=True)\n",
      "\n",
      "# Plot 1: Data with adaptations\n",
      "ax1 = axes[0]\n",
      "ax1.plot(gradual_stream['index'], gradual_stream['value'], alpha=0.3)\n",
      "ax1.plot(gradual_stream['index'], gradual_stream['value'].rolling(50).mean(), 'b-', linewidth=2)\n",
      "\n",
      "offset = 200 + 50  # initialization + test window\n",
      "for drift_idx in adaptive_detector.confirmed_drifts:\n",
      "    ax1.axvline(drift_idx + offset, color='red', linestyle='--', alpha=0.7, linewidth=2)\n",
      "\n",
      "ax1.set_title('Gradual Drift with Adaptive Detection')\n",
      "ax1.set_ylabel('Value')\n",
      "\n",
      "# Plot 2: PSI with threshold\n",
      "ax2 = axes[1]\n",
      "ax2.plot(adaptive_history.index + offset, adaptive_history['psi'], 'g-')\n",
      "ax2.axhline(0.1, color='red', linestyle='--', label='Threshold')\n",
      "ax2.set_ylabel('PSI')\n",
      "ax2.legend()\n",
      "\n",
      "# Plot 3: Reference vs Test mean\n",
      "ax3 = axes[2]\n",
      "ax3.plot(adaptive_history.index + offset, adaptive_history['ref_mean'], 'b-', label='Reference Mean')\n",
      "ax3.plot(adaptive_history.index + offset, adaptive_history['test_mean'], 'r-', label='Test Mean')\n",
      "ax3.set_xlabel('Time Index')\n",
      "ax3.set_ylabel('Mean')\n",
      "ax3.legend()\n",
      "\n",
      "plt.tight_layout()\n",
      "plt.savefig('adaptive_detection.png', dpi=150, bbox_inches='tight')\n",
      "plt.show()\n",
      "\n",
      "print(\"\\nüí° Adaptive detector ‡∏õ‡∏£‡∏±‡∏ö reference window ‡πÄ‡∏°‡∏∑‡πà‡∏≠ detect drift\")\n",
      "print(\"   ‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏° gradual drift ‡πÑ‡∏î‡πâ‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á\")\n",
      "\n",
      "#%% [markdown]\n",
      "# ## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 6: Page-Hinkley Test for Change Detection\n",
      "# \n",
      "# Algorithm ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö detect mean shift ‡πÉ‡∏ô streaming data\n",
      "\n",
      "#%%\n",
      "class PageHinkleyDetector:\n",
      "    \"\"\"\n",
      "    Page-Hinkley test ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö detect mean shift\n",
      "    \n",
      "    ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö:\n",
      "    - Streaming data\n",
      "    - Detect upward ‡∏´‡∏£‡∏∑‡∏≠ downward shifts\n",
      "    - Low memory footprint\n",
      "    \"\"\"\n",
      "    \n",
      "    def __init__(self, delta=0.005, lambda_=50, alpha=0.9999):\n",
      "        \"\"\"\n",
      "        Parameters:\n",
      "        -----------\n",
      "        delta : float - magnitude ‡∏ó‡∏µ‡πà‡∏¢‡∏≠‡∏°‡∏£‡∏±‡∏ö‡πÑ‡∏î‡πâ‡∏Ç‡∏≠‡∏á change\n",
      "        lambda_ : float - detection threshold\n",
      "        alpha : float - forgetting factor ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö mean estimation\n",
      "        \"\"\"\n",
      "        self.delta = delta\n",
      "        self.lambda_ = lambda_\n",
      "        self.alpha = alpha\n",
      "        \n",
      "        self.mean = 0\n",
      "        self.sum = 0\n",
      "        self.min_sum = float('inf')\n",
      "        self.max_sum = float('-inf')\n",
      "        \n",
      "        self.n_samples = 0\n",
      "        self.history = []\n",
      "        self.drift_points = []\n",
      "    \n",
      "    def update(self, value, timestamp=None):\n",
      "        \"\"\"\n",
      "        Update detector ‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡πà‡∏≤‡πÉ‡∏´‡∏°‡πà\n",
      "        \"\"\"\n",
      "        self.n_samples += 1\n",
      "        \n",
      "        # Update mean\n",
      "        if self.n_samples == 1:\n",
      "            self.mean = value\n",
      "        else:\n",
      "            self.mean = self.alpha * self.mean + (1 - self.alpha) * value\n",
      "        \n",
      "        # Update cumulative sum\n",
      "        self.sum += value - self.mean - self.delta\n",
      "        \n",
      "        # Update min/max\n",
      "        self.min_sum = min(self.min_sum, self.sum)\n",
      "        self.max_sum = max(self.max_sum, self.sum)\n",
      "        \n",
      "        # Calculate test statistics\n",
      "        ph_positive = self.sum - self.min_sum  # Detect upward shift\n",
      "        ph_negative = self.max_sum - self.sum  # Detect downward shift\n",
      "        \n",
      "        # Detection\n",
      "        drift_up = ph_positive > self.lambda_\n",
      "        drift_down = ph_negative > self.lambda_\n",
      "        drift_detected = drift_up or drift_down\n",
      "        \n",
      "        result = {\n",
      "            'timestamp': timestamp,\n",
      "            'value': value,\n",
      "            'mean': self.mean,\n",
      "            'sum': self.sum,\n",
      "            'ph_positive': ph_positive,\n",
      "            'ph_negative': ph_negative,\n",
      "            'drift_detected': drift_detected,\n",
      "            'drift_direction': 'up' if drift_up else ('down' if drift_down else None)\n",
      "        }\n",
      "        \n",
      "        self.history.append(result)\n",
      "        \n",
      "        if drift_detected:\n",
      "            self.drift_points.append(self.n_samples - 1)\n",
      "            self.reset()\n",
      "        \n",
      "        return result\n",
      "    \n",
      "    def reset(self):\n",
      "        \"\"\"Reset statistics ‡∏´‡∏•‡∏±‡∏á detect drift\"\"\"\n",
      "        self.sum = 0\n",
      "        self.min_sum = float('inf')\n",
      "        self.max_sum = float('-inf')\n",
      "    \n",
      "    def get_history_df(self):\n",
      "        return pd.DataFrame(self.history)\n",
      "\n",
      "#%%\n",
      "# ‡∏ó‡∏î‡∏™‡∏≠‡∏ö Page-Hinkley\n",
      "print(\"=\" * 60)\n",
      "print(\"üîç Testing PAGE-HINKLEY Detector\")\n",
      "print(\"=\" * 60)\n",
      "\n",
      "ph_detector = PageHinkleyDetector(delta=0.01, lambda_=30)\n",
      "\n",
      "# ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏±‡∏ö sudden drift\n",
      "for idx, row in sudden_stream.iterrows():\n",
      "    result = ph_detector.update(row['value'], timestamp=row['timestamp'])\n",
      "    \n",
      "    if result['drift_detected']:\n",
      "        print(f\"‚ö†Ô∏è DRIFT at index {idx}: Direction = {result['drift_direction']}\")\n",
      "\n",
      "print(f\"\\nüìä Total drifts detected: {len(ph_detector.drift_points)}\")\n",
      "\n",
      "#%%\n",
      "# Visualize Page-Hinkley results\n",
      "ph_history = ph_detector.get_history_df()\n",
      "\n",
      "fig, axes = plt.subplots(3, 1, figsize=(14, 10), sharex=True)\n",
      "\n",
      "# Plot 1: Data\n",
      "ax1 = axes[0]\n",
      "ax1.plot(sudden_stream['index'], sudden_stream['value'], alpha=0.3)\n",
      "ax1.plot(ph_history.index, ph_history['mean'], 'r-', linewidth=2, label='Estimated Mean')\n",
      "for dp in ph_detector.drift_points:\n",
      "    ax1.axvline(dp, color='red', linestyle='--', alpha=0.7)\n",
      "ax1.set_title('Page-Hinkley Detection')\n",
      "ax1.set_ylabel('Value')\n",
      "ax1.legend()\n",
      "\n",
      "# Plot 2: PH Statistics\n",
      "ax2 = axes[1]\n",
      "ax2.plot(ph_history.index, ph_history['ph_positive'], 'g-', label='PH+ (upward)')\n",
      "ax2.plot(ph_history.index, ph_history['ph_negative'], 'b-', label='PH- (downward)')\n",
      "ax2.axhline(30, color='red', linestyle='--', label='Threshold')\n",
      "ax2.set_ylabel('PH Statistics')\n",
      "ax2.legend()\n",
      "\n",
      "# Plot 3: Cumulative Sum\n",
      "ax3 = axes[2]\n",
      "ax3.plot(ph_history.index, ph_history['sum'], 'purple')\n",
      "ax3.set_xlabel('Time Index')\n",
      "ax3.set_ylabel('Cumulative Sum')\n",
      "\n",
      "plt.tight_layout()\n",
      "plt.savefig('page_hinkley_detection.png', dpi=150, bbox_inches='tight')\n",
      "plt.show()\n",
      "\n",
      "#%% [markdown]\n",
      "# ## ‡∏™‡∏£‡∏∏‡∏õ LAB 4\n",
      "# \n",
      "# ### ‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ:\n",
      "# 1. **Data Stream Simulation**: ‡∏™‡∏£‡πâ‡∏≤‡∏á streams ‡∏ó‡∏µ‡πà‡∏°‡∏µ drift patterns ‡∏ï‡πà‡∏≤‡∏á‡πÜ\n",
      "# 2. **Sliding Window**: ‡∏ß‡∏¥‡∏ò‡∏µ‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö streaming drift detection\n",
      "# 3. **Adaptive Detection**: ‡∏õ‡∏£‡∏±‡∏ö reference window ‡πÄ‡∏°‡∏∑‡πà‡∏≠ detect drift\n",
      "# 4. **Page-Hinkley**: Algorithm ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö mean shift detection\n",
      "# \n",
      "# ### Comparison:\n",
      "# | Method | Pros | Cons | Best For |\n",
      "# |--------|------|------|----------|\n",
      "# | Sliding Window | Simple, intuitive | Fixed reference | Sudden drift |\n",
      "# | Adaptive | Handles gradual drift | More complex | Production |\n",
      "# | Page-Hinkley | Low memory, fast | Mean shift only | Real-time |\n",
      "\n",
      "#%%\n",
      "print(\"=\" * 60)\n",
      "print(\"‚úÖ LAB 4 COMPLETED!\")\n",
      "print(\"=\" * 60)\n",
      "print(\"\"\"\n",
      "üìö Key Takeaways:\n",
      "1. Different drift types require different detection strategies\n",
      "2. Sliding window is fundamental for streaming detection\n",
      "3. Adaptive reference helps with gradual drift\n",
      "4. Page-Hinkley is efficient for mean shift detection\n",
      "\n",
      "üîú Next: LAB 5 - Custom Metrics & Drift Thresholds\n",
      "\"\"\")\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "## LAB 5: Custom Metrics & Drift Thresholds\n",
      "\n",
      "```python\n",
      "#%% [markdown]\n",
      "# # LAB 5: Custom Metrics & Drift Thresholds\n",
      "# ## ‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á Custom Drift Metrics ‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏±‡∏ö Thresholds\n",
      "# \n",
      "# ### ‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ:\n",
      "# 1. ‡∏™‡∏£‡πâ‡∏≤‡∏á custom drift metrics ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö domain\n",
      "# 2. ‡∏õ‡∏£‡∏±‡∏ö threshold ‡∏ï‡∏≤‡∏° business requirements\n",
      "# 3. Handle false positives/negatives ‡πÉ‡∏ô drift detection\n",
      "#\n",
      "# ### ‡∏ó‡∏§‡∏©‡∏é‡∏µ:\n",
      "# Default thresholds ‡∏≠‡∏≤‡∏à‡πÑ‡∏°‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö‡∏ó‡∏∏‡∏Å use case:\n",
      "# - ‡∏ö‡∏≤‡∏á domain ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ sensitivity ‡∏™‡∏π‡∏á\n",
      "# - ‡∏ö‡∏≤‡∏á domain ‡∏¢‡∏≠‡∏°‡∏£‡∏±‡∏ö drift ‡πÑ‡∏î‡πâ‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏´‡∏ô‡∏∂‡πà‡∏á\n",
      "# - Cost of false positive vs false negative ‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô\n",
      "\n",
      "#%% [markdown]\n",
      "# ## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 1: ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° Environment\n",
      "\n",
      "#%%\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy import stats\n",
      "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
      "import warnings\n",
      "warnings.filterwarnings('ignore')\n",
      "\n",
      "np.random.seed(42)\n",
      "plt.rcParams['figure.figsize'] = (12, 6)\n",
      "\n",
      "print(\"‚úÖ Libraries imported successfully!\")\n",
      "\n",
      "#%% [markdown]\n",
      "# ## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 2: ‡∏™‡∏£‡πâ‡∏≤‡∏á Dataset ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Threshold Tuning\n",
      "# \n",
      "# ‡∏™‡∏£‡πâ‡∏≤‡∏á labeled dataset ‡∏ó‡∏µ‡πà‡∏£‡∏π‡πâ‡∏ß‡πà‡∏≤‡∏°‡∏µ drift ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà\n",
      "\n",
      "#%%\n",
      "def create_labeled_drift_dataset(n_scenarios=100):\n",
      "    \"\"\"\n",
      "    ‡∏™‡∏£‡πâ‡∏≤‡∏á dataset ‡∏û‡∏£‡πâ‡∏≠‡∏° ground truth labels\n",
      "    \n",
      "    Returns:\n",
      "    --------\n",
      "    list of dicts: ‡πÅ‡∏ï‡πà‡∏•‡∏∞ scenario ‡∏°‡∏µ reference, current, ‡πÅ‡∏•‡∏∞ label\n",
      "    \"\"\"\n",
      "    scenarios = []\n",
      "    \n",
      "    for i in range(n_scenarios):\n",
      "        np.random.seed(i)\n",
      "        n_samples = 500\n",
      "        \n",
      "        # Reference data\n",
      "        ref_mean = 50\n",
      "        ref_std = 10\n",
      "        reference = np.random.normal(ref_mean, ref_std, n_samples)\n",
      "        \n",
      "        # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏ß‡πà‡∏≤‡∏°‡∏µ drift ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà\n",
      "        has_drift = i < n_scenarios // 2  # 50% ‡∏°‡∏µ drift\n",
      "        \n",
      "        if has_drift:\n",
      "            # ‡∏™‡∏£‡πâ‡∏≤‡∏á drift ‡πÉ‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ï‡πà‡∏≤‡∏á‡πÜ\n",
      "            drift_level = (i % 5 + 1) * 0.5  # 0.5, 1.0, 1.5, 2.0, 2.5 std\n",
      "            current_mean = ref_mean + drift_level * ref_std\n",
      "            drift_magnitude = drift_level\n",
      "        else:\n",
      "            # ‡πÑ‡∏°‡πà‡∏°‡∏µ drift ‡πÅ‡∏ï‡πà‡∏°‡∏µ noise ‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢\n",
      "            current_mean = ref_mean + np.random.uniform(-0.1, 0.1) * ref_std\n",
      "            drift_magnitude = 0\n",
      "        \n",
      "        current = np.random.normal(current_mean, ref_std, n_samples)\n",
      "        \n",
      "        scenarios.append({\n",
      "            'id': i,\n",
      "            'reference': reference,\n",
      "            'current': current,\n",
      "            'has_drift': has_drift,\n",
      "            'drift_magnitude': drift_magnitude,\n",
      "            'ref_mean': ref_mean,\n",
      "            'current_mean': current_mean\n",
      "        })\n",
      "    \n",
      "    return scenarios\n",
      "\n",
      "# ‡∏™‡∏£‡πâ‡∏≤‡∏á dataset\n",
      "scenarios = create_labeled_drift_dataset(100)\n",
      "\n",
      "print(f\"üìä Created {len(scenarios)} scenarios\")\n",
      "print(f\"   With drift: {sum(1 for s in scenarios if s['has_drift'])}\")\n",
      "print(f\"   Without drift: {sum(1 for s in scenarios if not s['has_drift'])}\")\n",
      "\n",
      "#%%\n",
      "# Visualize drift magnitude distribution\n",
      "drift_mags = [s['drift_magnitude'] for s in scenarios if s['has_drift']]\n",
      "plt.figure(figsize=(10, 4))\n",
      "plt.hist(drift_mags, bins=10, edgecolor='black', alpha=0.7)\n",
      "plt.xlabel('Drift Magnitude (in std)')\n",
      "plt.ylabel('Count')\n",
      "plt.title('Distribution of Drift Magnitudes in Scenarios with Drift')\n",
      "plt.show()\n",
      "\n",
      "#%% [markdown]\n",
      "# ## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 3: ‡∏™‡∏£‡πâ‡∏≤‡∏á Custom Drift Metrics\n",
      "\n",
      "#%%\n",
      "class CustomDriftMetrics:\n",
      "    \"\"\"\n",
      "    Class ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì custom drift metrics\n",
      "    \"\"\"\n",
      "    \n",
      "    @staticmethod\n",
      "    def psi(reference, current, bins=10):\n",
      "        \"\"\"Population Stability Index\"\"\"\n",
      "        breakpoints = np.percentile(reference, np.linspace(0, 100, bins + 1))\n",
      "        breakpoints = np.unique(breakpoints)\n",
      "        \n",
      "        ref_counts, _ = np.histogram(reference, bins=breakpoints)\n",
      "        cur_counts, _ = np.histogram(current, bins=breakpoints)\n",
      "        \n",
      "        eps = 1e-6\n",
      "        ref_props = ref_counts / len(reference) + eps\n",
      "        cur_props = cur_counts / len(current) + eps\n",
      "        \n",
      "        return np.sum((cur_props - ref_props) * np.log(cur_props / ref_props))\n",
      "    \n",
      "    @staticmethod\n",
      "    def normalized_wasserstein(reference, current):\n",
      "        \"\"\"Wasserstein distance normalized by reference std\"\"\"\n",
      "        distance = stats.wasserstein_distance(reference, current)\n",
      "        return distance / np.std(reference)\n",
      "    \n",
      "    @staticmethod\n",
      "    def mean_shift_ratio(reference, current):\n",
      "        \"\"\"Mean shift as ratio of reference std\"\"\"\n",
      "        mean_diff = abs(np.mean(current) - np.mean(reference))\n",
      "        return mean_diff / np.std(reference)\n",
      "    \n",
      "    @staticmethod\n",
      "    def std_ratio(reference, current):\n",
      "        \"\"\"Ratio of standard deviations\"\"\"\n",
      "        return np.std(current) / np.std(reference)\n",
      "    \n",
      "    @staticmethod\n",
      "    def percentile_shift(reference, current, percentiles=[25, 50, 75]):\n",
      "        \"\"\"Average shift in percentiles\"\"\"\n",
      "        shifts = []\n",
      "        ref_std = np.std(reference)\n",
      "        \n",
      "        for p in percentiles:\n",
      "            ref_p = np.percentile(reference, p)\n",
      "            cur_p = np.percentile(current, p)\n",
      "            shifts.append(abs(cur_p - ref_p) / ref_std)\n",
      "        \n",
      "        return np.mean(shifts)\n",
      "    \n",
      "    @staticmethod\n",
      "    def jensen_shannon_divergence(reference, current, bins=10):\n",
      "        \"\"\"Jensen-Shannon divergence\"\"\"\n",
      "        # ‡∏™‡∏£‡πâ‡∏≤‡∏á histograms\n",
      "        all_data = np.concatenate([reference, current])\n",
      "        bins = np.histogram_bin_edges(all_data, bins=bins)\n",
      "        \n",
      "        ref_hist, _ = np.histogram(reference, bins=bins, density=True)\n",
      "        cur_hist, _ = np.histogram(current, bins=bins, density=True)\n",
      "        \n",
      "        # Normalize\n",
      "        ref_hist = ref_hist / (ref_hist.sum() + 1e-10)\n",
      "        cur_hist = cur_hist / (cur_hist.sum() + 1e-10)\n",
      "        \n",
      "        # Average distribution\n",
      "        m = 0.5 * (ref_hist + cur_hist)\n",
      "        \n",
      "        # KL divergences\n",
      "        kl_pm = np.sum(ref_hist * np.log((ref_hist + 1e-10) / (m + 1e-10)))\n",
      "        kl_qm = np.sum(cur_hist * np.log((cur_hist + 1e-10) / (m + 1e-10)))\n",
      "        \n",
      "        return 0.5 * (kl_pm + kl_qm)\n",
      "    \n",
      "    @staticmethod\n",
      "    def combined_score(reference, current, weights=None):\n",
      "        \"\"\"\n",
      "        Combined drift score ‡∏à‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢ metrics\n",
      "        \n",
      "        Default weights: PSI=0.3, Wasserstein=0.3, Mean Shift=0.2, Percentile=0.2\n",
      "        \"\"\"\n",
      "        if weights is None:\n",
      "            weights = {\n",
      "                'psi': 0.3,\n",
      "                'wasserstein': 0.3,\n",
      "                'mean_shift': 0.2,\n",
      "                'percentile': 0.2\n",
      "            }\n",
      "        \n",
      "        metrics = CustomDriftMetrics\n",
      "        \n",
      "        # Normalize each metric to 0-1 range\n",
      "        psi = min(metrics.psi(reference, current), 1.0)  # Cap at 1\n",
      "        wasserstein = min(metrics.normalized_wasserstein(reference, current) / 3, 1.0)  # 3 std = max\n",
      "        mean_shift = min(metrics.mean_shift_ratio(reference, current) / 3, 1.0)\n",
      "        percentile = min(metrics.percentile_shift(reference, current) / 3, 1.0)\n",
      "        \n",
      "        score = (\n",
      "            weights['psi'] * psi +\n",
      "            weights['wasserstein'] * wasserstein +\n",
      "            weights['mean_shift'] * mean_shift +\n",
      "            weights['percentile'] * percentile\n",
      "        )\n",
      "        \n",
      "        return score\n",
      "\n",
      "#%%\n",
      "# ‡∏ó‡∏î‡∏™‡∏≠‡∏ö custom metrics\n",
      "print(\"=\" * 60)\n",
      "print(\"üìä Testing Custom Drift Metrics\")\n",
      "print(\"=\" * 60)\n",
      "\n",
      "# ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å scenarios ‡∏ó‡∏µ‡πà‡∏°‡∏µ drift levels ‡∏ï‡πà‡∏≤‡∏á‡πÜ\n",
      "test_scenarios = [s for s in scenarios if s['drift_magnitude'] in [0, 0.5, 1.0, 2.0]][:8]\n",
      "\n",
      "metrics_results = []\n",
      "for s in test_scenarios:\n",
      "    result = {\n",
      "        'id': s['id'],\n",
      "        'has_drift': s['has_drift'],\n",
      "        'magnitude': s['drift_magnitude'],\n",
      "        'psi': CustomDriftMetrics.psi(s['reference'], s['current']),\n",
      "        'wasserstein': CustomDriftMetrics.normalized_wasserstein(s['reference'], s['current']),\n",
      "        'mean_shift': CustomDriftMetrics.mean_shift_ratio(s['reference'], s['current']),\n",
      "        'percentile': CustomDriftMetrics.percentile_shift(s['reference'], s['current']),\n",
      "        'js_div': CustomDriftMetrics.jensen_shannon_divergence(s['reference'], s['current']),\n",
      "        'combined': CustomDriftMetrics.combined_score(s['reference'], s['current'])\n",
      "    }\n",
      "    metrics_results.append(result)\n",
      "\n",
      "metrics_df = pd.DataFrame(metrics_results)\n",
      "print(metrics_df.to_string(index=False))\n",
      "\n",
      "#%% [markdown]\n",
      "# ## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 4: Threshold Optimization\n",
      "# \n",
      "# ‡∏´‡∏≤ optimal threshold ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ labeled data\n",
      "\n",
      "#%%\n",
      "def calculate_metrics_for_threshold(scenarios, metric_func, threshold):\n",
      "    \"\"\"\n",
      "    ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì precision, recall, f1 ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö threshold ‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î\n",
      "    \"\"\"\n",
      "    y_true = []\n",
      "    y_pred = []\n",
      "    \n",
      "    for s in scenarios:\n",
      "        y_true.append(1 if s['has_drift'] else 0)\n",
      "        \n",
      "        score = metric_func(s['reference'], s['current'])\n",
      "        y_pred.append(1 if score > threshold else 0)\n",
      "    \n",
      "    # Handle edge cases\n",
      "    if sum(y_pred) == 0:\n",
      "        return {'precision': 0, 'recall': 0, 'f1': 0}\n",
      "    \n",
      "    precision = precision_score(y_true, y_pred, zero_division=0)\n",
      "    recall = recall_score(y_true, y_pred, zero_division=0)\n",
      "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
      "    \n",
      "    return {'precision': precision, 'recall': recall, 'f1': f1}\n",
      "\n",
      "def find_optimal_threshold(scenarios, metric_func, thresholds, optimize_for='f1'):\n",
      "    \"\"\"\n",
      "    ‡∏´‡∏≤ optimal threshold\n",
      "    \"\"\"\n",
      "    results = []\n",
      "    \n",
      "    for t in thresholds:\n",
      "        metrics = calculate_metrics_for_threshold(scenarios, metric_func, t)\n",
      "        metrics['threshold'] = t\n",
      "        results.append(metrics)\n",
      "    \n",
      "    results_df = pd.DataFrame(results)\n",
      "    \n",
      "    # ‡∏´‡∏≤ optimal\n",
      "    optimal_idx = results_df[optimize_for].idxmax()\n",
      "    optimal = results_df.iloc[optimal_idx]\n",
      "    \n",
      "    return results_df, optimal\n",
      "\n",
      "#%%\n",
      "# ‡∏´‡∏≤ optimal threshold ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö PSI\n",
      "thresholds = np.linspace(0.01, 0.5, 50)\n",
      "\n",
      "psi_results, psi_optimal = find_optimal_threshold(\n",
      "    scenarios, \n",
      "    CustomDriftMetrics.psi, \n",
      "    thresholds\n",
      ")\n",
      "\n",
      "print(\"=\" * 60)\n",
      "print(\"üìä PSI Threshold Optimization\")\n",
      "print(\"=\" * 60)\n",
      "print(f\"Optimal threshold: {psi_optimal['threshold']:.3f}\")\n",
      "print(f\"Precision: {psi_optimal['precision']:.3f}\")\n",
      "print(f\"Recall: {psi_optimal['recall']:.3f}\")\n",
      "print(f\"F1 Score: {psi_optimal['f1']:.3f}\")\n",
      "\n",
      "#%%\n",
      "# Visualize threshold optimization\n",
      "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
      "\n",
      "metrics_to_plot = ['precision', 'recall', 'f1']\n",
      "colors = ['blue', 'green', 'red']\n",
      "\n",
      "for ax, metric, color in zip(axes, metrics_to_plot, colors):\n",
      "    ax.plot(psi_results['threshold'], psi_results[metric], f'{color}-', linewidth=2)\n",
      "    ax.axvline(psi_optimal['threshold'], color='black', linestyle='--', \n",
      "               label=f'Optimal ({psi_optimal[\"threshold\"]:.3f})')\n",
      "    ax.set_xlabel('PSI Threshold')\n",
      "    ax.set_ylabel(metric.capitalize())\n",
      "    ax.set_title(f'{metric.capitalize()} vs Threshold')\n",
      "    ax.legend()\n",
      "    ax.grid(True, alpha=0.3)\n",
      "\n",
      "plt.suptitle('PSI Threshold Optimization', fontsize=14, fontweight='bold')\n",
      "plt.tight_layout()\n",
      "plt.savefig('threshold_optimization_psi.png', dpi=150, bbox_inches='tight')\n",
      "plt.show()\n",
      "\n",
      "#%%\n",
      "# ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö optimal thresholds ‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞ metric\n",
      "print(\"=\" * 60)\n",
      "print(\"üìä Comparing Optimal Thresholds for Different Metrics\")\n",
      "print(\"=\" * 60)\n",
      "\n",
      "metrics_funcs = {\n",
      "    'PSI': CustomDriftMetrics.psi,\n",
      "    'Normalized Wasserstein': CustomDriftMetrics.normalized_wasserstein,\n",
      "    'Mean Shift Ratio': CustomDriftMetrics.mean_shift_ratio,\n",
      "    'Combined Score': CustomDriftMetrics.combined_score\n",
      "}\n",
      "\n",
      "threshold_ranges = {\n",
      "    'PSI': np.linspace(0.01, 0.5, 50),\n",
      "    'Normalized Wasserstein': np.linspace(0.01, 2.0, 50),\n",
      "    'Mean Shift Ratio': np.linspace(0.01, 2.0, 50),\n",
      "    'Combined Score': np.linspace(0.01, 0.5, 50)\n",
      "}\n",
      "\n",
      "optimal_thresholds = {}\n",
      "for name, func in metrics_funcs.items():\n",
      "    results, optimal = find_optimal_threshold(scenarios, func, threshold_ranges[name])\n",
      "    optimal_thresholds[name] = optimal\n",
      "    print(f\"\\n{name}:\")\n",
      "    print(f\"  Optimal threshold: {optimal['threshold']:.3f}\")\n",
      "    print(f\"  F1 Score: {optimal['f1']:.3f}\")\n",
      "\n",
      "#%% [markdown]\n",
      "# ## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 5: Business-driven Threshold Setting\n",
      "# \n",
      "# ‡∏õ‡∏£‡∏±‡∏ö threshold ‡∏ï‡∏≤‡∏° business requirements\n",
      "\n",
      "#%%\n",
      "class BusinessDriftThreshold:\n",
      "    \"\"\"\n",
      "    Class ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≥‡∏´‡∏ô‡∏î threshold ‡∏ï‡∏≤‡∏° business context\n",
      "    \"\"\"\n",
      "    \n",
      "    def __init__(self, false_positive_cost=1, false_negative_cost=10):\n",
      "        \"\"\"\n",
      "        Parameters:\n",
      "        -----------\n",
      "        false_positive_cost : float - cost ‡∏Ç‡∏≠‡∏á false alarm\n",
      "        false_negative_cost : float - cost ‡∏Ç‡∏≠‡∏á missing drift\n",
      "        \"\"\"\n",
      "        self.fp_cost = false_positive_cost\n",
      "        self.fn_cost = false_negative_cost\n",
      "    \n",
      "    def calculate_total_cost(self, y_true, y_pred):\n",
      "        \"\"\"‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì total cost\"\"\"\n",
      "        tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
      "        return fp * self.fp_cost + fn * self.fn_cost\n",
      "    \n",
      "    def find_cost_optimal_threshold(self, scenarios, metric_func, thresholds):\n",
      "        \"\"\"‡∏´‡∏≤ threshold ‡∏ó‡∏µ‡πà minimize total cost\"\"\"\n",
      "        costs = []\n",
      "        \n",
      "        for t in thresholds:\n",
      "            y_true = [1 if s['has_drift'] else 0 for s in scenarios]\n",
      "            y_pred = [1 if metric_func(s['reference'], s['current']) > t else 0 for s in scenarios]\n",
      "            \n",
      "            cost = self.calculate_total_cost(y_true, y_pred)\n",
      "            costs.append({'threshold': t, 'cost': cost})\n",
      "        \n",
      "        cost_df = pd.DataFrame(costs)\n",
      "        optimal_idx = cost_df['cost'].idxmin()\n",
      "        return cost_df, cost_df.iloc[optimal_idx]\n",
      "\n",
      "#%%\n",
      "# ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö threshold ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö scenarios ‡∏ï‡πà‡∏≤‡∏á‡πÜ\n",
      "print(\"=\" * 60)\n",
      "print(\"üìä Business-driven Threshold Optimization\")\n",
      "print(\"=\" * 60)\n",
      "\n",
      "# Scenario 1: High cost of missing drift (e.g., fraud detection)\n",
      "print(\"\\nüî¥ Scenario 1: High cost of missing drift (FN cost = 10x)\")\n",
      "high_fn_cost = BusinessDriftThreshold(false_positive_cost=1, false_negative_cost=10)\n",
      "cost_df_1, optimal_1 = high_fn_cost.find_cost_optimal_threshold(\n",
      "    scenarios, CustomDriftMetrics.psi, thresholds\n",
      ")\n",
      "print(f\"   Optimal threshold: {optimal_1['threshold']:.3f}\")\n",
      "print(f\"   Total cost: {optimal_1['cost']:.0f}\")\n",
      "\n",
      "# Scenario 2: High cost of false alarms (e.g., model retraining is expensive)\n",
      "print(\"\\nüü° Scenario 2: High cost of false alarms (FP cost = 10x)\")\n",
      "high_fp_cost = BusinessDriftThreshold(false_positive_cost=10, false_negative_cost=1)\n",
      "cost_df_2, optimal_2 = high_fp_cost.find_cost_optimal_threshold(\n",
      "    scenarios, CustomDriftMetrics.psi, thresholds\n",
      ")\n",
      "print(f\"   Optimal threshold: {optimal_2['threshold']:.3f}\")\n",
      "print(f\"   Total cost: {optimal_2['cost']:.0f}\")\n",
      "\n",
      "# Scenario 3: Balanced costs\n",
      "print(\"\\nüü¢ Scenario 3: Balanced costs (FP = FN)\")\n",
      "balanced_cost = BusinessDriftThreshold(false_positive_cost=1, false_negative_cost=1)\n",
      "cost_df_3, optimal_3 = balanced_cost.find_cost_optimal_threshold(\n",
      "    scenarios, CustomDriftMetrics.psi, thresholds\n",
      ")\n",
      "print(f\"   Optimal threshold: {optimal_3['threshold']:.3f}\")\n",
      "print(f\"   Total cost: {optimal_3['cost']:.0f}\")\n",
      "\n",
      "#%%\n",
      "# Visualize cost-based optimization\n",
      "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
      "\n",
      "scenarios_data = [\n",
      "    (cost_df_1, optimal_1, 'High FN Cost (10x)', 'red'),\n",
      "    (cost_df_2, optimal_2, 'High FP Cost (10x)', 'orange'),\n",
      "    (cost_df_3, optimal_3, 'Balanced Cost', 'green')\n",
      "]\n",
      "\n",
      "for ax, (cost_df, optimal, title, color) in zip(axes, scenarios_data):\n",
      "    ax.plot(cost_df['threshold'], cost_df['cost'], f'{color[0]}-', linewidth=2)\n",
      "    ax.axvline(optimal['threshold'], color='black', linestyle='--',\n",
      "               label=f'Optimal ({optimal[\"threshold\"]:.3f})')\n",
      "    ax.scatter([optimal['threshold']], [optimal['cost']], color='black', s=100, zorder=5)\n",
      "    ax.set_xlabel('PSI Threshold')\n",
      "    ax.set_ylabel('Total Cost')\n",
      "    ax.set_title(title)\n",
      "    ax.legend()\n",
      "    ax.grid(True, alpha=0.3)\n",
      "\n",
      "plt.tight_layout()\n",
      "plt.savefig('cost_based_optimization.png', dpi=150, bbox_inches='tight')\n",
      "plt.show()\n",
      "\n",
      "print(\"\\nüí° Insight:\")\n",
      "print(\"   - High FN cost ‚Üí Lower threshold (detect more, accept false alarms)\")\n",
      "print(\"   - High FP cost ‚Üí Higher threshold (be more conservative)\")\n",
      "print(\"   - Balanced ‚Üí Somewhere in between\")\n",
      "\n",
      "#%% [markdown]\n",
      "# ## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 6: Handling False Positives/Negatives\n",
      "\n",
      "#%%\n",
      "class RobustDriftDetector:\n",
      "    \"\"\"\n",
      "    Drift detector ‡∏ó‡∏µ‡πà‡∏°‡∏µ mechanisms ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö handle FP/FN\n",
      "    \"\"\"\n",
      "    \n",
      "    def __init__(self, primary_metric='psi', primary_threshold=0.1,\n",
      "                 confirmation_window=3, use_ensemble=True):\n",
      "        \"\"\"\n",
      "        Parameters:\n",
      "        -----------\n",
      "        primary_metric : str - metric ‡∏´‡∏•‡∏±‡∏Å\n",
      "        primary_threshold : float - threshold ‡∏´‡∏•‡∏±‡∏Å\n",
      "        confirmation_window : int - ‡∏ï‡πâ‡∏≠‡∏á detect ‡∏Å‡∏µ‡πà‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ï‡∏¥‡∏î‡∏Å‡∏±‡∏ô\n",
      "        use_ensemble : bool - ‡πÉ‡∏ä‡πâ‡∏´‡∏•‡∏≤‡∏¢ metrics ‡∏£‡πà‡∏ß‡∏°‡∏Å‡∏±‡∏ô\n",
      "        \"\"\"\n",
      "        self.primary_metric = primary_metric\n",
      "        self.primary_threshold = primary_threshold\n",
      "        self.confirmation_window = confirmation_window\n",
      "        self.use_ensemble = use_ensemble\n",
      "        \n",
      "        self.metrics = CustomDriftMetrics()\n",
      "        \n",
      "        # Thresholds for ensemble\n",
      "        self.thresholds = {\n",
      "            'psi': 0.1,\n",
      "            'wasserstein': 0.5,\n",
      "            'mean_shift': 0.5,\n",
      "            'percentile': 0.3\n",
      "        }\n",
      "        \n",
      "        self.consecutive_count = 0\n",
      "        self.history = []\n",
      "    \n",
      "    def _calculate_all_metrics(self, reference, current):\n",
      "        \"\"\"‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏ó‡∏∏‡∏Å metrics\"\"\"\n",
      "        return {\n",
      "            'psi': self.metrics.psi(reference, current),\n",
      "            'wasserstein': self.metrics.normalized_wasserstein(reference, current),\n",
      "            'mean_shift': self.metrics.mean_shift_ratio(reference, current),\n",
      "            'percentile': self.metrics.percentile_shift(reference, current)\n",
      "        }\n",
      "    \n",
      "    def detect(self, reference, current):\n",
      "        \"\"\"\n",
      "        Detect drift with robustness mechanisms\n",
      "        \"\"\"\n",
      "        all_metrics = self._calculate_all_metrics(reference, current)\n",
      "        \n",
      "        # Single metric detection\n",
      "        primary_value = all_metrics[self.primary_metric]\n",
      "        primary_drift = primary_value > self.primary_threshold\n",
      "        \n",
      "        if self.use_ensemble:\n",
      "            # Ensemble: majority voting\n",
      "            drift_votes = sum(\n",
      "                1 for m, v in all_metrics.items() \n",
      "                if v > self.thresholds.get(m, 0.5)\n",
      "            )\n",
      "            ensemble_drift = drift_votes >= 3  # ‡∏ï‡πâ‡∏≠‡∏á 3/4 metrics agree\n",
      "        else:\n",
      "            ensemble_drift = primary_drift\n",
      "        \n",
      "        # Confirmation mechanism\n",
      "        if ensemble_drift:\n",
      "            self.consecutive_count += 1\n",
      "        else:\n",
      "            self.consecutive_count = 0\n",
      "        \n",
      "        confirmed_drift = self.consecutive_count >= self.confirmation_window\n",
      "        \n",
      "        result = {\n",
      "            'metrics': all_metrics,\n",
      "            'primary_drift': primary_drift,\n",
      "            'ensemble_drift': ensemble_drift,\n",
      "            'consecutive_count': self.consecutive_count,\n",
      "            'confirmed_drift': confirmed_drift\n",
      "        }\n",
      "        \n",
      "        self.history.append(result)\n",
      "        \n",
      "        return result\n",
      "    \n",
      "    def evaluate(self, scenarios):\n",
      "        \"\"\"\n",
      "        Evaluate detector performance on labeled scenarios\n",
      "        \"\"\"\n",
      "        y_true = []\n",
      "        y_pred_primary = []\n",
      "        y_pred_ensemble = []\n",
      "        y_pred_confirmed = []\n",
      "        \n",
      "        for s in scenarios:\n",
      "            y_true.append(1 if s['has_drift'] else 0)\n",
      "            \n",
      "            result = self.detect(s['reference'], s['current'])\n",
      "            y_pred_primary.append(1 if result['primary_drift'] else 0)\n",
      "            y_pred_ensemble.append(1 if result['ensemble_drift'] else 0)\n",
      "            y_pred_confirmed.append(1 if result['confirmed_drift'] else 0)\n",
      "            \n",
      "            # Reset for next scenario\n",
      "            self.consecutive_count = 0\n",
      "        \n",
      "        evaluations = {}\n",
      "        for name, y_pred in [\n",
      "            ('primary', y_pred_primary),\n",
      "            ('ensemble', y_pred_ensemble),\n",
      "            ('confirmed', y_pred_confirmed)\n",
      "        ]:\n",
      "            evaluations[name] = {\n",
      "                'precision': precision_score(y_true, y_pred, zero_division=0),\n",
      "                'recall': recall_score(y_true, y_pred, zero_division=0),\n",
      "                'f1': f1_score(y_true, y_pred, zero_division=0)\n",
      "            }\n",
      "        \n",
      "        return evaluations\n",
      "\n",
      "#%%\n",
      "# ‡∏ó‡∏î‡∏™‡∏≠‡∏ö RobustDriftDetector\n",
      "print(\"=\" * 60)\n",
      "print(\"üìä Evaluating Robust Drift Detector\")\n",
      "print(\"=\" * 60)\n",
      "\n",
      "detector = RobustDriftDetector(\n",
      "    primary_metric='psi',\n",
      "    primary_threshold=0.1,\n",
      "    confirmation_window=1,  # ‡∏•‡∏î‡πÄ‡∏´‡∏•‡∏∑‡∏≠ 1 ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö single scenario testing\n",
      "    use_ensemble=True\n",
      ")\n",
      "\n",
      "evaluations = detector.evaluate(scenarios)\n",
      "\n",
      "for method, metrics in evaluations.items():\n",
      "    print(f\"\\n{method.upper()} Method:\")\n",
      "    print(f\"  Precision: {metrics['precision']:.3f}\")\n",
      "    print(f\"  Recall: {metrics['recall']:.3f}\")\n",
      "    print(f\"  F1 Score: {metrics['f1']:.3f}\")\n",
      "\n",
      "#%%\n",
      "# Visualize comparison\n",
      "fig, ax = plt.subplots(figsize=(10, 6))\n",
      "\n",
      "methods = list(evaluations.keys())\n",
      "x = np.arange(len(methods))\n",
      "width = 0.25\n",
      "\n",
      "metrics_names = ['precision', 'recall', 'f1']\n",
      "colors = ['blue', 'green', 'red']\n",
      "\n",
      "for i, (metric, color) in enumerate(zip(metrics_names, colors)):\n",
      "    values = [evaluations[m][metric] for m in methods]\n",
      "    ax.bar(x + i * width, values, width, label=metric.capitalize(), color=color, alpha=0.7)\n",
      "\n",
      "ax.set_ylabel('Score')\n",
      "ax.set_xlabel('Detection Method')\n",
      "ax.set_title('Comparison of Detection Methods')\n",
      "ax.set_xticks(x + width)\n",
      "ax.set_xticklabels([m.capitalize() for m in methods])\n",
      "ax.legend()\n",
      "ax.set_ylim(0, 1)\n",
      "\n",
      "for i, (metric, color) in enumerate(zip(metrics_names, colors)):\n",
      "    values = [evaluations[m][metric] for m in methods]\n",
      "    for j, v in enumerate(values):\n",
      "        ax.text(x[j] + i * width, v + 0.02, f'{v:.2f}', ha='center', fontsize=9)\n",
      "\n",
      "plt.tight_layout()\n",
      "plt.savefig('detection_methods_comparison.png', dpi=150, bbox_inches='tight')\n",
      "plt.show()\n",
      "\n",
      "#%% [markdown]\n",
      "# ## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 7: ‡∏™‡∏£‡∏∏‡∏õ‡πÅ‡∏•‡∏∞ Best Practices\n",
      "\n",
      "#%%\n",
      "print(\"=\" * 70)\n",
      "print(\"üìã THRESHOLD SETTING BEST PRACTICES\")\n",
      "print(\"=\" * 70)\n",
      "\n",
      "best_practices = \"\"\"\n",
      "1Ô∏è‚É£ DOMAIN-SPECIFIC THRESHOLDS\n",
      "   - ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ default thresholds ‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà validate\n",
      "   - ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏±‡∏ö labeled data ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ\n",
      "   - ‡∏õ‡∏£‡∏∂‡∏Å‡∏©‡∏≤‡∏Å‡∏±‡∏ö domain experts\n",
      "\n",
      "2Ô∏è‚É£ COST-BASED OPTIMIZATION\n",
      "   - ‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤ cost ‡∏Ç‡∏≠‡∏á FP vs FN\n",
      "   - FN ‡πÅ‡∏û‡∏á ‚Üí Lower threshold (more sensitive)\n",
      "   - FP ‡πÅ‡∏û‡∏á ‚Üí Higher threshold (more conservative)\n",
      "\n",
      "3Ô∏è‚É£ ENSEMBLE APPROACH\n",
      "   - ‡πÉ‡∏ä‡πâ‡∏´‡∏•‡∏≤‡∏¢ metrics ‡∏£‡πà‡∏ß‡∏°‡∏Å‡∏±‡∏ô\n",
      "   - Voting mechanism ‡∏•‡∏î false positives\n",
      "   - ‡∏ñ‡πâ‡∏≤ metrics ‡πÑ‡∏°‡πà agree ‚Üí investigate further\n",
      "\n",
      "4Ô∏è‚É£ CONFIRMATION MECHANISM\n",
      "   - Require consecutive detections\n",
      "   - ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô temporary spikes\n",
      "   - Trade-off: delay detection\n",
      "\n",
      "5Ô∏è‚É£ PERIODIC REVIEW\n",
      "   - Review thresholds ‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏∞‡∏¢‡∏∞\n",
      "   - Data patterns ‡∏≠‡∏≤‡∏à‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô\n",
      "   - Business requirements ‡∏≠‡∏≤‡∏à‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô\n",
      "\n",
      "6Ô∏è‚É£ MONITORING & ALERTING\n",
      "   - Different severity levels\n",
      "   - Different thresholds for warning vs critical\n",
      "   - Escalation procedures\n",
      "\"\"\"\n",
      "\n",
      "print(best_practices)\n",
      "\n",
      "#%% [markdown]\n",
      "# ## ‡∏™‡∏£‡∏∏‡∏õ LAB 5\n",
      "# \n",
      "# ### ‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ:\n",
      "# 1. **Custom Metrics**: PSI, Wasserstein, Mean Shift, Combined Score\n",
      "# 2. **Threshold Optimization**: ‡πÉ‡∏ä‡πâ labeled data ‡∏´‡∏≤ optimal threshold\n",
      "# 3. **Cost-based Approach**: ‡∏õ‡∏£‡∏±‡∏ö threshold ‡∏ï‡∏≤‡∏° FP/FN costs\n",
      "# 4. **Robust Detection**: Ensemble + Confirmation mechanisms\n",
      "\n",
      "#%%\n",
      "print(\"=\" * 60)\n",
      "print(\"‚úÖ LAB 5 COMPLETED!\")\n",
      "print(\"=\" * 60)\n",
      "print(\"\"\"\n",
      "üìö Key Takeaways:\n",
      "1. Default thresholds rarely optimal for your use case\n",
      "2. Use labeled data to optimize thresholds\n",
      "3. Consider business costs of FP vs FN\n",
      "4. Ensemble methods reduce false alarms\n",
      "5. Confirmation window adds robustness\n",
      "\n",
      "üîú Next: LAB 6 - End-to-End Monitoring Pipeline\n",
      "\"\"\")\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "## LAB 6: End-to-End Monitoring Pipeline\n",
      "\n",
      "```python\n",
      "#%% [markdown]\n",
      "# # LAB 6: End-to-End Monitoring Pipeline\n",
      "# ## ‡∏™‡∏£‡πâ‡∏≤‡∏á Pipeline ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Drift Monitoring ‡πÅ‡∏ö‡∏ö‡∏Ñ‡∏£‡∏ö‡∏ß‡∏á‡∏à‡∏£\n",
      "# \n",
      "# ### ‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ:\n",
      "# 1. ‡∏£‡∏ß‡∏°‡∏ó‡∏∏‡∏Å components ‡πÄ‡∏Ç‡πâ‡∏≤‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏±‡∏ô\n",
      "# 2. ‡∏™‡∏£‡πâ‡∏≤‡∏á automated monitoring workflow\n",
      "# 3. Integrate ‡∏Å‡∏±‡∏ö MLflow ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö experiment tracking\n",
      "#\n",
      "# ### ‡∏ó‡∏§‡∏©‡∏é‡∏µ:\n",
      "# Production ML monitoring ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£:\n",
      "# - Automated data ingestion\n",
      "# - Real-time drift detection\n",
      "# - Alerting mechanisms\n",
      "# - Experiment tracking\n",
      "# - Dashboard ‡πÅ‡∏•‡∏∞ reporting\n",
      "\n",
      "#%% [markdown]\n",
      "# ## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 1: ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° Environment\n",
      "\n",
      "#%%\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "from datetime import datetime, timedelta\n",
      "from scipy import stats\n",
      "import json\n",
      "import os\n",
      "import logging\n",
      "from collections import deque\n",
      "import warnings\n",
      "warnings.filterwarnings('ignore')\n",
      "\n",
      "# Setup logging\n",
      "logging.basicConfig(\n",
      "    level=logging.INFO,\n",
      "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
      ")\n",
      "logger = logging.getLogger(__name__)\n",
      "\n",
      "np.random.seed(42)\n",
      "plt.rcParams['figure.figsize'] = (14, 6)\n",
      "\n",
      "print(\"‚úÖ Libraries imported successfully!\")\n",
      "\n",
      "# ‡∏™‡∏£‡πâ‡∏≤‡∏á directory ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö output\n",
      "os.makedirs('monitoring_output', exist_ok=True)\n",
      "os.makedirs('monitoring_output/reports', exist_ok=True)\n",
      "os.makedirs('monitoring_output/alerts', exist_ok=True)\n",
      "\n",
      "#%% [markdown]\n",
      "# ## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 2: ‡∏™‡∏£‡πâ‡∏≤‡∏á Data Classes ‡πÅ‡∏•‡∏∞ Utilities\n",
      "\n",
      "#%%\n",
      "from dataclasses import dataclass, field\n",
      "from typing import List, Dict, Optional, Callable\n",
      "from enum import Enum\n",
      "\n",
      "class AlertSeverity(Enum):\n",
      "    INFO = \"info\"\n",
      "    WARNING = \"warning\"\n",
      "    CRITICAL = \"critical\"\n",
      "\n",
      "class DriftType(Enum):\n",
      "    NONE = \"none\"\n",
      "    MILD = \"mild\"\n",
      "    MODERATE = \"moderate\"\n",
      "    SEVERE = \"severe\"\n",
      "\n",
      "@dataclass\n",
      "class DriftResult:\n",
      "    \"\"\"‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö drift\"\"\"\n",
      "    timestamp: datetime\n",
      "    feature: str\n",
      "    drift_detected: bool\n",
      "    drift_type: DriftType\n",
      "    psi: float\n",
      "    ks_statistic: float\n",
      "    ks_pvalue: float\n",
      "    reference_mean: float\n",
      "    current_mean: float\n",
      "    reference_std: float\n",
      "    current_std: float\n",
      "    \n",
      "    def to_dict(self):\n",
      "        return {\n",
      "            'timestamp': self.timestamp.isoformat(),\n",
      "            'feature': self.feature,\n",
      "            'drift_detected': self.drift_detected,\n",
      "            'drift_type': self.drift_type.value,\n",
      "            'psi': self.psi,\n",
      "            'ks_statistic': self.ks_statistic,\n",
      "            'ks_pvalue': self.ks_pvalue,\n",
      "            'reference_mean': self.reference_mean,\n",
      "            'current_mean': self.current_mean,\n",
      "            'reference_std': self.reference_std,\n",
      "            'current_std': self.current_std\n",
      "        }\n",
      "\n",
      "@dataclass\n",
      "class Alert:\n",
      "    \"\"\"Alert object\"\"\"\n",
      "    timestamp: datetime\n",
      "    severity: AlertSeverity\n",
      "    message: str\n",
      "    details: Dict\n",
      "    acknowledged: bool = False\n",
      "    \n",
      "    def to_dict(self):\n",
      "        return {\n",
      "            'timestamp': self.timestamp.isoformat(),\n",
      "            'severity': self.severity.value,\n",
      "            'message': self.message,\n",
      "            'details': self.details,\n",
      "            'acknowledged': self.acknowledged\n",
      "        }\n",
      "\n",
      "@dataclass\n",
      "class MonitoringConfig:\n",
      "    \"\"\"Configuration ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö monitoring\"\"\"\n",
      "    reference_window_size: int = 1000\n",
      "    current_window_size: int = 200\n",
      "    psi_mild_threshold: float = 0.1\n",
      "    psi_moderate_threshold: float = 0.2\n",
      "    psi_severe_threshold: float = 0.25\n",
      "    ks_significance: float = 0.05\n",
      "    check_interval_seconds: int = 60\n",
      "    alert_cooldown_minutes: int = 30\n",
      "    features_to_monitor: List[str] = field(default_factory=list)\n",
      "\n",
      "#%% [markdown]\n",
      "# ## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 3: ‡∏™‡∏£‡πâ‡∏≤‡∏á Core Monitoring Components\n",
      "\n",
      "#%%\n",
      "class DriftCalculator:\n",
      "    \"\"\"\n",
      "    Component ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì drift metrics\n",
      "    \"\"\"\n",
      "    \n",
      "    @staticmethod\n",
      "    def calculate_psi(reference: np.ndarray, current: np.ndarray, bins: int = 10) -> float:\n",
      "        \"\"\"Calculate Population Stability Index\"\"\"\n",
      "        breakpoints = np.percentile(reference, np.linspace(0, 100, bins + 1))\n",
      "        breakpoints = np.unique(breakpoints)\n",
      "        \n",
      "        if len(breakpoints) < 2:\n",
      "            return 0.0\n",
      "        \n",
      "        ref_counts, _ = np.histogram(reference, bins=breakpoints)\n",
      "        cur_counts, _ = np.histogram(current, bins=breakpoints)\n",
      "        \n",
      "        eps = 1e-6\n",
      "        ref_props = ref_counts / len(reference) + eps\n",
      "        cur_props = cur_counts / len(current) + eps\n",
      "        \n",
      "        psi = np.sum((cur_props - ref_props) * np.log(cur_props / ref_props))\n",
      "        return float(psi)\n",
      "    \n",
      "    @staticmethod\n",
      "    def calculate_ks_test(reference: np.ndarray, current: np.ndarray) -> tuple:\n",
      "        \"\"\"Calculate Kolmogorov-Smirnov test\"\"\"\n",
      "        statistic, pvalue = stats.ks_2samp(reference, current)\n",
      "        return float(statistic), float(pvalue)\n",
      "    \n",
      "    @staticmethod\n",
      "    def determine_drift_type(psi: float, config: MonitoringConfig) -> DriftType:\n",
      "        \"\"\"Determine drift severity based on PSI\"\"\"\n",
      "        if psi >= config.psi_severe_threshold:\n",
      "            return DriftType.SEVERE\n",
      "        elif psi >= config.psi_moderate_threshold:\n",
      "            return DriftType.MODERATE\n",
      "        elif psi >= config.psi_mild_threshold:\n",
      "            return DriftType.MILD\n",
      "        return DriftType.NONE\n",
      "\n",
      "#%%\n",
      "class DataBuffer:\n",
      "    \"\"\"\n",
      "    Buffer ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏Å‡πá‡∏ö reference ‡πÅ‡∏•‡∏∞ current data\n",
      "    \"\"\"\n",
      "    \n",
      "    def __init__(self, config: MonitoringConfig):\n",
      "        self.config = config\n",
      "        self.reference_data: Dict[str, deque] = {}\n",
      "        self.current_data: Dict[str, deque] = {}\n",
      "        self.is_initialized = False\n",
      "        \n",
      "    def initialize(self, reference_df: pd.DataFrame):\n",
      "        \"\"\"Initialize with reference data\"\"\"\n",
      "        for feature in self.config.features_to_monitor:\n",
      "            if feature in reference_df.columns:\n",
      "                self.reference_data[feature] = deque(\n",
      "                    reference_df[feature].values[-self.config.reference_window_size:],\n",
      "                    maxlen=self.config.reference_window_size\n",
      "                )\n",
      "                self.current_data[feature] = deque(\n",
      "                    maxlen=self.config.current_window_size\n",
      "                )\n",
      "        self.is_initialized = True\n",
      "        logger.info(f\"DataBuffer initialized with {len(self.reference_data)} features\")\n",
      "    \n",
      "    def add_data(self, data: Dict[str, float]):\n",
      "        \"\"\"Add new data point\"\"\"\n",
      "        for feature, value in data.items():\n",
      "            if feature in self.current_data:\n",
      "                self.current_data[feature].append(value)\n",
      "    \n",
      "    def get_reference(self, feature: str) -> Optional[np.ndarray]:\n",
      "        \"\"\"Get reference data for a feature\"\"\"\n",
      "        if feature in self.reference_data:\n",
      "            return np.array(self.reference_data[feature])\n",
      "        return None\n",
      "    \n",
      "    def get_current(self, feature: str) -> Optional[np.ndarray]:\n",
      "        \"\"\"Get current data for a feature\"\"\"\n",
      "        if feature in self.current_data:\n",
      "            return np.array(self.current_data[feature])\n",
      "        return None\n",
      "    \n",
      "    def is_current_ready(self) -> bool:\n",
      "        \"\"\"Check if current buffer has enough data\"\"\"\n",
      "        for feature in self.config.features_to_monitor:\n",
      "            if feature in self.current_data:\n",
      "                if len(self.current_data[feature]) < self.config.current_window_size:\n",
      "                    return False\n",
      "        return True\n",
      "    \n",
      "    def update_reference(self):\n",
      "        \"\"\"Update reference with current data\"\"\"\n",
      "        for feature in self.config.features_to_monitor:\n",
      "            if feature in self.current_data and len(self.current_data[feature]) > 0:\n",
      "                # Add current data to reference\n",
      "                for val in self.current_data[feature]:\n",
      "                    self.reference_data[feature].append(val)\n",
      "                self.current_data[feature].clear()\n",
      "        logger.info(\"Reference data updated with current window\")\n",
      "\n",
      "#%%\n",
      "class AlertManager:\n",
      "    \"\"\"\n",
      "    Component ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ alerts\n",
      "    \"\"\"\n",
      "    \n",
      "    def __init__(self, config: MonitoringConfig):\n",
      "        self.config = config\n",
      "        self.alerts: List[Alert] = []\n",
      "        self.last_alert_time: Dict[str, datetime] = {}\n",
      "    \n",
      "    def should_alert(self, feature: str) -> bool:\n",
      "        \"\"\"Check if we should send alert (cooldown)\"\"\"\n",
      "        if feature not in self.last_alert_time:\n",
      "            return True\n",
      "        \n",
      "        elapsed = datetime.now() - self.last_alert_time[feature]\n",
      "        return elapsed > timedelta(minutes=self.config.alert_cooldown_minutes)\n",
      "    \n",
      "    def create_alert(self, drift_result: DriftResult) -> Optional[Alert]:\n",
      "        \"\"\"Create alert based on drift result\"\"\"\n",
      "        if not drift_result.drift_detected:\n",
      "            return None\n",
      "        \n",
      "        if not self.should_alert(drift_result.feature):\n",
      "            return None\n",
      "        \n",
      "        # Determine severity\n",
      "        if drift_result.drift_type == DriftType.SEVERE:\n",
      "            severity = AlertSeverity.CRITICAL\n",
      "        elif drift_result.drift_type == DriftType.MODERATE:\n",
      "            severity = AlertSeverity.WARNING\n",
      "        else:\n",
      "            severity = AlertSeverity.INFO\n",
      "        \n",
      "        message = (\n",
      "            f\"Drift detected in feature '{drift_result.feature}': \"\n",
      "            f\"PSI={drift_result.psi:.4f}, Type={drift_result.drift_type.value}\"\n",
      "        )\n",
      "        \n",
      "        alert = Alert(\n",
      "            timestamp=drift_result.timestamp,\n",
      "            severity=severity,\n",
      "            message=message,\n",
      "            details=drift_result.to_dict()\n",
      "        )\n",
      "        \n",
      "        self.alerts.append(alert)\n",
      "        self.last_alert_time[drift_result.feature] = datetime.now()\n",
      "        \n",
      "        # Log based on severity\n",
      "        if severity == AlertSeverity.CRITICAL:\n",
      "            logger.critical(message)\n",
      "        elif severity == AlertSeverity.WARNING:\n",
      "            logger.warning(message)\n",
      "        else:\n",
      "            logger.info(message)\n",
      "        \n",
      "        return alert\n",
      "    \n",
      "    def get_active_alerts(self) -> List[Alert]:\n",
      "        \"\"\"Get all unacknowledged alerts\"\"\"\n",
      "        return [a for a in self.alerts if not a.acknowledged]\n",
      "    \n",
      "    def acknowledge_alert(self, index: int):\n",
      "        \"\"\"Acknowledge an alert\"\"\"\n",
      "        if 0 <= index < len(self.alerts):\n",
      "            self.alerts[index].acknowledged = True\n",
      "    \n",
      "    def save_alerts(self, filepath: str):\n",
      "        \"\"\"Save alerts to file\"\"\"\n",
      "        alerts_data = [a.to_dict() for a in self.alerts]\n",
      "        with open(filepath, 'w') as f:\n",
      "            json.dump(alerts_data, f, indent=2)\n",
      "\n",
      "#%% [markdown]\n",
      "# ## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 4: ‡∏™‡∏£‡πâ‡∏≤‡∏á Main Monitoring Pipeline\n",
      "\n",
      "#%%\n",
      "class DriftMonitoringPipeline:\n",
      "    \"\"\"\n",
      "    Main pipeline ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö drift monitoring\n",
      "    \"\"\"\n",
      "    \n",
      "    def __init__(self, config: MonitoringConfig):\n",
      "        self.config = config\n",
      "        self.data_buffer = DataBuffer(config)\n",
      "        self.alert_manager = AlertManager(config)\n",
      "        self.drift_calculator = DriftCalculator()\n",
      "        self.results_history: List[DriftResult] = []\n",
      "        self.is_running = False\n",
      "        \n",
      "    def initialize(self, reference_data: pd.DataFrame):\n",
      "        \"\"\"Initialize pipeline with reference data\"\"\"\n",
      "        self.data_buffer.initialize(reference_data)\n",
      "        logger.info(\"DriftMonitoringPipeline initialized\")\n",
      "    \n",
      "    def process_batch(self, batch_data: pd.DataFrame) -> List[DriftResult]:\n",
      "        \"\"\"Process a batch of new data\"\"\"\n",
      "        results = []\n",
      "        \n",
      "        # Add data to buffer\n",
      "        for idx, row in batch_data.iterrows():\n",
      "            data_point = {f: row[f] for f in self.config.features_to_monitor if f in row}\n",
      "            self.data_buffer.add_data(data_point)\n",
      "        \n",
      "        # Check if ready for drift detection\n",
      "        if not self.data_buffer.is_current_ready():\n",
      "            logger.debug(\"Current buffer not ready yet\")\n",
      "            return results\n",
      "        \n",
      "        # Perform drift detection for each feature\n",
      "        for feature in self.config.features_to_monitor:\n",
      "            result = self._detect_drift_for_feature(feature)\n",
      "            if result:\n",
      "                results.append(result)\n",
      "                self.results_history.append(result)\n",
      "                \n",
      "                # Create alert if needed\n",
      "                if result.drift_detected:\n",
      "                    self.alert_manager.create_alert(result)\n",
      "        \n",
      "        return results\n",
      "    \n",
      "    def _detect_drift_for_feature(self, feature: str) -> Optional[DriftResult]:\n",
      "        \"\"\"Detect drift for a single feature\"\"\"\n",
      "        reference = self.data_buffer.get_reference(feature)\n",
      "        current = self.data_buffer.get_current(feature)\n",
      "        \n",
      "        if reference is None or current is None:\n",
      "            return None\n",
      "        \n",
      "        if len(current) < 10:  # Need minimum samples\n",
      "            return None\n",
      "        \n",
      "        # Calculate metrics\n",
      "        psi = self.drift_calculator.calculate_psi(reference, current)\n",
      "        ks_stat, ks_pval = self.drift_calculator.calculate_ks_test(reference, current)\n",
      "        \n",
      "        # Determine drift type\n",
      "        drift_type = self.drift_calculator.determine_drift_type(psi, self.config)\n",
      "        drift_detected = drift_type != DriftType.NONE or ks_pval < self.config.ks_significance\n",
      "        \n",
      "        return DriftResult(\n",
      "            timestamp=datetime.now(),\n",
      "            feature=feature,\n",
      "            drift_detected=drift_detected,\n",
      "            drift_type=drift_type,\n",
      "            psi=psi,\n",
      "            ks_statistic=ks_stat,\n",
      "            ks_pvalue=ks_pval,\n",
      "            reference_mean=float(np.mean(reference)),\n",
      "            current_mean=float(np.mean(current)),\n",
      "            reference_std=float(np.std(reference)),\n",
      "            current_std=float(np.std(current))\n",
      "        )\n",
      "    \n",
      "    def get_summary_report(self) -> Dict:\n",
      "        \"\"\"Generate summary report\"\"\"\n",
      "        if not self.results_history:\n",
      "            return {'status': 'no_data'}\n",
      "        \n",
      "        # Group by feature\n",
      "        feature_summary = {}\n",
      "        for feature in self.config.features_to_monitor:\n",
      "            feature_results = [r for r in self.results_history if r.feature == feature]\n",
      "            if feature_results:\n",
      "                latest = feature_results[-1]\n",
      "                drift_count = sum(1 for r in feature_results if r.drift_detected)\n",
      "                feature_summary[feature] = {\n",
      "                    'latest_psi': latest.psi,\n",
      "                    'latest_drift_type': latest.drift_type.value,\n",
      "                    'drift_count': drift_count,\n",
      "                    'total_checks': len(feature_results),\n",
      "                    'drift_rate': drift_count / len(feature_results)\n",
      "                }\n",
      "        \n",
      "        return {\n",
      "            'timestamp': datetime.now().isoformat(),\n",
      "            'total_checks': len(self.results_history),\n",
      "            'total_drifts_detected': sum(1 for r in self.results_history if r.drift_detected),\n",
      "            'active_alerts': len(self.alert_manager.get_active_alerts()),\n",
      "            'feature_summary': feature_summary\n",
      "        }\n",
      "    \n",
      "    def save_results(self, output_dir: str = 'monitoring_output'):\n",
      "        \"\"\"Save all results to files\"\"\"\n",
      "        # Save drift results\n",
      "        results_data = [r.to_dict() for r in self.results_history]\n",
      "        with open(f'{output_dir}/drift_results.json', 'w') as f:\n",
      "            json.dump(results_data, f, indent=2)\n",
      "        \n",
      "        # Save alerts\n",
      "        self.alert_manager.save_alerts(f'{output_dir}/alerts/alerts.json')\n",
      "        \n",
      "        # Save summary\n",
      "        summary = self.get_summary_report()\n",
      "        with open(f'{output_dir}/reports/summary.json', 'w') as f:\n",
      "            json.dump(summary, f, indent=2)\n",
      "        \n",
      "        logger.info(f\"Results saved to {output_dir}\")\n",
      "\n",
      "#%% [markdown]\n",
      "# ## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 5: ‡∏™‡∏£‡πâ‡∏≤‡∏á Report Generator\n",
      "\n",
      "#%%\n",
      "class ReportGenerator:\n",
      "    \"\"\"\n",
      "    Component ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡∏£‡πâ‡∏≤‡∏á reports ‡πÅ‡∏•‡∏∞ visualizations\n",
      "    \"\"\"\n",
      "    \n",
      "    def __init__(self, pipeline: DriftMonitoringPipeline):\n",
      "        self.pipeline = pipeline\n",
      "    \n",
      "    def generate_dashboard_data(self) -> Dict:\n",
      "        \"\"\"Generate data for dashboard\"\"\"\n",
      "        summary = self.pipeline.get_summary_report()\n",
      "        \n",
      "        # Time series data for each feature\n",
      "        time_series = {}\n",
      "        for feature in self.pipeline.config.features_to_monitor:\n",
      "            feature_results = [\n",
      "                r for r in self.pipeline.results_history \n",
      "                if r.feature == feature\n",
      "            ]\n",
      "            time_series[feature] = {\n",
      "                'timestamps': [r.timestamp.isoformat() for r in feature_results],\n",
      "                'psi_values': [r.psi for r in feature_results],\n",
      "                'drift_types': [r.drift_type.value for r in feature_results]\n",
      "            }\n",
      "        \n",
      "        return {\n",
      "            'summary': summary,\n",
      "            'time_series': time_series,\n",
      "            'alerts': [a.to_dict() for a in self.pipeline.alert_manager.get_active_alerts()]\n",
      "        }\n",
      "    \n",
      "    def plot_drift_trends(self, save_path: str = None):\n",
      "        \"\"\"Plot drift trends for all features\"\"\"\n",
      "        n_features = len(self.pipeline.config.features_to_monitor)\n",
      "        \n",
      "        if n_features == 0:\n",
      "            logger.warning(\"No features to plot\")\n",
      "            return\n",
      "        \n",
      "        fig, axes = plt.subplots(n_features, 1, figsize=(14, 4*n_features), sharex=True)\n",
      "        \n",
      "        if n_features == 1:\n",
      "            axes = [axes]\n",
      "        \n",
      "        colors = {'none': 'green', 'mild': 'yellow', 'moderate': 'orange', 'severe': 'red'}\n",
      "        \n",
      "        for ax, feature in zip(axes, self.pipeline.config.features_to_monitor):\n",
      "            feature_results = [\n",
      "                r for r in self.pipeline.results_history \n",
      "                if r.feature == feature\n",
      "            ]\n",
      "            \n",
      "            if not feature_results:\n",
      "                continue\n",
      "            \n",
      "            timestamps = [r.timestamp for r in feature_results]\n",
      "            psi_values = [r.psi for r in feature_results]\n",
      "            drift_types = [r.drift_type.value for r in feature_results]\n",
      "            \n",
      "            # Plot PSI\n",
      "            scatter_colors = [colors.get(dt, 'gray') for dt in drift_types]\n",
      "            ax.scatter(timestamps, psi_values, c=scatter_colors, s=30, alpha=0.7)\n",
      "            ax.plot(timestamps, psi_values, 'b-', alpha=0.3)\n",
      "            \n",
      "            # Add thresholds\n",
      "            ax.axhline(self.pipeline.config.psi_mild_threshold, \n",
      "                      color='yellow', linestyle='--', alpha=0.7, label='Mild')\n",
      "            ax.axhline(self.pipeline.config.psi_moderate_threshold, \n",
      "                      color='orange', linestyle='--', alpha=0.7, label='Moderate')\n",
      "            ax.axhline(self.pipeline.config.psi_severe_threshold, \n",
      "                      color='red', linestyle='--', alpha=0.7, label='Severe')\n",
      "            \n",
      "            ax.set_ylabel(f'{feature}\\nPSI')\n",
      "            ax.legend(loc='upper right')\n",
      "            ax.grid(True, alpha=0.3)\n",
      "        \n",
      "        axes[-1].set_xlabel('Time')\n",
      "        plt.suptitle('Drift Monitoring Dashboard', fontsize=14, fontweight='bold')\n",
      "        plt.tight_layout()\n",
      "        \n",
      "        if save_path:\n",
      "            plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
      "            logger.info(f\"Plot saved to {save_path}\")\n",
      "        \n",
      "        plt.show()\n",
      "    \n",
      "    def generate_html_report(self, output_path: str):\n",
      "        \"\"\"Generate HTML report\"\"\"\n",
      "        summary = self.pipeline.get_summary_report()\n",
      "        \n",
      "        html_content = f\"\"\"\n",
      "        <!DOCTYPE html>\n",
      "        <html>\n",
      "        <head>\n",
      "            <title>Drift Monitoring Report</title>\n",
      "            <style>\n",
      "                body {{ font-family: Arial, sans-serif; margin: 20px; }}\n",
      "                .header {{ background-color: #2196F3; color: white; padding: 20px; }}\n",
      "                .summary {{ background-color: #f0f0f0; padding: 15px; margin: 10px 0; }}\n",
      "                .alert-critical {{ background-color: #ffebee; border-left: 4px solid #f44336; padding: 10px; margin: 5px 0; }}\n",
      "                .alert-warning {{ background-color: #fff3e0; border-left: 4px solid #ff9800; padding: 10px; margin: 5px 0; }}\n",
      "                .alert-info {{ background-color: #e3f2fd; border-left: 4px solid #2196F3; padding: 10px; margin: 5px 0; }}\n",
      "                table {{ border-collapse: collapse; width: 100%; }}\n",
      "                th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}\n",
      "                th {{ background-color: #2196F3; color: white; }}\n",
      "            </style>\n",
      "        </head>\n",
      "        <body>\n",
      "            <div class=\"header\">\n",
      "                <h1>üîç Drift Monitoring Report</h1>\n",
      "                <p>Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>\n",
      "            </div>\n",
      "            \n",
      "            <div class=\"summary\">\n",
      "                <h2>üìä Summary</h2>\n",
      "                <p><strong>Total Checks:</strong> {summary.get('total_checks', 0)}</p>\n",
      "                <p><strong>Total Drifts Detected:</strong> {summary.get('total_drifts_detected', 0)}</p>\n",
      "                <p><strong>Active Alerts:</strong> {summary.get('active_alerts', 0)}</p>\n",
      "            </div>\n",
      "            \n",
      "            <h2>üìà Feature Summary</h2>\n",
      "            <table>\n",
      "                <tr>\n",
      "                    <th>Feature</th>\n",
      "                    <th>Latest PSI</th>\n",
      "                    <th>Drift Type</th>\n",
      "                    <th>Drift Count</th>\n",
      "                    <th>Drift Rate</th>\n",
      "                </tr>\n",
      "        \"\"\"\n",
      "        \n",
      "        for feature, data in summary.get('feature_summary', {}).items():\n",
      "            html_content += f\"\"\"\n",
      "                <tr>\n",
      "                    <td>{feature}</td>\n",
      "                    <td>{data['latest_psi']:.4f}</td>\n",
      "                    <td>{data['latest_drift_type']}</td>\n",
      "                    <td>{data['drift_count']}</td>\n",
      "                    <td>{data['drift_rate']:.1%}</td>\n",
      "                </tr>\n",
      "            \"\"\"\n",
      "        \n",
      "        html_content += \"\"\"\n",
      "            </table>\n",
      "            \n",
      "            <h2>‚ö†Ô∏è Active Alerts</h2>\n",
      "        \"\"\"\n",
      "        \n",
      "        for alert in self.pipeline.alert_manager.get_active_alerts():\n",
      "            alert_class = f\"alert-{alert.severity.value}\"\n",
      "            html_content += f\"\"\"\n",
      "            <div class=\"{alert_class}\">\n",
      "                <strong>{alert.severity.value.upper()}</strong>: {alert.message}\n",
      "                <br><small>{alert.timestamp.strftime('%Y-%m-%d %H:%M:%S')}</small>\n",
      "            </div>\n",
      "            \"\"\"\n",
      "        \n",
      "        html_content += \"\"\"\n",
      "        </body>\n",
      "        </html>\n",
      "        \"\"\"\n",
      "        \n",
      "        with open(output_path, 'w') as f:\n",
      "            f.write(html_content)\n",
      "        \n",
      "        logger.info(f\"HTML report saved to {output_path}\")\n",
      "\n",
      "#%% [markdown]\n",
      "# ## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 6: ‡∏ó‡∏î‡∏™‡∏≠‡∏ö Full Pipeline\n",
      "\n",
      "#%%\n",
      "# ‡∏™‡∏£‡πâ‡∏≤‡∏á test data\n",
      "def generate_test_data(n_samples=5000):\n",
      "    \"\"\"‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡∏°‡∏µ drift\"\"\"\n",
      "    np.random.seed(42)\n",
      "    \n",
      "    data = []\n",
      "    base_means = {'feature_a': 50, 'feature_b': 100, 'feature_c': 75}\n",
      "    \n",
      "    for i in range(n_samples):\n",
      "        # ‡∏™‡∏£‡πâ‡∏≤‡∏á drift ‡∏ó‡∏µ‡πà feature_a ‡∏´‡∏•‡∏±‡∏á sample 3000\n",
      "        if i < 3000:\n",
      "            feature_a_mean = base_means['feature_a']\n",
      "        else:\n",
      "            # Gradual drift\n",
      "            progress = (i - 3000) / 2000\n",
      "            feature_a_mean = base_means['feature_a'] + progress * 20\n",
      "        \n",
      "        # ‡∏™‡∏£‡πâ‡∏≤‡∏á sudden drift ‡∏ó‡∏µ‡πà feature_b ‡∏ó‡∏µ‡πà sample 2000\n",
      "        if i < 2000:\n",
      "            feature_b_mean = base_means['feature_b']\n",
      "        else:\n",
      "            feature_b_mean = base_means['feature_b'] + 30\n",
      "        \n",
      "        # feature_c ‡πÑ‡∏°‡πà‡∏°‡∏µ drift\n",
      "        feature_c_mean = base_means['feature_c']\n",
      "        \n",
      "        data.append({\n",
      "            'timestamp': datetime(2024, 1, 1) + timedelta(hours=i),\n",
      "            'feature_a': np.random.normal(feature_a_mean, 10),\n",
      "            'feature_b': np.random.normal(feature_b_mean, 15),\n",
      "            'feature_c': np.random.normal(feature_c_mean, 12)\n",
      "        })\n",
      "    \n",
      "    return pd.DataFrame(data)\n",
      "\n",
      "# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏î‡∏™‡∏≠‡∏ö\n",
      "test_data = generate_test_data(5000)\n",
      "print(f\"üìä Test data shape: {test_data.shape}\")\n",
      "print(test_data.head())\n",
      "\n",
      "#%%\n",
      "# Visualize test data\n",
      "fig, axes = plt.subplots(3, 1, figsize=(14, 8), sharex=True)\n",
      "\n",
      "for ax, feature in zip(axes, ['feature_a', 'feature_b', 'feature_c']):\n",
      "    ax.plot(test_data['timestamp'], test_data[feature], alpha=0.3)\n",
      "    rolling_mean = test_data[feature].rolling(100).mean()\n",
      "    ax.plot(test_data['timestamp'], rolling_mean, 'r-', linewidth=2, label='Rolling Mean')\n",
      "    ax.set_ylabel(feature)\n",
      "    ax.legend()\n",
      "\n",
      "axes[0].set_title('Test Data with Drift Patterns')\n",
      "axes[-1].set_xlabel('Time')\n",
      "plt.tight_layout()\n",
      "plt.show()\n",
      "\n",
      "#%%\n",
      "# Configure ‡πÅ‡∏•‡∏∞ run pipeline\n",
      "config = MonitoringConfig(\n",
      "    reference_window_size=1000,\n",
      "    current_window_size=200,\n",
      "    psi_mild_threshold=0.1,\n",
      "    psi_moderate_threshold=0.2,\n",
      "    psi_severe_threshold=0.25,\n",
      "    ks_significance=0.05,\n",
      "    features_to_monitor=['feature_a', 'feature_b', 'feature_c']\n",
      ")\n",
      "\n",
      "# ‡∏™‡∏£‡πâ‡∏≤‡∏á pipeline\n",
      "pipeline = DriftMonitoringPipeline(config)\n",
      "\n",
      "# ‡πÉ‡∏ä‡πâ 1000 samples ‡πÅ‡∏£‡∏Å‡πÄ‡∏õ‡πá‡∏ô reference\n",
      "reference_data = test_data.iloc[:1000]\n",
      "pipeline.initialize(reference_data)\n",
      "\n",
      "print(\"=\" * 60)\n",
      "print(\"üöÄ Starting Drift Monitoring Pipeline\")\n",
      "print(\"=\" * 60)\n",
      "\n",
      "# Process data ‡πÄ‡∏õ‡πá‡∏ô batches\n",
      "batch_size = 200\n",
      "remaining_data = test_data.iloc[1000:]\n",
      "\n",
      "for i in range(0, len(remaining_data), batch_size):\n",
      "    batch = remaining_data.iloc[i:i+batch_size]\n",
      "    results = pipeline.process_batch(batch)\n",
      "    \n",
      "    if results and any(r.drift_detected for r in results):\n",
      "        print(f\"\\nüìç Batch {i//batch_size + 1} results:\")\n",
      "        for r in results:\n",
      "            if r.drift_detected:\n",
      "                print(f\"   ‚ö†Ô∏è {r.feature}: PSI={r.psi:.4f}, Type={r.drift_type.value}\")\n",
      "\n",
      "#%%\n",
      "# Generate summary\n",
      "print(\"\\n\" + \"=\" * 60)\n",
      "print(\"üìä FINAL SUMMARY REPORT\")\n",
      "print(\"=\" * 60)\n",
      "\n",
      "summary = pipeline.get_summary_report()\n",
      "print(f\"\\nTotal Checks: {summary['total_checks']}\")\n",
      "print(f\"Total Drifts Detected: {summary['total_drifts_detected']}\")\n",
      "print(f\"Active Alerts: {summary['active_alerts']}\")\n",
      "\n",
      "print(\"\\nFeature Summary:\")\n",
      "for feature, data in summary['feature_summary'].items():\n",
      "    print(f\"\\n  {feature}:\")\n",
      "    print(f\"    Latest PSI: {data['latest_psi']:.4f}\")\n",
      "    print(f\"    Drift Type: {data['latest_drift_type']}\")\n",
      "    print(f\"    Drift Count: {data['drift_count']}/{data['total_checks']}\")\n",
      "    print(f\"    Drift Rate: {data['drift_rate']:.1%}\")\n",
      "\n",
      "#%%\n",
      "# Generate reports\n",
      "report_generator = ReportGenerator(pipeline)\n",
      "\n",
      "# Plot trends\n",
      "report_generator.plot_drift_trends(save_path='monitoring_output/drift_trends.png')\n",
      "\n",
      "# Generate HTML report\n",
      "report_generator.generate_html_report('monitoring_output/reports/drift_report.html')\n",
      "\n",
      "# Save all results\n",
      "pipeline.save_results()\n",
      "\n",
      "print(\"\\n‚úÖ All reports generated and saved to monitoring_output/\")\n",
      "\n",
      "#%% [markdown]\n",
      "# ## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 7: Integration ‡∏Å‡∏±‡∏ö MLflow (Optional)\n",
      "\n",
      "#%%\n",
      "# Note: ‡∏™‡πà‡∏ß‡∏ô‡∏ô‡∏µ‡πâ‡∏ï‡πâ‡∏≠‡∏á install mlflow ‡∏Å‡πà‡∏≠‡∏ô: pip install mlflow\n",
      "\n",
      "try:\n",
      "    import mlflow\n",
      "    MLFLOW_AVAILABLE = True\n",
      "except ImportError:\n",
      "    MLFLOW_AVAILABLE = False\n",
      "    print(\"‚ö†Ô∏è MLflow not installed. Skipping MLflow integration.\")\n",
      "    print(\"   Install with: pip install mlflow\")\n",
      "\n",
      "if MLFLOW_AVAILABLE:\n",
      "    class MLflowDriftTracker:\n",
      "        \"\"\"\n",
      "        Integration ‡∏Å‡∏±‡∏ö MLflow ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö track drift experiments\n",
      "        \"\"\"\n",
      "        \n",
      "        def __init__(self, experiment_name: str = \"drift_monitoring\"):\n",
      "            mlflow.set_experiment(experiment_name)\n",
      "            self.active_run = None\n",
      "        \n",
      "        def start_run(self, run_name: str = None):\n",
      "            \"\"\"Start new MLflow run\"\"\"\n",
      "            self.active_run = mlflow.start_run(run_name=run_name)\n",
      "            return self.active_run\n",
      "        \n",
      "        def log_drift_result(self, result: DriftResult):\n",
      "            \"\"\"Log drift result to MLflow\"\"\"\n",
      "            if self.active_run is None:\n",
      "                return\n",
      "            \n",
      "            # Log metrics\n",
      "            mlflow.log_metric(f\"{result.feature}_psi\", result.psi)\n",
      "            mlflow.log_metric(f\"{result.feature}_ks_stat\", result.ks_statistic)\n",
      "            mlflow.log_metric(f\"{result.feature}_drift\", 1 if result.drift_detected else 0)\n",
      "            \n",
      "            # Log params\n",
      "            mlflow.log_param(f\"{result.feature}_drift_type\", result.drift_type.value)\n",
      "        \n",
      "        def log_summary(self, summary: Dict):\n",
      "            \"\"\"Log summary to MLflow\"\"\"\n",
      "            if self.active_run is None:\n",
      "                return\n",
      "            \n",
      "            mlflow.log_metric(\"total_drifts\", summary.get('total_drifts_detected', 0))\n",
      "            mlflow.log_metric(\"total_checks\", summary.get('total_checks', 0))\n",
      "            \n",
      "            for feature, data in summary.get('feature_summary', {}).items():\n",
      "                mlflow.log_metric(f\"{feature}_drift_rate\", data['drift_rate'])\n",
      "        \n",
      "        def log_artifact(self, artifact_path: str):\n",
      "            \"\"\"Log artifact to MLflow\"\"\"\n",
      "            if self.active_run is None:\n",
      "                return\n",
      "            mlflow.log_artifact(artifact_path)\n",
      "        \n",
      "        def end_run(self):\n",
      "            \"\"\"End MLflow run\"\"\"\n",
      "            if self.active_run:\n",
      "                mlflow.end_run()\n",
      "                self.active_run = None\n",
      "    \n",
      "    # Example usage\n",
      "    print(\"\\n\" + \"=\" * 60)\n",
      "    print(\"üìä Logging to MLflow\")\n",
      "    print(\"=\" * 60)\n",
      "    \n",
      "    tracker = MLflowDriftTracker(\"drift_monitoring_lab\")\n",
      "    tracker.start_run(run_name=\"pipeline_run_1\")\n",
      "    \n",
      "    # Log summary\n",
      "    tracker.log_summary(summary)\n",
      "    \n",
      "    # Log artifacts\n",
      "    tracker.log_artifact('monitoring_output/drift_trends.png')\n",
      "    tracker.log_artifact('monitoring_output/reports/drift_report.html')\n",
      "    \n",
      "    tracker.end_run()\n",
      "    print(\"‚úÖ Results logged to MLflow\")\n",
      "\n",
      "#%% [markdown]\n",
      "# ## ‡∏™‡∏£‡∏∏‡∏õ LAB 6\n",
      "# \n",
      "# ### ‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ:\n",
      "# 1. **Pipeline Architecture**: ‡πÅ‡∏¢‡∏Å components ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô\n",
      "# 2. **Data Classes**: ‡πÉ‡∏ä‡πâ dataclasses ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö type safety\n",
      "# 3. **Alert Management**: ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ alerts ‡∏û‡∏£‡πâ‡∏≠‡∏° cooldown\n",
      "# 4. **Report Generation**: HTML reports ‡πÅ‡∏•‡∏∞ visualizations\n",
      "# 5. **MLflow Integration**: Track experiments\n",
      "\n",
      "#%%\n",
      "print(\"=\" * 60)\n",
      "print(\"‚úÖ LAB 6 COMPLETED!\")\n",
      "print(\"=\" * 60)\n",
      "print(\"\"\"\n",
      "üìö Key Takeaways:\n",
      "1. Modular architecture ‡∏ó‡∏≥‡πÉ‡∏´‡πâ maintain ‡∏á‡πà‡∏≤‡∏¢\n",
      "2. Data buffering ‡∏ä‡πà‡∏ß‡∏¢ handle streaming data\n",
      "3. Alert management ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô alert fatigue\n",
      "4. Automated reporting saves time\n",
      "5. MLflow integration enables experiment tracking\n",
      "\n",
      "üéâ CONGRATULATIONS! You have completed all 6 labs!\n",
      "\n",
      "üìã What you've learned:\n",
      "- LAB 1: Data Drift Concepts (Covariate/Concept Shift)\n",
      "- LAB 2: Feature Drift Detection (per-feature analysis)\n",
      "- LAB 3: Multivariate Drift Analysis (correlation/PCA)\n",
      "- LAB 4: Production Simulation (streaming detection)\n",
      "- LAB 5: Custom Metrics & Thresholds (optimization)\n",
      "- LAB 6: End-to-End Pipeline (production-ready)\n",
      "\n",
      "üöÄ Next steps:\n",
      "- Deploy pipeline to production\n",
      "- Add more sophisticated alerting (email, Slack)\n",
      "- Integrate with model retraining triggers\n",
      "- Add A/B testing capabilities\n",
      "\"\"\")\n",
      "\n",
      "#%%\n",
      "# Final cleanup and summary\n",
      "print(\"\\nüìÅ Output files created:\")\n",
      "for root, dirs, files in os.walk('monitoring_output'):\n",
      "    for file in files:\n",
      "        filepath = os.path.join(root, file)\n",
      "        print(f\"   {filepath}\")\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "## ‡∏™‡∏£‡∏∏‡∏õ‡∏£‡∏ß‡∏°\n",
      "\n",
      "‡∏ó‡∏±‡πâ‡∏á 6 Labs ‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏î‡∏±‡∏á‡∏ô‡∏µ‡πâ:\n",
      "\n",
      "| Lab | ‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠ | ‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ |\n",
      "|-----|--------|----------------|\n",
      "| 1 | Understanding Data Drift | Covariate/Concept Shift, KS/PSI/Wasserstein |\n",
      "| 2 | Feature Drift Detection | Per-feature analysis, Numerical vs Categorical |\n",
      "| 3 | Multivariate Drift | Correlation, PCA, Mahalanobis Distance |\n",
      "| 4 | Production Simulation | Streaming, Sliding Window, Page-Hinkley |\n",
      "| 5 | Custom Metrics & Thresholds | Optimization, Cost-based, Ensemble |\n",
      "| 6 | End-to-End Pipeline | Architecture, Alerting, Reporting, MLflow |\n"
     ]
    }
   ],
   "source": [
    "# Simple extraction\n",
    "text = response[\"content\"][0][\"text\"]\n",
    "print(text)\n",
    "\n",
    "# Save to file\n",
    "with open(\"output.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c858923",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
