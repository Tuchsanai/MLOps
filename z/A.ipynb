{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a6d079c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9.91M/9.91M [00:17<00:00, 575kB/s] \n",
      "100%|██████████| 28.9k/28.9k [00:00<00:00, 120kB/s]\n",
      "100%|██████████| 1.65M/1.65M [00:01<00:00, 856kB/s] \n",
      "100%|██████████| 4.54k/4.54k [00:00<00:00, 5.45MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teacher Parameters: 1,554,954\n",
      "Student Parameters: 206,922\n",
      "Compression Ratio: 7.51x\n",
      "\n",
      "Training Teacher Model...\n",
      "Epoch 1/5, Batch 0, Loss: 2.3112\n",
      "Epoch 1/5, Batch 100, Loss: 0.0697\n",
      "Epoch 1/5, Batch 200, Loss: 0.0817\n",
      "Epoch 1/5, Batch 300, Loss: 0.1090\n",
      "Epoch 1/5, Batch 400, Loss: 0.3740\n",
      "Epoch 1/5, Batch 500, Loss: 0.0923\n",
      "Epoch 1/5, Batch 600, Loss: 0.0862\n",
      "Epoch 1/5, Batch 700, Loss: 0.0426\n",
      "Epoch 1/5, Batch 800, Loss: 0.0191\n",
      "Epoch 1/5, Batch 900, Loss: 0.0517\n",
      "Epoch 1 - Avg Loss: 0.1398, Accuracy: 95.51%\n",
      "\n",
      "Epoch 2/5, Batch 0, Loss: 0.0055\n",
      "Epoch 2/5, Batch 100, Loss: 0.0049\n",
      "Epoch 2/5, Batch 200, Loss: 0.0608\n",
      "Epoch 2/5, Batch 300, Loss: 0.0351\n",
      "Epoch 2/5, Batch 400, Loss: 0.1175\n",
      "Epoch 2/5, Batch 500, Loss: 0.0055\n",
      "Epoch 2/5, Batch 600, Loss: 0.0160\n",
      "Epoch 2/5, Batch 700, Loss: 0.0268\n",
      "Epoch 2/5, Batch 800, Loss: 0.0196\n",
      "Epoch 2/5, Batch 900, Loss: 0.1173\n",
      "Epoch 2 - Avg Loss: 0.0447, Accuracy: 98.69%\n",
      "\n",
      "Epoch 3/5, Batch 0, Loss: 0.0078\n",
      "Epoch 3/5, Batch 100, Loss: 0.0058\n",
      "Epoch 3/5, Batch 200, Loss: 0.0557\n",
      "Epoch 3/5, Batch 300, Loss: 0.0719\n",
      "Epoch 3/5, Batch 400, Loss: 0.0522\n",
      "Epoch 3/5, Batch 500, Loss: 0.0232\n",
      "Epoch 3/5, Batch 600, Loss: 0.0317\n",
      "Epoch 3/5, Batch 700, Loss: 0.0342\n",
      "Epoch 3/5, Batch 800, Loss: 0.1073\n",
      "Epoch 3/5, Batch 900, Loss: 0.0116\n",
      "Epoch 3 - Avg Loss: 0.0350, Accuracy: 98.89%\n",
      "\n",
      "Epoch 4/5, Batch 0, Loss: 0.0369\n",
      "Epoch 4/5, Batch 100, Loss: 0.0214\n",
      "Epoch 4/5, Batch 200, Loss: 0.0079\n",
      "Epoch 4/5, Batch 300, Loss: 0.0014\n",
      "Epoch 4/5, Batch 400, Loss: 0.0656\n",
      "Epoch 4/5, Batch 500, Loss: 0.0024\n",
      "Epoch 4/5, Batch 600, Loss: 0.0005\n",
      "Epoch 4/5, Batch 700, Loss: 0.0033\n",
      "Epoch 4/5, Batch 800, Loss: 0.0876\n",
      "Epoch 4/5, Batch 900, Loss: 0.0098\n",
      "Epoch 4 - Avg Loss: 0.0279, Accuracy: 99.16%\n",
      "\n",
      "Epoch 5/5, Batch 0, Loss: 0.0004\n",
      "Epoch 5/5, Batch 100, Loss: 0.0001\n",
      "Epoch 5/5, Batch 200, Loss: 0.0014\n",
      "Epoch 5/5, Batch 300, Loss: 0.0005\n",
      "Epoch 5/5, Batch 400, Loss: 0.0098\n",
      "Epoch 5/5, Batch 500, Loss: 0.0362\n",
      "Epoch 5/5, Batch 600, Loss: 0.0022\n",
      "Epoch 5/5, Batch 700, Loss: 0.0044\n",
      "Epoch 5/5, Batch 800, Loss: 0.0005\n",
      "Epoch 5/5, Batch 900, Loss: 0.1112\n",
      "Epoch 5 - Avg Loss: 0.0208, Accuracy: 99.41%\n",
      "\n",
      "Teacher Test Accuracy: 99.21%\n",
      "\n",
      "Training Student Model with Knowledge Distillation...\n",
      "Epoch 1/10, Batch 0, Loss: 14.1414\n",
      "Epoch 1/10, Batch 100, Loss: 1.6821\n",
      "Epoch 1/10, Batch 200, Loss: 1.4628\n",
      "Epoch 1/10, Batch 300, Loss: 1.1491\n",
      "Epoch 1/10, Batch 400, Loss: 0.5746\n",
      "Epoch 1/10, Batch 500, Loss: 0.4050\n",
      "Epoch 1/10, Batch 600, Loss: 0.5311\n",
      "Epoch 1/10, Batch 700, Loss: 0.2662\n",
      "Epoch 1/10, Batch 800, Loss: 0.2953\n",
      "Epoch 1/10, Batch 900, Loss: 0.5770\n",
      "Epoch 1 - Total Loss: 1.0198, Distill Loss: 1.3723, Student Loss: 0.1973, Accuracy: 94.92%\n",
      "\n",
      "Epoch 2/10, Batch 0, Loss: 0.2179\n",
      "Epoch 2/10, Batch 100, Loss: 0.3113\n",
      "Epoch 2/10, Batch 200, Loss: 0.5235\n",
      "Epoch 2/10, Batch 300, Loss: 0.1484\n",
      "Epoch 2/10, Batch 400, Loss: 0.2976\n",
      "Epoch 2/10, Batch 500, Loss: 0.2322\n",
      "Epoch 2/10, Batch 600, Loss: 0.1745\n",
      "Epoch 2/10, Batch 700, Loss: 0.1759\n",
      "Epoch 2/10, Batch 800, Loss: 0.2532\n",
      "Epoch 2/10, Batch 900, Loss: 0.2006\n",
      "Epoch 2 - Total Loss: 0.2456, Distill Loss: 0.3298, Student Loss: 0.0492, Accuracy: 98.62%\n",
      "\n",
      "Epoch 3/10, Batch 0, Loss: 0.1268\n",
      "Epoch 3/10, Batch 100, Loss: 0.1994\n",
      "Epoch 3/10, Batch 200, Loss: 0.0788\n",
      "Epoch 3/10, Batch 300, Loss: 0.2659\n",
      "Epoch 3/10, Batch 400, Loss: 0.1705\n",
      "Epoch 3/10, Batch 500, Loss: 0.1351\n",
      "Epoch 3/10, Batch 600, Loss: 0.1199\n",
      "Epoch 3/10, Batch 700, Loss: 0.1136\n",
      "Epoch 3/10, Batch 800, Loss: 0.0834\n",
      "Epoch 3/10, Batch 900, Loss: 0.1158\n",
      "Epoch 3 - Total Loss: 0.1667, Distill Loss: 0.2243, Student Loss: 0.0324, Accuracy: 99.05%\n",
      "\n",
      "Epoch 4/10, Batch 0, Loss: 0.1341\n",
      "Epoch 4/10, Batch 100, Loss: 0.0859\n",
      "Epoch 4/10, Batch 200, Loss: 0.1223\n",
      "Epoch 4/10, Batch 300, Loss: 0.1414\n",
      "Epoch 4/10, Batch 400, Loss: 0.1542\n",
      "Epoch 4/10, Batch 500, Loss: 0.0986\n",
      "Epoch 4/10, Batch 600, Loss: 0.1732\n",
      "Epoch 4/10, Batch 700, Loss: 0.1600\n",
      "Epoch 4/10, Batch 800, Loss: 0.1690\n",
      "Epoch 4/10, Batch 900, Loss: 0.1448\n",
      "Epoch 4 - Total Loss: 0.1292, Distill Loss: 0.1740, Student Loss: 0.0245, Accuracy: 99.27%\n",
      "\n",
      "Epoch 5/10, Batch 0, Loss: 0.0454\n",
      "Epoch 5/10, Batch 100, Loss: 0.0809\n",
      "Epoch 5/10, Batch 200, Loss: 0.0600\n",
      "Epoch 5/10, Batch 300, Loss: 0.0625\n",
      "Epoch 5/10, Batch 400, Loss: 0.1171\n",
      "Epoch 5/10, Batch 500, Loss: 0.3941\n",
      "Epoch 5/10, Batch 600, Loss: 0.1188\n",
      "Epoch 5/10, Batch 700, Loss: 0.0758\n",
      "Epoch 5/10, Batch 800, Loss: 0.0381\n",
      "Epoch 5/10, Batch 900, Loss: 0.2839\n",
      "Epoch 5 - Total Loss: 0.1075, Distill Loss: 0.1450, Student Loss: 0.0199, Accuracy: 99.38%\n",
      "\n",
      "Epoch 6/10, Batch 0, Loss: 0.1272\n",
      "Epoch 6/10, Batch 100, Loss: 0.0584\n",
      "Epoch 6/10, Batch 200, Loss: 0.1230\n",
      "Epoch 6/10, Batch 300, Loss: 0.1036\n",
      "Epoch 6/10, Batch 400, Loss: 0.1373\n",
      "Epoch 6/10, Batch 500, Loss: 0.1104\n",
      "Epoch 6/10, Batch 600, Loss: 0.0996\n",
      "Epoch 6/10, Batch 700, Loss: 0.1114\n",
      "Epoch 6/10, Batch 800, Loss: 0.0830\n",
      "Epoch 6/10, Batch 900, Loss: 0.0965\n",
      "Epoch 6 - Total Loss: 0.0914, Distill Loss: 0.1234, Student Loss: 0.0166, Accuracy: 99.51%\n",
      "\n",
      "Epoch 7/10, Batch 0, Loss: 0.0972\n",
      "Epoch 7/10, Batch 100, Loss: 0.0750\n",
      "Epoch 7/10, Batch 200, Loss: 0.1001\n",
      "Epoch 7/10, Batch 300, Loss: 0.1963\n",
      "Epoch 7/10, Batch 400, Loss: 0.0727\n",
      "Epoch 7/10, Batch 500, Loss: 0.0791\n",
      "Epoch 7/10, Batch 600, Loss: 0.0681\n",
      "Epoch 7/10, Batch 700, Loss: 0.0830\n",
      "Epoch 7/10, Batch 800, Loss: 0.0897\n",
      "Epoch 7/10, Batch 900, Loss: 0.0673\n",
      "Epoch 7 - Total Loss: 0.0806, Distill Loss: 0.1087, Student Loss: 0.0150, Accuracy: 99.56%\n",
      "\n",
      "Epoch 8/10, Batch 0, Loss: 0.0762\n",
      "Epoch 8/10, Batch 100, Loss: 0.0731\n",
      "Epoch 8/10, Batch 200, Loss: 0.0737\n",
      "Epoch 8/10, Batch 300, Loss: 0.0733\n",
      "Epoch 8/10, Batch 400, Loss: 0.0841\n",
      "Epoch 8/10, Batch 500, Loss: 0.1057\n",
      "Epoch 8/10, Batch 600, Loss: 0.0760\n",
      "Epoch 8/10, Batch 700, Loss: 0.0559\n",
      "Epoch 8/10, Batch 800, Loss: 0.1003\n",
      "Epoch 8/10, Batch 900, Loss: 0.0550\n",
      "Epoch 8 - Total Loss: 0.0724, Distill Loss: 0.0979, Student Loss: 0.0130, Accuracy: 99.60%\n",
      "\n",
      "Epoch 9/10, Batch 0, Loss: 0.0738\n",
      "Epoch 9/10, Batch 100, Loss: 0.0557\n",
      "Epoch 9/10, Batch 200, Loss: 0.0576\n",
      "Epoch 9/10, Batch 300, Loss: 0.0503\n",
      "Epoch 9/10, Batch 400, Loss: 0.0501\n",
      "Epoch 9/10, Batch 500, Loss: 0.0493\n",
      "Epoch 9/10, Batch 600, Loss: 0.0503\n",
      "Epoch 9/10, Batch 700, Loss: 0.0562\n",
      "Epoch 9/10, Batch 800, Loss: 0.0420\n",
      "Epoch 9/10, Batch 900, Loss: 0.1065\n",
      "Epoch 9 - Total Loss: 0.0652, Distill Loss: 0.0880, Student Loss: 0.0118, Accuracy: 99.64%\n",
      "\n",
      "Epoch 10/10, Batch 0, Loss: 0.0562\n",
      "Epoch 10/10, Batch 100, Loss: 0.0505\n",
      "Epoch 10/10, Batch 200, Loss: 0.0662\n",
      "Epoch 10/10, Batch 300, Loss: 0.0687\n",
      "Epoch 10/10, Batch 400, Loss: 0.0346\n",
      "Epoch 10/10, Batch 500, Loss: 0.0537\n",
      "Epoch 10/10, Batch 600, Loss: 0.0828\n",
      "Epoch 10/10, Batch 700, Loss: 0.0647\n",
      "Epoch 10/10, Batch 800, Loss: 0.0644\n",
      "Epoch 10/10, Batch 900, Loss: 0.0513\n",
      "Epoch 10 - Total Loss: 0.0604, Distill Loss: 0.0815, Student Loss: 0.0111, Accuracy: 99.63%\n",
      "\n",
      "Student Test Accuracy: 99.18%\n",
      "\n",
      "Training baseline student without distillation for comparison...\n",
      "Training Teacher Model...\n",
      "Epoch 1/10, Batch 0, Loss: 2.3071\n",
      "Epoch 1/10, Batch 100, Loss: 0.2542\n",
      "Epoch 1/10, Batch 200, Loss: 0.1984\n",
      "Epoch 1/10, Batch 300, Loss: 0.1103\n",
      "Epoch 1/10, Batch 400, Loss: 0.0325\n",
      "Epoch 1/10, Batch 500, Loss: 0.1036\n",
      "Epoch 1/10, Batch 600, Loss: 0.0528\n",
      "Epoch 1/10, Batch 700, Loss: 0.0868\n",
      "Epoch 1/10, Batch 800, Loss: 0.0344\n",
      "Epoch 1/10, Batch 900, Loss: 0.0925\n",
      "Epoch 1 - Avg Loss: 0.1646, Accuracy: 95.02%\n",
      "\n",
      "Epoch 2/10, Batch 0, Loss: 0.0275\n",
      "Epoch 2/10, Batch 100, Loss: 0.0807\n",
      "Epoch 2/10, Batch 200, Loss: 0.0211\n",
      "Epoch 2/10, Batch 300, Loss: 0.0543\n",
      "Epoch 2/10, Batch 400, Loss: 0.0485\n",
      "Epoch 2/10, Batch 500, Loss: 0.1695\n",
      "Epoch 2/10, Batch 600, Loss: 0.0584\n",
      "Epoch 2/10, Batch 700, Loss: 0.0040\n",
      "Epoch 2/10, Batch 800, Loss: 0.0180\n",
      "Epoch 2/10, Batch 900, Loss: 0.0253\n",
      "Epoch 2 - Avg Loss: 0.0521, Accuracy: 98.41%\n",
      "\n",
      "Epoch 3/10, Batch 0, Loss: 0.0282\n",
      "Epoch 3/10, Batch 100, Loss: 0.1007\n",
      "Epoch 3/10, Batch 200, Loss: 0.0574\n",
      "Epoch 3/10, Batch 300, Loss: 0.1013\n",
      "Epoch 3/10, Batch 400, Loss: 0.0089\n",
      "Epoch 3/10, Batch 500, Loss: 0.0133\n",
      "Epoch 3/10, Batch 600, Loss: 0.0126\n",
      "Epoch 3/10, Batch 700, Loss: 0.0323\n",
      "Epoch 3/10, Batch 800, Loss: 0.0171\n",
      "Epoch 3/10, Batch 900, Loss: 0.0478\n",
      "Epoch 3 - Avg Loss: 0.0343, Accuracy: 98.95%\n",
      "\n",
      "Epoch 4/10, Batch 0, Loss: 0.0103\n",
      "Epoch 4/10, Batch 100, Loss: 0.0048\n",
      "Epoch 4/10, Batch 200, Loss: 0.0031\n",
      "Epoch 4/10, Batch 300, Loss: 0.0180\n",
      "Epoch 4/10, Batch 400, Loss: 0.0710\n",
      "Epoch 4/10, Batch 500, Loss: 0.0303\n",
      "Epoch 4/10, Batch 600, Loss: 0.0108\n",
      "Epoch 4/10, Batch 700, Loss: 0.0113\n",
      "Epoch 4/10, Batch 800, Loss: 0.0576\n",
      "Epoch 4/10, Batch 900, Loss: 0.0035\n",
      "Epoch 4 - Avg Loss: 0.0269, Accuracy: 99.17%\n",
      "\n",
      "Epoch 5/10, Batch 0, Loss: 0.0105\n",
      "Epoch 5/10, Batch 100, Loss: 0.0317\n",
      "Epoch 5/10, Batch 200, Loss: 0.0196\n",
      "Epoch 5/10, Batch 300, Loss: 0.0116\n",
      "Epoch 5/10, Batch 400, Loss: 0.0357\n",
      "Epoch 5/10, Batch 500, Loss: 0.0089\n",
      "Epoch 5/10, Batch 600, Loss: 0.0029\n",
      "Epoch 5/10, Batch 700, Loss: 0.0011\n",
      "Epoch 5/10, Batch 800, Loss: 0.0213\n",
      "Epoch 5/10, Batch 900, Loss: 0.0010\n",
      "Epoch 5 - Avg Loss: 0.0204, Accuracy: 99.36%\n",
      "\n",
      "Epoch 6/10, Batch 0, Loss: 0.0019\n",
      "Epoch 6/10, Batch 100, Loss: 0.0087\n",
      "Epoch 6/10, Batch 200, Loss: 0.0111\n",
      "Epoch 6/10, Batch 300, Loss: 0.0189\n",
      "Epoch 6/10, Batch 400, Loss: 0.0128\n",
      "Epoch 6/10, Batch 500, Loss: 0.0806\n",
      "Epoch 6/10, Batch 600, Loss: 0.0418\n",
      "Epoch 6/10, Batch 700, Loss: 0.0594\n",
      "Epoch 6/10, Batch 800, Loss: 0.0176\n",
      "Epoch 6/10, Batch 900, Loss: 0.0950\n",
      "Epoch 6 - Avg Loss: 0.0172, Accuracy: 99.42%\n",
      "\n",
      "Epoch 7/10, Batch 0, Loss: 0.0408\n",
      "Epoch 7/10, Batch 100, Loss: 0.0047\n",
      "Epoch 7/10, Batch 200, Loss: 0.0198\n",
      "Epoch 7/10, Batch 300, Loss: 0.0013\n",
      "Epoch 7/10, Batch 400, Loss: 0.0026\n",
      "Epoch 7/10, Batch 500, Loss: 0.0029\n",
      "Epoch 7/10, Batch 600, Loss: 0.0010\n",
      "Epoch 7/10, Batch 700, Loss: 0.0005\n",
      "Epoch 7/10, Batch 800, Loss: 0.0026\n",
      "Epoch 7/10, Batch 900, Loss: 0.0044\n",
      "Epoch 7 - Avg Loss: 0.0130, Accuracy: 99.58%\n",
      "\n",
      "Epoch 8/10, Batch 0, Loss: 0.0006\n",
      "Epoch 8/10, Batch 100, Loss: 0.0003\n",
      "Epoch 8/10, Batch 200, Loss: 0.0040\n",
      "Epoch 8/10, Batch 300, Loss: 0.0044\n",
      "Epoch 8/10, Batch 400, Loss: 0.0034\n",
      "Epoch 8/10, Batch 500, Loss: 0.0036\n",
      "Epoch 8/10, Batch 600, Loss: 0.0003\n",
      "Epoch 8/10, Batch 700, Loss: 0.0016\n",
      "Epoch 8/10, Batch 800, Loss: 0.0068\n",
      "Epoch 8/10, Batch 900, Loss: 0.0026\n",
      "Epoch 8 - Avg Loss: 0.0103, Accuracy: 99.66%\n",
      "\n",
      "Epoch 9/10, Batch 0, Loss: 0.0017\n",
      "Epoch 9/10, Batch 100, Loss: 0.0001\n",
      "Epoch 9/10, Batch 200, Loss: 0.0020\n",
      "Epoch 9/10, Batch 300, Loss: 0.0496\n",
      "Epoch 9/10, Batch 400, Loss: 0.0010\n",
      "Epoch 9/10, Batch 500, Loss: 0.0014\n",
      "Epoch 9/10, Batch 600, Loss: 0.0001\n",
      "Epoch 9/10, Batch 700, Loss: 0.0879\n",
      "Epoch 9/10, Batch 800, Loss: 0.0005\n",
      "Epoch 9/10, Batch 900, Loss: 0.0032\n",
      "Epoch 9 - Avg Loss: 0.0082, Accuracy: 99.73%\n",
      "\n",
      "Epoch 10/10, Batch 0, Loss: 0.0094\n",
      "Epoch 10/10, Batch 100, Loss: 0.0000\n",
      "Epoch 10/10, Batch 200, Loss: 0.0065\n",
      "Epoch 10/10, Batch 300, Loss: 0.0003\n",
      "Epoch 10/10, Batch 400, Loss: 0.0023\n",
      "Epoch 10/10, Batch 500, Loss: 0.0002\n",
      "Epoch 10/10, Batch 600, Loss: 0.0005\n",
      "Epoch 10/10, Batch 700, Loss: 0.0000\n",
      "Epoch 10/10, Batch 800, Loss: 0.0091\n",
      "Epoch 10/10, Batch 900, Loss: 0.0010\n",
      "Epoch 10 - Avg Loss: 0.0078, Accuracy: 99.75%\n",
      "\n",
      "Baseline Student Test Accuracy: 99.13%\n",
      "\n",
      "============================================================\n",
      "FINAL RESULTS:\n",
      "Teacher Accuracy: 99.21%\n",
      "Student (with distillation) Accuracy: 99.18%\n",
      "Student (baseline) Accuracy: 99.13%\n",
      "Improvement from distillation: 0.05%\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Knowledge Distillation in PyTorch - Complete Tutorial\n",
    "# This notebook demonstrates how to train a small \"student\" model to mimic a larger \"teacher\" model\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 1: Define Teacher Model (Large, Complex Network)\n",
    "# ============================================================================\n",
    "\n",
    "class TeacherModel(nn.Module):\n",
    "    \"\"\"Large model with more parameters - the 'expert' teacher\"\"\"\n",
    "    def __init__(self):\n",
    "        super(TeacherModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 64, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(128, 256, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(256 * 3 * 3, 512)\n",
    "        self.fc2 = nn.Linear(512, 10)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = x.view(-1, 256 * 3 * 3)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 2: Define Student Model (Small, Simple Network)\n",
    "# ============================================================================\n",
    "\n",
    "class StudentModel(nn.Module):\n",
    "    \"\"\"Smaller model with fewer parameters - learns from teacher\"\"\"\n",
    "    def __init__(self):\n",
    "        super(StudentModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(32 * 7 * 7, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 32 * 7 * 7)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 3: Knowledge Distillation Loss Function\n",
    "# ============================================================================\n",
    "\n",
    "class DistillationLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Combines two losses:\n",
    "    1. Distillation Loss: Student learns from teacher's soft predictions\n",
    "    2. Student Loss: Student learns from true labels\n",
    "    \n",
    "    Temperature (T): Softens probability distributions\n",
    "    - Higher T: More uniform distribution (more information transfer)\n",
    "    - Lower T: Sharper distribution (closer to hard labels)\n",
    "    \n",
    "    Alpha: Balance between distillation and student loss\n",
    "    \"\"\"\n",
    "    def __init__(self, temperature=3.0, alpha=0.7):\n",
    "        super(DistillationLoss, self).__init__()\n",
    "        self.temperature = temperature\n",
    "        self.alpha = alpha\n",
    "        self.kl_div = nn.KLDivLoss(reduction='batchmean')\n",
    "        self.ce_loss = nn.CrossEntropyLoss()\n",
    "        \n",
    "    def forward(self, student_logits, teacher_logits, labels):\n",
    "        # Soften the predictions using temperature\n",
    "        soft_student = F.log_softmax(student_logits / self.temperature, dim=1)\n",
    "        soft_teacher = F.softmax(teacher_logits / self.temperature, dim=1)\n",
    "        \n",
    "        # Distillation loss (KL divergence between soft predictions)\n",
    "        distillation_loss = self.kl_div(soft_student, soft_teacher) * (self.temperature ** 2)\n",
    "        \n",
    "        # Student loss (cross-entropy with true labels)\n",
    "        student_loss = self.ce_loss(student_logits, labels)\n",
    "        \n",
    "        # Combined loss\n",
    "        total_loss = self.alpha * distillation_loss + (1 - self.alpha) * student_loss\n",
    "        \n",
    "        return total_loss, distillation_loss, student_loss\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 4: Data Loading (MNIST Dataset)\n",
    "# ============================================================================\n",
    "\n",
    "def get_data_loaders(batch_size=64):\n",
    "    \"\"\"Prepare MNIST dataset\"\"\"\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "    \n",
    "    train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "    test_dataset = datasets.MNIST('./data', train=False, transform=transform)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_loader, test_loader\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 5: Training Functions\n",
    "# ============================================================================\n",
    "\n",
    "def train_teacher(model, train_loader, epochs=5, device='cuda'):\n",
    "    \"\"\"Train the teacher model from scratch\"\"\"\n",
    "    print(\"Training Teacher Model...\")\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            pred = output.argmax(dim=1)\n",
    "            correct += pred.eq(target).sum().item()\n",
    "            total += target.size(0)\n",
    "            \n",
    "            if batch_idx % 100 == 0:\n",
    "                print(f'Epoch {epoch+1}/{epochs}, Batch {batch_idx}, Loss: {loss.item():.4f}')\n",
    "        \n",
    "        accuracy = 100. * correct / total\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f'Epoch {epoch+1} - Avg Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\\n')\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train_student_with_distillation(student, teacher, train_loader, epochs=10, \n",
    "                                   temperature=3.0, alpha=0.7, device='cuda'):\n",
    "    \"\"\"Train student model using knowledge distillation\"\"\"\n",
    "    print(\"Training Student Model with Knowledge Distillation...\")\n",
    "    student = student.to(device)\n",
    "    teacher = teacher.to(device)\n",
    "    teacher.eval()  # Teacher in evaluation mode\n",
    "    \n",
    "    optimizer = optim.Adam(student.parameters(), lr=0.001)\n",
    "    distillation_criterion = DistillationLoss(temperature=temperature, alpha=alpha)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        student.train()\n",
    "        total_loss = 0\n",
    "        total_distill_loss = 0\n",
    "        total_student_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Get predictions from both models\n",
    "            student_logits = student(data)\n",
    "            with torch.no_grad():\n",
    "                teacher_logits = teacher(data)\n",
    "            \n",
    "            # Calculate distillation loss\n",
    "            loss, distill_loss, student_loss = distillation_criterion(\n",
    "                student_logits, teacher_logits, target\n",
    "            )\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            total_distill_loss += distill_loss.item()\n",
    "            total_student_loss += student_loss.item()\n",
    "            \n",
    "            pred = student_logits.argmax(dim=1)\n",
    "            correct += pred.eq(target).sum().item()\n",
    "            total += target.size(0)\n",
    "            \n",
    "            if batch_idx % 100 == 0:\n",
    "                print(f'Epoch {epoch+1}/{epochs}, Batch {batch_idx}, Loss: {loss.item():.4f}')\n",
    "        \n",
    "        accuracy = 100. * correct / total\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        avg_distill = total_distill_loss / len(train_loader)\n",
    "        avg_student = total_student_loss / len(train_loader)\n",
    "        \n",
    "        print(f'Epoch {epoch+1} - Total Loss: {avg_loss:.4f}, '\n",
    "              f'Distill Loss: {avg_distill:.4f}, Student Loss: {avg_student:.4f}, '\n",
    "              f'Accuracy: {accuracy:.2f}%\\n')\n",
    "    \n",
    "    return student\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 6: Evaluation Function\n",
    "# ============================================================================\n",
    "\n",
    "def evaluate_model(model, test_loader, device='cuda'):\n",
    "    \"\"\"Evaluate model accuracy\"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            pred = output.argmax(dim=1)\n",
    "            correct += pred.eq(target).sum().item()\n",
    "            total += target.size(0)\n",
    "    \n",
    "    accuracy = 100. * correct / total\n",
    "    return accuracy\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 7: Main Execution\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\\n\")\n",
    "    \n",
    "    # Load data\n",
    "    train_loader, test_loader = get_data_loaders(batch_size=64)\n",
    "    \n",
    "    # Initialize models\n",
    "    teacher = TeacherModel()\n",
    "    student = StudentModel()\n",
    "    \n",
    "    # Count parameters\n",
    "    teacher_params = sum(p.numel() for p in teacher.parameters())\n",
    "    student_params = sum(p.numel() for p in student.parameters())\n",
    "    print(f\"Teacher Parameters: {teacher_params:,}\")\n",
    "    print(f\"Student Parameters: {student_params:,}\")\n",
    "    print(f\"Compression Ratio: {teacher_params/student_params:.2f}x\\n\")\n",
    "    \n",
    "    # Train teacher model\n",
    "    teacher = train_teacher(teacher, train_loader, epochs=5, device=device)\n",
    "    teacher_accuracy = evaluate_model(teacher, test_loader, device=device)\n",
    "    print(f\"Teacher Test Accuracy: {teacher_accuracy:.2f}%\\n\")\n",
    "    \n",
    "    # Train student with distillation\n",
    "    student = train_student_with_distillation(\n",
    "        student, teacher, train_loader, \n",
    "        epochs=10, temperature=3.0, alpha=0.7, device=device\n",
    "    )\n",
    "    student_accuracy = evaluate_model(student, test_loader, device=device)\n",
    "    print(f\"Student Test Accuracy: {student_accuracy:.2f}%\\n\")\n",
    "    \n",
    "    # Compare with student trained without distillation (baseline)\n",
    "    print(\"Training baseline student without distillation for comparison...\")\n",
    "    baseline_student = StudentModel()\n",
    "    baseline_student = train_teacher(baseline_student, train_loader, epochs=10, device=device)\n",
    "    baseline_accuracy = evaluate_model(baseline_student, test_loader, device=device)\n",
    "    print(f\"Baseline Student Test Accuracy: {baseline_accuracy:.2f}%\\n\")\n",
    "    \n",
    "    # Summary\n",
    "    print(\"=\" * 60)\n",
    "    print(\"FINAL RESULTS:\")\n",
    "    print(f\"Teacher Accuracy: {teacher_accuracy:.2f}%\")\n",
    "    print(f\"Student (with distillation) Accuracy: {student_accuracy:.2f}%\")\n",
    "    print(f\"Student (baseline) Accuracy: {baseline_accuracy:.2f}%\")\n",
    "    print(f\"Improvement from distillation: {student_accuracy - baseline_accuracy:.2f}%\")\n",
    "    print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d302b310",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
