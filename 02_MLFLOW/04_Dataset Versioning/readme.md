# ğŸ“š MLflow Dataset Versioning and Tracking

## à¸„à¸¹à¹ˆà¸¡à¸·à¸­à¸›à¸£à¸°à¸à¸­à¸šà¸à¸²à¸£à¹€à¸£à¸µà¸¢à¸™à¸£à¸¹à¹‰ - à¸—à¸¤à¸©à¸à¸µà¹à¸¥à¸°à¸›à¸à¸´à¸šà¸±à¸•à¸´

---

## ğŸ“‹ à¸ªà¸²à¸£à¸šà¸±à¸

1. [à¸šà¸—à¸™à¸³: à¸„à¸§à¸²à¸¡à¸ªà¸³à¸„à¸±à¸à¸‚à¸­à¸‡ Dataset Versioning](#1-à¸šà¸—à¸™à¸³-à¸„à¸§à¸²à¸¡à¸ªà¸³à¸„à¸±à¸à¸‚à¸­à¸‡-dataset-versioning)
2. [à¸—à¸¤à¸©à¸à¸µ: Data Pipeline à¹ƒà¸™ MLOps](#2-à¸—à¸¤à¸©à¸à¸µ-data-pipeline-à¹ƒà¸™-mlops)
3. [MLflow Dataset Tracking Architecture](#3-mlflow-dataset-tracking-architecture)
4. [à¸à¸²à¸£ Version à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸›à¸£à¸°à¹€à¸ à¸—à¸•à¹ˆà¸²à¸‡à¹†](#4-à¸à¸²à¸£-version-à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸›à¸£à¸°à¹€à¸ à¸—à¸•à¹ˆà¸²à¸‡à¹†)
5. [Best Practices à¹à¸¥à¸° Design Patterns](#5-best-practices-à¹à¸¥à¸°-design-patterns)
6. [à¹à¸šà¸šà¸à¸¶à¸à¸«à¸±à¸”](#6-à¹à¸šà¸šà¸à¸¶à¸à¸«à¸±à¸”)

---

## 1. à¸šà¸—à¸™à¸³: à¸„à¸§à¸²à¸¡à¸ªà¸³à¸„à¸±à¸à¸‚à¸­à¸‡ Dataset Versioning

### 1.1 à¸›à¸±à¸à¸«à¸²à¸—à¸µà¹ˆà¸à¸šà¸šà¹ˆà¸­à¸¢à¹ƒà¸™à¸à¸²à¸£à¸ˆà¸±à¸”à¸à¸²à¸£à¸‚à¹‰à¸­à¸¡à¸¹à¸¥ ML

à¹ƒà¸™à¹‚à¸›à¸£à¹€à¸ˆà¸à¸•à¹Œ Machine Learning à¸—à¸±à¹ˆà¸§à¹„à¸› à¸—à¸µà¸¡à¸¡à¸±à¸à¸›à¸£à¸°à¸ªà¸šà¸›à¸±à¸à¸«à¸²à¹€à¸«à¸¥à¹ˆà¸²à¸™à¸µà¹‰:

```
âŒ "Model à¸—à¸³à¸‡à¸²à¸™à¹„à¸”à¹‰à¸”à¸µà¹€à¸¡à¸·à¹ˆà¸­à¸§à¸²à¸™à¸™à¸µà¹‰ à¹à¸•à¹ˆà¸§à¸±à¸™à¸™à¸µà¹‰ Accuracy à¸•à¸ à¹„à¸¡à¹ˆà¸£à¸¹à¹‰à¸§à¹ˆà¸²à¹€à¸›à¸¥à¸µà¹ˆà¸¢à¸™à¸­à¸°à¹„à¸£à¹„à¸›"
âŒ "à¹ƒà¸Šà¹‰ Data à¸Šà¸¸à¸”à¹„à¸«à¸™à¹ƒà¸™à¸à¸²à¸£ Train Model version à¸—à¸µà¹ˆ deploy à¸­à¸¢à¸¹à¹ˆ?"
âŒ "à¹ƒà¸„à¸£à¹à¸à¹‰à¹„à¸‚ Dataset? à¹€à¸¡à¸·à¹ˆà¸­à¹„à¸«à¸£à¹ˆ? à¹à¸à¹‰à¸­à¸°à¹„à¸£?"
âŒ "à¸•à¹‰à¸­à¸‡à¸à¸²à¸£à¸à¸¥à¸±à¸šà¹„à¸›à¹ƒà¸Šà¹‰ Data version à¹€à¸”à¸´à¸¡ à¹à¸•à¹ˆà¹„à¸¡à¹ˆà¸¡à¸µ backup"
```

### 1.2 Dataset Versioning à¸„à¸·à¸­à¸­à¸°à¹„à¸£?

**Dataset Versioning** à¸„à¸·à¸­à¸à¸£à¸°à¸šà¸§à¸™à¸à¸²à¸£à¸•à¸´à¸”à¸•à¸²à¸¡à¹à¸¥à¸°à¸ˆà¸±à¸”à¸à¸²à¸£à¸à¸²à¸£à¹€à¸›à¸¥à¸µà¹ˆà¸¢à¸™à¹à¸›à¸¥à¸‡à¸‚à¸­à¸‡à¸Šà¸¸à¸”à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸­à¸¢à¹ˆà¸²à¸‡à¹€à¸›à¹‡à¸™à¸£à¸°à¸šà¸š à¸„à¸¥à¹‰à¸²à¸¢à¸à¸±à¸šà¸—à¸µà¹ˆ Git à¸—à¸³à¸à¸±à¸š Source Code

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Dataset Versioning                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚   Dataset v1.0  â”€â”€â–º  Dataset v1.1  â”€â”€â–º  Dataset v2.0        â”‚
â”‚   (1000 rows)        (+500 rows)        (+new column)        â”‚
â”‚       â”‚                  â”‚                   â”‚               â”‚
â”‚       â–¼                  â–¼                   â–¼               â”‚
â”‚   Model v1           Model v2            Model v3            â”‚
â”‚   (Acc: 85%)        (Acc: 87%)          (Acc: 91%)          â”‚
â”‚                                                              â”‚
â”‚   âœ“ Traceability: à¸•à¸´à¸”à¸•à¸²à¸¡à¹„à¸”à¹‰à¸§à¹ˆà¸² Model à¹ƒà¸Šà¹‰ Data version à¹„à¸«à¸™   â”‚
â”‚   âœ“ Reproducibility: à¸ªà¸²à¸¡à¸²à¸£à¸– reproduce à¸œà¸¥à¸¥à¸±à¸à¸˜à¹Œà¹„à¸”à¹‰            â”‚
â”‚   âœ“ Rollback: à¸à¸¥à¸±à¸šà¹„à¸› version à¹€à¸”à¸´à¸¡à¹„à¸”à¹‰à¹€à¸¡à¸·à¹ˆà¸­à¹€à¸à¸´à¸”à¸›à¸±à¸à¸«à¸²          â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.3 à¸—à¸³à¹„à¸¡à¸•à¹‰à¸­à¸‡à¹ƒà¸Šà¹‰ MLflow à¸ªà¸³à¸«à¸£à¸±à¸š Dataset Tracking?

| à¸„à¸¸à¸“à¸ªà¸¡à¸šà¸±à¸•à¸´ | à¸›à¸£à¸°à¹‚à¸¢à¸Šà¸™à¹Œ |
|-----------|---------|
| **Centralized Tracking** | à¹€à¸à¹‡à¸šà¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸—à¸¸à¸à¸­à¸¢à¹ˆà¸²à¸‡à¹„à¸§à¹‰à¸—à¸µà¹ˆà¹€à¸”à¸µà¸¢à¸§ |
| **Metadata Logging** | à¸šà¸±à¸™à¸—à¸¶à¸ schema, statistics, hash |
| **Artifact Storage** | à¹€à¸à¹‡à¸š Dataset files à¸à¸£à¹‰à¸­à¸¡ versioning |
| **UI Dashboard** | à¸”à¸¹à¹à¸¥à¸°à¹€à¸›à¸£à¸µà¸¢à¸šà¹€à¸—à¸µà¸¢à¸š versions à¹„à¸”à¹‰à¸‡à¹ˆà¸²à¸¢ |
| **API Access** | à¹€à¸‚à¹‰à¸²à¸–à¸¶à¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸œà¹ˆà¸²à¸™ Python API |

---

## 2. à¸—à¸¤à¸©à¸à¸µ: Data Pipeline à¹ƒà¸™ MLOps

### 2.1 ML Pipeline Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        ML Data Pipeline                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚  Raw    â”‚    â”‚  Data   â”‚    â”‚ Feature â”‚    â”‚ Model   â”‚
     â”‚  Data   â”‚â”€â”€â”€â–ºâ”‚ Process â”‚â”€â”€â”€â–ºâ”‚  Store  â”‚â”€â”€â”€â–ºâ”‚Training â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚              â”‚              â”‚              â”‚
          â–¼              â–¼              â–¼              â–¼
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚              MLflow Tracking Server                  â”‚
     â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
     â”‚  â”‚Dataset  â”‚ â”‚ Process â”‚ â”‚Feature  â”‚ â”‚ Model   â”‚   â”‚
     â”‚  â”‚Metadata â”‚ â”‚  Logs   â”‚ â”‚Metadata â”‚ â”‚Metrics  â”‚   â”‚
     â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.2 Data Lineage (à¸ªà¸²à¸¢à¸à¸±à¸™à¸˜à¸¸à¹Œà¸‚à¹‰à¸­à¸¡à¸¹à¸¥)

**Data Lineage** à¸„à¸·à¸­à¸à¸²à¸£à¸•à¸´à¸”à¸•à¸²à¸¡à¸—à¸µà¹ˆà¸¡à¸²à¹à¸¥à¸°à¸à¸²à¸£à¹€à¸›à¸¥à¸µà¹ˆà¸¢à¸™à¹à¸›à¸¥à¸‡à¸‚à¸­à¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸•à¸¥à¸­à¸” Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Data Lineage Example                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                              â”‚
â”‚  â”‚ Raw CSV      â”‚ â—„â”€â”€ Source: Customer Database                â”‚
â”‚  â”‚ v1.0         â”‚     Date: 2024-01-01                         â”‚
â”‚  â”‚ 1000 rows    â”‚     Hash: abc123...                          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                              â”‚
â”‚         â”‚                                                       â”‚
â”‚         â–¼ Transformation: Remove nulls, normalize              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                              â”‚
â”‚  â”‚ Cleaned CSV  â”‚ â—„â”€â”€ Process: clean_data.py                   â”‚
â”‚  â”‚ v1.1         â”‚     Date: 2024-01-02                         â”‚
â”‚  â”‚ 950 rows     â”‚     Parent: v1.0                             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                              â”‚
â”‚         â”‚                                                       â”‚
â”‚         â–¼ Transformation: Feature engineering                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                              â”‚
â”‚  â”‚ Feature Set  â”‚ â—„â”€â”€ Process: feature_eng.py                  â”‚
â”‚  â”‚ v1.0         â”‚     Date: 2024-01-03                         â”‚
â”‚  â”‚ 950 rows     â”‚     Parent: cleaned v1.1                     â”‚
â”‚  â”‚ 15 features  â”‚                                              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                              â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.3 Dataset Versioning Strategies

#### Strategy 1: Semantic Versioning (MAJOR.MINOR.PATCH)

```python
# Version Number Meaning:
# MAJOR: Breaking changes (schema change, column removal)
# MINOR: Backward compatible additions (new rows, new columns)
# PATCH: Bug fixes (data corrections)

version_examples = {
    "1.0.0": "Initial dataset",
    "1.1.0": "Added 500 new samples",        # Minor: more data
    "1.1.1": "Fixed typos in labels",        # Patch: corrections
    "2.0.0": "Added 'region' column",        # Major: schema change
}
```

#### Strategy 2: Date-based Versioning

```python
# Format: YYYY-MM-DD or YYYYMMDD
version_examples = {
    "2024-01-01": "January snapshot",
    "2024-02-01": "February snapshot",
    "2024-02-15": "Mid-month update",
}
```

#### Strategy 3: Hash-based Versioning

```python
# à¹ƒà¸Šà¹‰ Content Hash à¹€à¸›à¹‡à¸™ Version Identifier
import hashlib

def get_dataset_version(filepath):
    """à¸ªà¸£à¹‰à¸²à¸‡ version à¸ˆà¸²à¸ file content hash"""
    hash_md5 = hashlib.md5()
    with open(filepath, "rb") as f:
        for chunk in iter(lambda: f.read(4096), b""):
            hash_md5.update(chunk)
    return hash_md5.hexdigest()[:8]  # à¹ƒà¸Šà¹‰ 8 characters à¹à¸£à¸

# à¸œà¸¥à¸¥à¸±à¸à¸˜à¹Œ: "a1b2c3d4"
```

---

## 3. MLflow Dataset Tracking Architecture

### 3.1 MLflow Components à¸ªà¸³à¸«à¸£à¸±à¸š Dataset Tracking

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   MLflow Dataset Tracking                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚   Experiments   â”‚  â”‚      Runs       â”‚  â”‚    Artifacts    â”‚ â”‚
â”‚  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚ â”‚
â”‚  â”‚ â€¢ CSV_Dataset   â”‚  â”‚ â€¢ Parameters    â”‚  â”‚ â€¢ Dataset files â”‚ â”‚
â”‚  â”‚ â€¢ Image_Dataset â”‚  â”‚ â€¢ Metrics       â”‚  â”‚ â€¢ Schema JSON   â”‚ â”‚
â”‚  â”‚ â€¢ JSON_Dataset  â”‚  â”‚ â€¢ Tags          â”‚  â”‚ â€¢ Samples       â”‚ â”‚
â”‚  â”‚ â€¢ Parquet_Data  â”‚  â”‚ â€¢ Dataset Input â”‚  â”‚ â€¢ Metadata      â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚           â”‚                    â”‚                    â”‚           â”‚
â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚                                â–¼                                 â”‚
â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚
â”‚                    â”‚  Tracking Server  â”‚                        â”‚
â”‚                    â”‚   (SQLite/DB)     â”‚                        â”‚
â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3.2 MLflow Dataset Types

MLflow à¸£à¸­à¸‡à¸£à¸±à¸š Dataset à¸«à¸¥à¸²à¸¢à¸›à¸£à¸°à¹€à¸ à¸—:

```python
from mlflow.data.pandas_dataset import PandasDataset
from mlflow.data.numpy_dataset import NumpyDataset
from mlflow.data.spark_dataset import SparkDataset

# 1. Pandas Dataset
dataset = mlflow.data.from_pandas(
    df,                          # DataFrame
    source="path/to/file.csv",   # à¹à¸«à¸¥à¹ˆà¸‡à¸—à¸µà¹ˆà¸¡à¸²
    name="my_dataset",           # à¸Šà¸·à¹ˆà¸­ dataset
    targets="label_column"       # target column (optional)
)

# 2. NumPy Dataset
dataset = mlflow.data.from_numpy(
    features=X,                  # feature array
    targets=y,                   # target array
    source="sklearn.datasets"
)
```

### 3.3 Logging Dataset to MLflow

```python
# à¹‚à¸„à¸£à¸‡à¸ªà¸£à¹‰à¸²à¸‡à¸à¸²à¸£ Log Dataset à¹à¸šà¸šà¸ªà¸¡à¸šà¸¹à¸£à¸“à¹Œ

with mlflow.start_run(run_name="dataset_v1"):
    
    # 1. à¸ªà¸£à¹‰à¸²à¸‡ MLflow Dataset object
    dataset = mlflow.data.from_pandas(df, source=filepath)
    
    # 2. Log Dataset Input (à¸ªà¸³à¸„à¸±à¸!)
    mlflow.log_input(dataset, context="training")
    
    # 3. Log Parameters (Metadata)
    mlflow.log_param("dataset_version", "1.0.0")
    mlflow.log_param("dataset_name", "customer_data")
    mlflow.log_param("source_type", "csv")
    mlflow.log_param("creation_date", "2024-01-01")
    
    # 4. Log Metrics (Statistics)
    mlflow.log_metric("num_rows", len(df))
    mlflow.log_metric("num_columns", len(df.columns))
    mlflow.log_metric("missing_values", df.isnull().sum().sum())
    
    # 5. Log Artifacts (Files)
    mlflow.log_artifact(filepath, artifact_path="datasets")
```

### 3.4 Dataset Statistics à¸—à¸µà¹ˆà¸„à¸§à¸£ Track

```python
def get_comprehensive_dataset_stats(df):
    """
    à¸„à¸³à¸™à¸§à¸“ Statistics à¸—à¸µà¹ˆà¸ªà¸³à¸„à¸±à¸à¸ªà¸³à¸«à¸£à¸±à¸š Dataset
    """
    stats = {
        # Basic Info
        "num_rows": len(df),
        "num_columns": len(df.columns),
        "memory_usage_mb": df.memory_usage(deep=True).sum() / (1024**2),
        
        # Data Quality
        "missing_total": df.isnull().sum().sum(),
        "missing_percentage": (df.isnull().sum().sum() / df.size) * 100,
        "duplicate_rows": df.duplicated().sum(),
        
        # Column Types
        "numeric_columns": len(df.select_dtypes(include=['number']).columns),
        "categorical_columns": len(df.select_dtypes(include=['object']).columns),
        "datetime_columns": len(df.select_dtypes(include=['datetime']).columns),
    }
    
    # Target Distribution (à¸–à¹‰à¸²à¸¡à¸µ target column)
    if 'target' in df.columns:
        stats["target_distribution"] = df['target'].value_counts().to_dict()
        stats["class_balance_ratio"] = df['target'].value_counts().min() / df['target'].value_counts().max()
    
    return stats
```

---

## 4. à¸à¸²à¸£ Version à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸›à¸£à¸°à¹€à¸ à¸—à¸•à¹ˆà¸²à¸‡à¹†

### 4.1 CSV Dataset Versioning

**Use Cases:** Tabular data, Customer data, Transaction logs

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CSV Dataset Workflow                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Read CSV   â”‚ â”€â”€â”€â–º â”‚  Validate   â”‚ â”€â”€â”€â–º â”‚  Log to    â”‚ â”‚
â”‚  â”‚             â”‚      â”‚  Schema     â”‚      â”‚  MLflow    â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚         â”‚                    â”‚                    â”‚        â”‚
â”‚         â–¼                    â–¼                    â–¼        â”‚
â”‚   pd.read_csv()      Check columns,       log_input(),    â”‚
â”‚                      dtypes, nulls        log_artifact()  â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key Tracking Elements:**

```python
# à¸ªà¸´à¹ˆà¸‡à¸—à¸µà¹ˆà¸„à¸§à¸£ Track à¸ªà¸³à¸«à¸£à¸±à¸š CSV Dataset
tracking_elements = {
    "parameters": [
        "dataset_version",
        "file_hash_md5",      # à¸ªà¸³à¸«à¸£à¸±à¸š integrity check
        "schema_version",
        "encoding",           # e.g., "utf-8"
        "delimiter",          # e.g., ","
    ],
    "metrics": [
        "num_rows",
        "num_columns", 
        "missing_values",
        "memory_mb",
        "target_distribution",  # à¸ªà¸³à¸«à¸£à¸±à¸š classification
    ],
    "artifacts": [
        "dataset.csv",
        "schema.json",
        "statistics.json",
    ]
}
```

### 4.2 Image Dataset Versioning

**Use Cases:** Computer Vision, Image Classification, Object Detection

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Image Dataset Structure                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  dataset/                                                    â”‚
â”‚  â”œâ”€â”€ v1/                                                    â”‚
â”‚  â”‚   â”œâ”€â”€ class_a/                                           â”‚
â”‚  â”‚   â”‚   â”œâ”€â”€ img_0001.png                                   â”‚
â”‚  â”‚   â”‚   â”œâ”€â”€ img_0002.png                                   â”‚
â”‚  â”‚   â”‚   â””â”€â”€ ...                                            â”‚
â”‚  â”‚   â”œâ”€â”€ class_b/                                           â”‚
â”‚  â”‚   â”‚   â”œâ”€â”€ img_0001.png                                   â”‚
â”‚  â”‚   â”‚   â””â”€â”€ ...                                            â”‚
â”‚  â”‚   â””â”€â”€ dataset_info.json                                  â”‚
â”‚  â””â”€â”€ v2/                                                    â”‚
â”‚      â”œâ”€â”€ class_a/                                           â”‚
â”‚      â”œâ”€â”€ class_b/                                           â”‚
â”‚      â”œâ”€â”€ class_c/           â—„â”€â”€ New class added             â”‚
â”‚      â””â”€â”€ dataset_info.json                                  â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key Tracking Elements:**

```python
# à¸ªà¸´à¹ˆà¸‡à¸—à¸µà¹ˆà¸„à¸§à¸£ Track à¸ªà¸³à¸«à¸£à¸±à¸š Image Dataset
tracking_elements = {
    "parameters": [
        "dataset_version",
        "image_format",         # PNG, JPEG
        "image_size",           # "64x64", "224x224"
        "num_classes",
        "class_names",
        "augmentation_applied", # True/False
    ],
    "metrics": [
        "total_images",
        "images_per_class",     # à¹à¸•à¹ˆà¸¥à¸° class
        "class_balance_ratio",
        "total_size_mb",
    ],
    "artifacts": [
        "samples/",             # à¸•à¸±à¸§à¸­à¸¢à¹ˆà¸²à¸‡à¸£à¸¹à¸›à¸ˆà¸²à¸à¹à¸•à¹ˆà¸¥à¸° class
        "dataset_info.json",
        "class_distribution.png",
    ]
}
```

### 4.3 JSON Dataset Versioning

**Use Cases:** NLP data, API responses, Semi-structured data

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  JSON Dataset Structure                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  // text_classification_v1.json                             â”‚
â”‚  [                                                          â”‚
â”‚    {                                                        â”‚
â”‚      "id": 1,                                               â”‚
â”‚      "text": "The team won the championship",               â”‚
â”‚      "label": "sports",                                     â”‚
â”‚      "metadata": {                                          â”‚
â”‚        "source": "synthetic",                               â”‚
â”‚        "confidence": 0.95                                   â”‚
â”‚      }                                                      â”‚
â”‚    },                                                       â”‚
â”‚    ...                                                      â”‚
â”‚  ]                                                          â”‚
â”‚                                                              â”‚
â”‚  // Changes in v2:                                          â”‚
â”‚  // - More samples (100 â†’ 200)                              â”‚
â”‚  // - Added "timestamp" field                               â”‚
â”‚  // - Added new label categories                            â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key Tracking Elements:**

```python
# à¸ªà¸´à¹ˆà¸‡à¸—à¸µà¹ˆà¸„à¸§à¸£ Track à¸ªà¸³à¸«à¸£à¸±à¸š JSON Dataset
tracking_elements = {
    "parameters": [
        "dataset_version",
        "task_type",           # classification, NER, etc.
        "num_classes",
        "text_field",          # field à¸—à¸µà¹ˆà¹€à¸à¹‡à¸š text
        "label_field",
    ],
    "metrics": [
        "total_samples",
        "avg_text_length",
        "max_text_length",
        "vocabulary_size",
        "label_distribution",
    ],
    "artifacts": [
        "dataset.json",
        "label_stats.json",
        "vocabulary.txt",
    ]
}
```

### 4.4 Parquet Dataset Versioning

**Use Cases:** Big data, Data warehousing, Columnar analytics

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Parquet vs CSV Comparison                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  Feature              â”‚    CSV    â”‚   Parquet               â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€         â”‚
â”‚  Storage Format       â”‚    Row    â”‚   Columnar              â”‚
â”‚  Compression          â”‚    No     â”‚   Built-in              â”‚
â”‚  Schema               â”‚    No     â”‚   Embedded              â”‚
â”‚  Read Speed           â”‚   Slow    â”‚   Fast                  â”‚
â”‚  Write Speed          â”‚   Fast    â”‚   Moderate              â”‚
â”‚  File Size            â”‚   Large   â”‚   Small                 â”‚
â”‚  Column Selection     â”‚   Full    â”‚   Selective             â”‚
â”‚  Data Types           â”‚   Text    â”‚   Native                â”‚
â”‚                                                              â”‚
â”‚  Best For:                                                  â”‚
â”‚  - CSV: Small data, interchange, human-readable             â”‚
â”‚  - Parquet: Large data, analytics, ML pipelines             â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key Tracking Elements:**

```python
# à¸ªà¸´à¹ˆà¸‡à¸—à¸µà¹ˆà¸„à¸§à¸£ Track à¸ªà¸³à¸«à¸£à¸±à¸š Parquet Dataset
tracking_elements = {
    "parameters": [
        "dataset_version",
        "compression",         # snappy, gzip, etc.
        "row_group_size",
        "partitioning",        # partition columns
    ],
    "metrics": [
        "num_records",
        "num_columns",
        "file_size_mb",
        "compression_ratio",
        "row_groups",
    ],
    "artifacts": [
        "dataset.parquet",
        "schema.json",
        "statistics.json",
    ]
}
```

---

## 5. Best Practices à¹à¸¥à¸° Design Patterns

### 5.1 Dataset Versioning Best Practices

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              âœ… Best Practices Checklist                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  1. VERSION CONTROL                                         â”‚
â”‚     â–¡ à¹ƒà¸Šà¹‰ Semantic Versioning (MAJOR.MINOR.PATCH)           â”‚
â”‚     â–¡ à¸šà¸±à¸™à¸—à¸¶à¸ version à¸—à¸¸à¸à¸„à¸£à¸±à¹‰à¸‡à¸—à¸µà¹ˆ data à¹€à¸›à¸¥à¸µà¹ˆà¸¢à¸™               â”‚
â”‚     â–¡ Document à¸à¸²à¸£à¹€à¸›à¸¥à¸µà¹ˆà¸¢à¸™à¹à¸›à¸¥à¸‡à¹ƒà¸™ changelog                   â”‚
â”‚                                                              â”‚
â”‚  2. DATA INTEGRITY                                          â”‚
â”‚     â–¡ à¸„à¸³à¸™à¸§à¸“à¹à¸¥à¸°à¸šà¸±à¸™à¸—à¸¶à¸ file hash (MD5/SHA256)                â”‚
â”‚     â–¡ Validate schema à¸à¹ˆà¸­à¸™ log                              â”‚
â”‚     â–¡ Check data quality metrics                            â”‚
â”‚                                                              â”‚
â”‚  3. METADATA                                                â”‚
â”‚     â–¡ à¸šà¸±à¸™à¸—à¸¶à¸ creation date                                  â”‚
â”‚     â–¡ à¸šà¸±à¸™à¸—à¸¶à¸ source/origin                                  â”‚
â”‚     â–¡ à¸šà¸±à¸™à¸—à¸¶à¸ processing steps                               â”‚
â”‚     â–¡ à¸šà¸±à¸™à¸—à¸¶à¸ owner/responsible person                       â”‚
â”‚                                                              â”‚
â”‚  4. ARTIFACTS                                               â”‚
â”‚     â–¡ Log sample data à¸ªà¸³à¸«à¸£à¸±à¸š quick preview                  â”‚
â”‚     â–¡ Log schema information                                â”‚
â”‚     â–¡ Log statistics summary                                â”‚
â”‚                                                              â”‚
â”‚  5. NAMING CONVENTION                                       â”‚
â”‚     â–¡ à¹ƒà¸Šà¹‰ meaningful run names                              â”‚
â”‚     â–¡ à¹ƒà¸Šà¹‰ consistent experiment naming                      â”‚
â”‚     â–¡ à¹ƒà¸Šà¹‰ descriptive parameter names                       â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 5.2 File Integrity Verification

```python
import hashlib

def calculate_file_hash(filepath, algorithm='md5'):
    """
    à¸„à¸³à¸™à¸§à¸“ Hash à¸‚à¸­à¸‡à¹„à¸Ÿà¸¥à¹Œà¸ªà¸³à¸«à¸£à¸±à¸š integrity verification
    
    à¹ƒà¸Šà¹‰à¸ªà¸³à¸«à¸£à¸±à¸š:
    1. à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸§à¹ˆà¸²à¹„à¸Ÿà¸¥à¹Œà¹„à¸¡à¹ˆà¸–à¸¹à¸à¹à¸à¹‰à¹„à¸‚
    2. à¸£à¸°à¸šà¸¸ unique version à¸‚à¸­à¸‡ dataset
    3. Detect duplicate datasets
    """
    if algorithm == 'md5':
        hash_obj = hashlib.md5()
    elif algorithm == 'sha256':
        hash_obj = hashlib.sha256()
    
    with open(filepath, "rb") as f:
        for chunk in iter(lambda: f.read(4096), b""):
            hash_obj.update(chunk)
    
    return hash_obj.hexdigest()

# à¸•à¸±à¸§à¸­à¸¢à¹ˆà¸²à¸‡à¸à¸²à¸£à¹ƒà¸Šà¹‰à¸‡à¸²à¸™
file_hash = calculate_file_hash("data/customers.csv")
mlflow.log_param("file_hash_md5", file_hash)
```

### 5.3 Schema Management

```python
def log_schema_info(df, version):
    """
    à¸šà¸±à¸™à¸—à¸¶à¸ Schema Information à¸‚à¸­à¸‡ DataFrame
    """
    schema = {
        "version": version,
        "columns": list(df.columns),
        "dtypes": {col: str(dtype) for col, dtype in df.dtypes.items()},
        "nullable": {col: df[col].isnull().any() for col in df.columns},
        "unique_counts": {col: df[col].nunique() for col in df.columns},
    }
    return schema

# Schema Change Detection
def detect_schema_changes(schema_v1, schema_v2):
    """
    à¸•à¸£à¸§à¸ˆà¸ˆà¸±à¸šà¸à¸²à¸£à¹€à¸›à¸¥à¸µà¹ˆà¸¢à¸™à¹à¸›à¸¥à¸‡ Schema à¸£à¸°à¸«à¸§à¹ˆà¸²à¸‡ 2 versions
    """
    changes = {
        "added_columns": set(schema_v2["columns"]) - set(schema_v1["columns"]),
        "removed_columns": set(schema_v1["columns"]) - set(schema_v2["columns"]),
        "dtype_changes": {},
    }
    
    # Check dtype changes
    common_cols = set(schema_v1["columns"]) & set(schema_v2["columns"])
    for col in common_cols:
        if schema_v1["dtypes"][col] != schema_v2["dtypes"][col]:
            changes["dtype_changes"][col] = {
                "from": schema_v1["dtypes"][col],
                "to": schema_v2["dtypes"][col]
            }
    
    return changes
```

### 5.4 Dataset Comparison Pattern

```python
def compare_dataset_versions(run_id_v1, run_id_v2):
    """
    à¹€à¸›à¸£à¸µà¸¢à¸šà¹€à¸—à¸µà¸¢à¸š 2 versions à¸‚à¸­à¸‡ dataset
    """
    client = mlflow.tracking.MlflowClient()
    
    run_v1 = client.get_run(run_id_v1)
    run_v2 = client.get_run(run_id_v2)
    
    comparison = {
        "v1": {
            "version": run_v1.data.params.get("dataset_version"),
            "rows": run_v1.data.metrics.get("num_rows"),
            "columns": run_v1.data.metrics.get("num_columns"),
        },
        "v2": {
            "version": run_v2.data.params.get("dataset_version"),
            "rows": run_v2.data.metrics.get("num_rows"),
            "columns": run_v2.data.metrics.get("num_columns"),
        },
        "diff": {
            "rows_change": run_v2.data.metrics.get("num_rows", 0) - 
                          run_v1.data.metrics.get("num_rows", 0),
            "columns_change": run_v2.data.metrics.get("num_columns", 0) - 
                             run_v1.data.metrics.get("num_columns", 0),
        }
    }
    
    return comparison
```

---

## 6. à¹à¸šà¸šà¸à¸¶à¸à¸«à¸±à¸”

### à¹à¸šà¸šà¸à¸¶à¸à¸«à¸±à¸”à¸—à¸µà¹ˆ 1: Basic Dataset Versioning

**à¸§à¸±à¸•à¸–à¸¸à¸›à¸£à¸°à¸ªà¸‡à¸„à¹Œ:** à¸à¸¶à¸à¸à¸²à¸£à¸ªà¸£à¹‰à¸²à¸‡à¹à¸¥à¸° version CSV dataset

```python
# TODO: Complete the following tasks

# Task 1.1: à¸ªà¸£à¹‰à¸²à¸‡ Customer Dataset Version 3
# - à¹€à¸à¸´à¹ˆà¸¡ column: "loyalty_points" (integer)
# - à¹€à¸à¸´à¹ˆà¸¡ column: "last_purchase_date" (datetime)
# - à¹€à¸à¸´à¹ˆà¸¡à¸ˆà¸³à¸™à¸§à¸™ samples à¹€à¸›à¹‡à¸™ 2000 rows

def create_customer_dataset_v3():
    """
    à¸ªà¸£à¹‰à¸²à¸‡ Version 3 à¸‚à¸­à¸‡ customer dataset
    """
    # Your code here
    pass

# Task 1.2: Log à¹„à¸›à¸¢à¸±à¸‡ MLflow à¸à¸£à¹‰à¸­à¸¡ metadata à¸—à¸µà¹ˆà¸„à¸£à¸šà¸–à¹‰à¸§à¸™
def log_dataset_v3(df, filepath):
    """
    Log dataset v3 à¹„à¸›à¸¢à¸±à¸‡ MLflow
    """
    # Your code here
    pass
```

### à¹à¸šà¸šà¸à¸¶à¸à¸«à¸±à¸”à¸—à¸µà¹ˆ 2: Image Dataset Augmentation Tracking

**à¸§à¸±à¸•à¸–à¸¸à¸›à¸£à¸°à¸ªà¸‡à¸„à¹Œ:** à¸à¸¶à¸à¸à¸²à¸£ track augmented image dataset

```python
# TODO: Complete the following tasks

# Task 2.1: à¸ªà¸£à¹‰à¸²à¸‡ Augmented Image Dataset
# - Apply rotation (90Â°, 180Â°, 270Â°)
# - Apply horizontal flip
# - Track augmentation parameters

def create_augmented_dataset(original_dir, output_dir):
    """
    à¸ªà¸£à¹‰à¸²à¸‡ augmented version à¸‚à¸­à¸‡ image dataset
    """
    # Your code here
    pass

# Task 2.2: Log augmentation metadata
def log_augmented_dataset(dataset_dir, augmentation_params):
    """
    Log augmented dataset à¸à¸£à¹‰à¸­à¸¡ augmentation parameters
    """
    # Your code here
    pass
```

### à¹à¸šà¸šà¸à¸¶à¸à¸«à¸±à¸”à¸—à¸µà¹ˆ 3: Dataset Lineage Tracking

**à¸§à¸±à¸•à¸–à¸¸à¸›à¸£à¸°à¸ªà¸‡à¸„à¹Œ:** à¸à¸¶à¸à¸à¸²à¸£ track à¸„à¸§à¸²à¸¡à¸ªà¸±à¸¡à¸à¸±à¸™à¸˜à¹Œà¸£à¸°à¸«à¸§à¹ˆà¸²à¸‡ raw à¹à¸¥à¸° processed data

```python
# TODO: Complete the following tasks

# Task 3.1: à¸ªà¸£à¹‰à¸²à¸‡ Raw â†’ Cleaned â†’ Featured pipeline
def track_data_pipeline():
    """
    Track dataset transformations:
    1. Raw data â†’ Log as "raw_v1"
    2. Cleaned data â†’ Log as "cleaned_v1" with parent="raw_v1"
    3. Featured data â†’ Log as "featured_v1" with parent="cleaned_v1"
    """
    # Your code here
    pass

# Task 3.2: Query lineage information
def get_dataset_lineage(run_id):
    """
    à¸”à¸¶à¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥ lineage à¸‚à¸­à¸‡ dataset à¸ˆà¸²à¸ run_id
    """
    # Your code here
    pass
```

### à¹à¸šà¸šà¸à¸¶à¸à¸«à¸±à¸”à¸—à¸µà¹ˆ 4: Multi-format Dataset Comparison

**à¸§à¸±à¸•à¸–à¸¸à¸›à¸£à¸°à¸ªà¸‡à¸„à¹Œ:** à¹€à¸›à¸£à¸µà¸¢à¸šà¹€à¸—à¸µà¸¢à¸š dataset format à¸•à¹ˆà¸²à¸‡à¹†

```python
# TODO: Complete the following tasks

# Task 4.1: à¸ªà¸£à¹‰à¸²à¸‡ same dataset à¹ƒà¸™ 3 formats
# - CSV
# - Parquet  
# - JSON

# Task 4.2: Log à¹à¸•à¹ˆà¸¥à¸° format à¹à¸¥à¸°à¹€à¸›à¸£à¸µà¸¢à¸šà¹€à¸—à¸µà¸¢à¸š
# - File size
# - Read time
# - Write time

def compare_dataset_formats(df):
    """
    à¹€à¸›à¸£à¸µà¸¢à¸šà¹€à¸—à¸µà¸¢à¸š performance à¸‚à¸­à¸‡ format à¸•à¹ˆà¸²à¸‡à¹†
    """
    # Your code here
    pass
```

---

## ğŸ“– à¹€à¸­à¸à¸ªà¸²à¸£à¸­à¹‰à¸²à¸‡à¸­à¸´à¸‡

1. [MLflow Documentation - Dataset Tracking](https://mlflow.org/docs/latest/tracking.html#datasets)
2. [MLflow Data Module API](https://mlflow.org/docs/latest/python_api/mlflow.data.html)
3. [Data Version Control Best Practices](https://dvc.org/doc)
4. [Apache Parquet Documentation](https://parquet.apache.org/docs/)

---

## ğŸ¯ à¸ªà¸£à¸¸à¸›

à¸«à¸¥à¸±à¸‡à¸ˆà¸²à¸à¸¨à¸¶à¸à¸©à¸²à¹€à¸™à¸·à¹‰à¸­à¸«à¸²à¸™à¸µà¹‰à¹à¸¥à¹‰à¸§ à¸™à¸±à¸à¸¨à¸¶à¸à¸©à¸²à¸„à¸§à¸£à¸ªà¸²à¸¡à¸²à¸£à¸–:

1. âœ… à¸­à¸˜à¸´à¸šà¸²à¸¢à¸„à¸§à¸²à¸¡à¸ªà¸³à¸„à¸±à¸à¸‚à¸­à¸‡ Dataset Versioning à¹ƒà¸™ ML Pipeline
2. âœ… à¹ƒà¸Šà¹‰ MLflow à¹ƒà¸™à¸à¸²à¸£ track à¹à¸¥à¸° version datasets à¸›à¸£à¸°à¹€à¸ à¸—à¸•à¹ˆà¸²à¸‡à¹†
3. âœ… à¸­à¸­à¸à¹à¸šà¸š metadata schema à¸ªà¸³à¸«à¸£à¸±à¸š dataset tracking
4. âœ… à¹€à¸›à¸£à¸µà¸¢à¸šà¹€à¸—à¸µà¸¢à¸š dataset versions à¹à¸¥à¸° track lineage
5. âœ… à¸›à¸£à¸°à¸¢à¸¸à¸à¸•à¹Œà¹ƒà¸Šà¹‰ best practices à¹ƒà¸™à¹‚à¸›à¸£à¹€à¸ˆà¸à¸•à¹Œà¸ˆà¸£à¸´à¸‡

---

**Happy Learning! ğŸš€**