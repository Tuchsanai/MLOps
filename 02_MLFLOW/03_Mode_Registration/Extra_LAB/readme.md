# üìù ‡∏Å‡∏≤‡∏£‡∏ö‡πâ‡∏≤‡∏ô: MLflow Model Registry

**‡∏ß‡∏¥‡∏ä‡∏≤:** Machine Learning Operations (MLOps)  
**‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠:** MLflow Model Registry

---

## üìã ‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ

1. ‡∏ó‡∏≥‡∏Å‡∏≤‡∏£‡∏ö‡πâ‡∏≤‡∏ô‡πÉ‡∏ô Jupyter Notebook ‡∏´‡∏£‡∏∑‡∏≠ Python script
2. MLflow Server ‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏±‡∏ô‡∏≠‡∏¢‡∏π‡πà‡∏ó‡∏µ‡πà `http://127.0.0.1:5000`
3. ‡πÉ‡∏™‡πà‡∏ä‡∏∑‡πà‡∏≠-‡∏ô‡∏≤‡∏°‡∏™‡∏Å‡∏∏‡∏• ‡πÅ‡∏•‡∏∞‡∏£‡∏´‡∏±‡∏™‡∏ô‡∏±‡∏Å‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏™‡πà‡∏á

### üì§ ‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏™‡πà‡∏á‡πÉ‡∏´‡πâ TA ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö
- ‡πÑ‡∏ü‡∏•‡πå code (.ipynb ‡∏´‡∏£‡∏∑‡∏≠ .py) ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏ô
- Screenshot ‡∏´‡∏ô‡πâ‡∏≤ MLflow UI ‡πÅ‡∏™‡∏î‡∏á Registered Model ‡πÅ‡∏•‡∏∞ Versions
- ‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö accuracy ‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞ Model Version
- ‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏™‡∏£‡∏∏‡∏õ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ç‡πâ‡∏≠

---

# üìö ‡∏Ç‡πâ‡∏≠‡∏ó‡∏µ‡πà 1: Wine Quality Classification Model Registry

## üéØ ‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå
‡∏ù‡∏∂‡∏Å‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ MLflow Model Registry ‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Wine Quality ‡πÇ‡∏î‡∏¢‡∏Å‡∏≤‡∏£ train ‡∏´‡∏•‡∏≤‡∏¢ models, ‡∏•‡∏á‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô‡πÄ‡∏Ç‡πâ‡∏≤ Registry, ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ versions ‡πÅ‡∏•‡∏∞ aliases

## üìñ ‡πÇ‡∏à‡∏ó‡∏¢‡πå

‡∏ô‡∏±‡∏Å‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏∞‡∏ö‡∏ö‡∏à‡∏≥‡πÅ‡∏ô‡∏Å‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡πÑ‡∏ß‡∏ô‡πå (Wine Quality Classification) ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ MLflow Model Registry ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏´‡∏•‡∏≤‡∏¢‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô

### ‡∏á‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏ó‡∏≥:

1. **Train Model 3 Versions**
   - Version 1: `DecisionTreeClassifier` (baseline)
   - Version 2: `RandomForestClassifier` (n_estimators=100)
   - Version 3: `GradientBoostingClassifier` (n_estimators=100)
   
2. **‡∏•‡∏á‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô Models ‡πÄ‡∏Ç‡πâ‡∏≤ Registry**
   - ‡πÉ‡∏ä‡πâ‡∏ä‡∏∑‡πà‡∏≠ Registered Model: `wine-quality-classifier`
   - ‡πÄ‡∏û‡∏¥‡πà‡∏° Description ‡∏ó‡∏µ‡πà‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô
   - ‡πÄ‡∏û‡∏¥‡πà‡∏° Tags ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Registered Model: `task`, `dataset`, `team`
   - ‡πÄ‡∏û‡∏¥‡πà‡∏° Tags ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞ Version: `model_type`, `status`

3. **‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Aliases**
   - ‡∏Å‡∏≥‡∏´‡∏ô‡∏î `baseline` alias ‡πÉ‡∏´‡πâ Version 1
   - ‡∏Å‡∏≥‡∏´‡∏ô‡∏î `staging` alias ‡πÉ‡∏´‡πâ Version ‡∏ó‡∏µ‡πà‡∏°‡∏µ accuracy ‡∏™‡∏π‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö 2
   - ‡∏Å‡∏≥‡∏´‡∏ô‡∏î `champion` alias ‡πÉ‡∏´‡πâ Version ‡∏ó‡∏µ‡πà‡∏°‡∏µ accuracy ‡∏™‡∏π‡∏á‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î

4. **‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏•‡∏∞‡∏ó‡∏î‡∏™‡∏≠‡∏ö Models**
   - ‡πÇ‡∏´‡∏•‡∏î champion model ‡∏à‡∏≤‡∏Å Registry
   - ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• test set 10 ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á
   - ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏• predictions ‡πÅ‡∏•‡∏∞ actual values

### üîß Code ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô - ‡∏Ç‡πâ‡∏≠‡∏ó‡∏µ‡πà 1

```python
# === ‡∏Å‡∏≤‡∏£‡∏ö‡πâ‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏ó‡∏µ‡πà 1: Wine Quality Classification ===
# ‡∏ä‡∏∑‡πà‡∏≠-‡∏ô‡∏≤‡∏°‡∏™‡∏Å‡∏∏‡∏•: _______________
# ‡∏£‡∏´‡∏±‡∏™‡∏ô‡∏±‡∏Å‡∏®‡∏∂‡∏Å‡∏©‡∏≤: _______________

import mlflow
from mlflow.tracking import MlflowClient
from sklearn.datasets import load_wine
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.metrics import accuracy_score, f1_score
from mlflow.models import infer_signature
import numpy as np

# === Setup ===
MLFLOW_TRACKING_URI = "http://127.0.0.1:5000"
mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)
client = MlflowClient()

# === ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Wine Quality ===
wine = load_wine()
X_train, X_test, y_train, y_test = train_test_split(
    wine.data, wine.target, test_size=0.2, random_state=42
)

print(f"‚úÖ ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Wine Quality ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
print(f"üìä Training samples: {len(X_train)}")
print(f"üìä Test samples: {len(X_test)}")
print(f"üìä Features: {wine.feature_names}")
print(f"üìä Classes: {wine.target_names}")

# === ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏ä‡∏∑‡πà‡∏≠ Registered Model ===
MODEL_NAME = "wine-quality-classifier"

# === ‡∏™‡∏£‡πâ‡∏≤‡∏á Experiment ===
mlflow.set_experiment("wine-quality-homework")

# TODO: ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô code ‡∏ï‡πà‡∏≠‡∏à‡∏≤‡∏Å‡∏ô‡∏µ‡πâ
# 1. Train ‡πÅ‡∏•‡∏∞‡∏•‡∏á‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô Model Version 1 (DecisionTree)
# 2. Train ‡πÅ‡∏•‡∏∞‡∏•‡∏á‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô Model Version 2 (RandomForest)
# 3. Train ‡πÅ‡∏•‡∏∞‡∏•‡∏á‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô Model Version 3 (GradientBoosting)
# 4. ‡πÄ‡∏û‡∏¥‡πà‡∏° Description ‡πÅ‡∏•‡∏∞ Tags
# 5. ‡∏Å‡∏≥‡∏´‡∏ô‡∏î Aliases
# 6. ‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏•‡∏∞‡∏ó‡∏î‡∏™‡∏≠‡∏ö Champion Model

```

### üìã Expected Output - ‡∏Ç‡πâ‡∏≠‡∏ó‡∏µ‡πà 1

```
‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö Wine Quality Models:

| Version | Algorithm | Accuracy | F1-Score |
|---------|-----------|----------|----------|
| 1 | DecisionTree | XX% | XX |
| 2 | RandomForest | XX% | XX |
| 3 | GradientBoosting | XX% | XX |

Champion Model: Version 3 (GradientBoosting)
Alias Assignment:
- baseline ‚Üí Version 1
- staging ‚Üí Version 2
- champion ‚Üí Version 3
```

---

# üìö ‡∏Ç‡πâ‡∏≠‡∏ó‡∏µ‡πà 2: MNIST Digit Classification with PyTorch Neural Networks

## üéØ ‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå
‡∏ù‡∏∂‡∏Å‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ MLflow Model Registry ‡∏Å‡∏±‡∏ö PyTorch Deep Learning models ‡πÇ‡∏î‡∏¢‡∏Å‡∏≤‡∏£ train neural networks ‡∏´‡∏•‡∏≤‡∏¢‡∏™‡∏ñ‡∏≤‡∏õ‡∏±‡∏ï‡∏¢‡∏Å‡∏£‡∏£‡∏°, ‡∏•‡∏á‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô‡πÄ‡∏Ç‡πâ‡∏≤ Registry, ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ versions ‡πÅ‡∏•‡∏∞ aliases

## üìñ ‡πÇ‡∏à‡∏ó‡∏¢‡πå

‡∏ô‡∏±‡∏Å‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏∞‡∏ö‡∏ö‡∏à‡∏≥‡πÅ‡∏ô‡∏Å‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç MNIST (Digit Classification) ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ PyTorch Neural Networks ‡πÅ‡∏•‡πâ‡∏ß‡∏•‡∏á‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô‡πÄ‡∏Ç‡πâ‡∏≤ MLflow Model Registry

### ‡∏á‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏ó‡∏≥:

#### 1Ô∏è‚É£ **Prepare Data**
   - ‡πÇ‡∏´‡∏•‡∏î MNIST dataset (‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç 0-9)
   - ‡πÅ‡∏ö‡πà‡∏á data ‡πÄ‡∏õ‡πá‡∏ô training set (70%) ‡πÅ‡∏•‡∏∞ test set (30%)
   - Normalize images ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô range [0, 1]
   - ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô samples, image shape, ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô classes

#### 2Ô∏è‚É£ **Define Neural Network Architectures**
   
   ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á Neural Network 3 ‡πÅ‡∏ö‡∏ö:
   
   ```
   Version 1: Shallow Neural Network (Baseline)
   ‚îú‚îÄ‚îÄ Input: 28√ó28 = 784 pixels
   ‚îú‚îÄ‚îÄ Hidden Layer 1: 128 neurons + ReLU
   ‚îú‚îÄ‚îÄ Output Layer: 10 neurons (for 10 digits)
   ‚îî‚îÄ‚îÄ Parameters: ~100K
   
   Version 2: Medium Deep Network (Improved)
   ‚îú‚îÄ‚îÄ Input: 28√ó28 = 784 pixels
   ‚îú‚îÄ‚îÄ Hidden Layer 1: 256 neurons + ReLU
   ‚îú‚îÄ‚îÄ Hidden Layer 2: 128 neurons + ReLU
   ‚îú‚îÄ‚îÄ Output Layer: 10 neurons
   ‚îî‚îÄ‚îÄ Parameters: ~130K
   
   Version 3: Deeper Network with Dropout (Champion)
   ‚îú‚îÄ‚îÄ Input: 28√ó28 = 784 pixels
   ‚îú‚îÄ‚îÄ Hidden Layer 1: 512 neurons + ReLU + Dropout(0.2)
   ‚îú‚îÄ‚îÄ Hidden Layer 2: 256 neurons + ReLU + Dropout(0.2)
   ‚îú‚îÄ‚îÄ Hidden Layer 3: 128 neurons + ReLU + Dropout(0.1)
   ‚îú‚îÄ‚îÄ Output Layer: 10 neurons
   ‚îî‚îÄ‚îÄ Parameters: ~300K
   ```

#### 3Ô∏è‚É£ **Train Models 3 Versions**
   
   ‡∏ï‡πâ‡∏≠‡∏á‡∏ù‡∏∂‡∏Å Neural Networks 3 ‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô:
   
   | Version | Architecture | Epochs | Learning Rate | Batch Size | Expected Accuracy |
   |---------|--------------|--------|---------------|------------|------------------|
   | **V1** | Shallow NN | 50 | 0.001 | 64 | ~95% |
   | **V2** | Medium Deep | 100 | 0.001 | 32 | ~97% |
   | **V3** | Deep + Dropout | 150 | 0.0005 | 32 | ~98% |
   
   **Hyperparameters ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å:**
   - hidden_size_1, hidden_size_2, hidden_size_3
   - dropout_rate
   - learning_rate
   - batch_size
   - num_epochs
   - optimizer type
   - loss function

#### 4Ô∏è‚É£ **‡∏•‡∏á‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô Models ‡πÄ‡∏Ç‡πâ‡∏≤ Registry**
   - ‡πÉ‡∏ä‡πâ‡∏ä‡∏∑‡πà‡∏≠ Registered Model: `mnist-digit-classifier`
   - ‡πÄ‡∏û‡∏¥‡πà‡∏° Description ‡∏ó‡∏µ‡πà‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢ architecture
   - ‡πÄ‡∏û‡∏¥‡πà‡∏° Tags ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Registered Model:
     - `task`: classification
     - `dataset`: mnist
     - `framework`: pytorch
     - `team`: your-team-name
   
   - ‡πÄ‡∏û‡∏¥‡πà‡∏° Tags ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞ Version:
     - `model_type`: architecture name (e.g., "shallow-nn", "medium-deep-nn")
     - `status`: baseline / improved / champion
     - `num_parameters`: total parameters
     - `training_time_seconds`: ‡πÄ‡∏ß‡∏•‡∏≤‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å

#### 5Ô∏è‚É£ **‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å Training Metrics**
   
   ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞ epoch:
   - `train_loss`: Loss ‡∏ö‡∏ô training set
   - `train_accuracy`: Accuracy ‡∏ö‡∏ô training set
   - `test_loss`: Loss ‡∏ö‡∏ô test set (optional, ‡∏ó‡∏∏‡∏Å 5 epochs)
   - `test_accuracy`: Accuracy ‡∏ö‡∏ô test set (optional, ‡∏ó‡∏∏‡∏Å 5 epochs)
   
   ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏à‡∏ö‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å:
   - `final_test_accuracy`: Accuracy ‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢
   - `final_test_loss`: Loss ‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢
   - `model_size_mb`: ‡∏Ç‡∏ô‡∏≤‡∏î‡πÑ‡∏ü‡∏•‡πå model

#### 6Ô∏è‚É£ **‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Aliases**
   - ‡∏Å‡∏≥‡∏´‡∏ô‡∏î `baseline` alias ‡πÉ‡∏´‡πâ Version 1
   - ‡∏Å‡∏≥‡∏´‡∏ô‡∏î `staging` alias ‡πÉ‡∏´‡πâ Version ‡∏ó‡∏µ‡πà‡∏°‡∏µ accuracy ‡∏™‡∏π‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö 2
   - ‡∏Å‡∏≥‡∏´‡∏ô‡∏î `champion` alias ‡πÉ‡∏´‡πâ Version ‡∏ó‡∏µ‡πà‡∏°‡∏µ accuracy ‡∏™‡∏π‡∏á‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î

#### 7Ô∏è‚É£ **‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏•‡∏∞‡∏ó‡∏î‡∏™‡∏≠‡∏ö Models**
   - ‡πÇ‡∏´‡∏•‡∏î champion model ‡∏à‡∏≤‡∏Å Registry ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ Alias
   - ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• test set 20 ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á
   - ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏• predictions, actual values, ‡πÅ‡∏•‡∏∞ confidence scores
   - ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì accuracy ‡∏ö‡∏ô test set

#### 8Ô∏è‚É£ **‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö Models**
   
   ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö:
   
   ```
   | Metric | Version 1 | Version 2 | Version 3 |
   |--------|-----------|-----------|-----------|
   | Accuracy | | | |
   | Loss | | | |
   | Parameters | | | |
   | Training Time | | | |
   | Model Size | | | |
   ```

### üîß Code ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô - ‡∏Ç‡πâ‡∏≠‡∏ó‡∏µ‡πà 2

```python
# === ‡∏Å‡∏≤‡∏£‡∏ö‡πâ‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏ó‡∏µ‡πà 2: MNIST Digit Classification with PyTorch ===
# ‡∏ä‡∏∑‡πà‡∏≠-‡∏ô‡∏≤‡∏°‡∏™‡∏Å‡∏∏‡∏•: _______________
# ‡∏£‡∏´‡∏±‡∏™‡∏ô‡∏±‡∏Å‡∏®‡∏∂‡∏Å‡∏©‡∏≤: _______________

import mlflow
from mlflow.tracking import MlflowClient
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
from sklearn.metrics import accuracy_score, f1_score
from mlflow.models import infer_signature
import time
import numpy as np

# === Setup ===
MLFLOW_TRACKING_URI = "http://127.0.0.1:5000"
mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)
client = MlflowClient()

# === Data Preparation ===
# ‡πÇ‡∏´‡∏•‡∏î MNIST dataset
print("üì• ‡πÇ‡∏´‡∏•‡∏î MNIST Dataset...")

transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.1307,), (0.3081,))  # MNIST mean and std
])

train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)
test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)

# ‡∏™‡∏£‡πâ‡∏≤‡∏á DataLoaders
train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)

print(f"‚úÖ ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• MNIST ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
print(f"üìä Training samples: {len(train_dataset)}")
print(f"üìä Test samples: {len(test_dataset)}")
print(f"üìä Input shape: (1, 28, 28)")
print(f"üìä Number of classes: 10 (digits 0-9)")

# === ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏ä‡∏∑‡πà‡∏≠ Registered Model ===
MODEL_NAME = "mnist-digit-classifier"

# === ‡∏™‡∏£‡πâ‡∏≤‡∏á Experiment ===
mlflow.set_experiment("mnist-pytorch-homework")

# === Define Neural Network Classes ===

class ShallowNN(nn.Module):
    """Version 1: Shallow Neural Network (Baseline)"""
    def __init__(self, input_size=784, hidden_size=128, num_classes=10):
        super(ShallowNN, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(hidden_size, num_classes)
    
    def forward(self, x):
        x = x.view(x.size(0), -1)  # Flatten
        out = self.fc1(x)
        out = self.relu(out)
        out = self.fc2(out)
        return out


class MediumDeepNN(nn.Module):
    """Version 2: Medium Deep Neural Network (Improved)"""
    def __init__(self, input_size=784, hidden_size1=256, hidden_size2=128, num_classes=10):
        super(MediumDeepNN, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size1)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(hidden_size1, hidden_size2)
        self.fc3 = nn.Linear(hidden_size2, num_classes)
    
    def forward(self, x):
        x = x.view(x.size(0), -1)  # Flatten
        out = self.fc1(x)
        out = self.relu(out)
        out = self.fc2(out)
        out = self.relu(out)
        out = self.fc3(out)
        return out


class DeepNNWithDropout(nn.Module):
    """Version 3: Deep Neural Network with Dropout (Champion)"""
    def __init__(self, input_size=784, hidden_size1=512, hidden_size2=256, 
                 hidden_size3=128, dropout_rate=0.2, num_classes=10):
        super(DeepNNWithDropout, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size1)
        self.relu = nn.ReLU()
        self.dropout1 = nn.Dropout(dropout_rate)
        
        self.fc2 = nn.Linear(hidden_size1, hidden_size2)
        self.dropout2 = nn.Dropout(dropout_rate)
        
        self.fc3 = nn.Linear(hidden_size2, hidden_size3)
        self.dropout3 = nn.Dropout(dropout_rate * 0.5)
        
        self.fc4 = nn.Linear(hidden_size3, num_classes)
    
    def forward(self, x):
        x = x.view(x.size(0), -1)  # Flatten
        out = self.fc1(x)
        out = self.relu(out)
        out = self.dropout1(out)
        
        out = self.fc2(out)
        out = self.relu(out)
        out = self.dropout2(out)
        
        out = self.fc3(out)
        out = self.relu(out)
        out = self.dropout3(out)
        
        out = self.fc4(out)
        return out


# === Helper Functions ===

def count_parameters(model):
    """‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô parameters ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏Ç‡∏≠‡∏á model"""
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def train_epoch(model, train_loader, criterion, optimizer, device):
    """‡∏ù‡∏∂‡∏Å model 1 epoch"""
    model.train()
    total_loss = 0
    correct = 0
    total = 0
    
    for images, labels in train_loader:
        images = images.to(device)
        labels = labels.to(device)
        
        # Forward pass
        outputs = model(images)
        loss = criterion(outputs, labels)
        
        # Backward pass
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        # Statistics
        total_loss += loss.item()
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()
    
    avg_loss = total_loss / len(train_loader)
    accuracy = 100 * correct / total
    
    return avg_loss, accuracy


def evaluate(model, test_loader, criterion, device):
    """‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏• model ‡∏ö‡∏ô test set"""
    model.eval()
    total_loss = 0
    correct = 0
    total = 0
    
    with torch.no_grad():
        for images, labels in test_loader:
            images = images.to(device)
            labels = labels.to(device)
            
            outputs = model(images)
            loss = criterion(outputs, labels)
            
            total_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    
    avg_loss = total_loss / len(test_loader)
    accuracy = 100 * correct / total
    
    return avg_loss, accuracy


def get_sample_input(train_loader, device):
    """‡∏î‡∏∂‡∏á batch ‡πÅ‡∏£‡∏Å‡∏à‡∏≤‡∏Å train set ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö signature"""
    for images, labels in train_loader:
        return images[:5].to(device)


# === Device Configuration ===
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"\nüñ•Ô∏è  ‡πÉ‡∏ä‡πâ Device: {device}")

# TODO: ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô code ‡∏ï‡πà‡∏≠‡∏à‡∏≤‡∏Å‡∏ô‡∏µ‡πâ
# 1. Train ‡πÅ‡∏•‡∏∞‡∏•‡∏á‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô Model Version 1 (ShallowNN)
# 2. Train ‡πÅ‡∏•‡∏∞‡∏•‡∏á‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô Model Version 2 (MediumDeepNN)
# 3. Train ‡πÅ‡∏•‡∏∞‡∏•‡∏á‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô Model Version 3 (DeepNNWithDropout)
# 4. ‡πÄ‡∏û‡∏¥‡πà‡∏° Description ‡πÅ‡∏•‡∏∞ Tags
# 5. ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏ú‡∏• accuracy ‡∏Ç‡∏≠‡∏á 3 versions
# 6. ‡∏Å‡∏≥‡∏´‡∏ô‡∏î Aliases (baseline, staging, champion)
# 7. ‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏•‡∏∞‡∏ó‡∏î‡∏™‡∏≠‡∏ö Champion Model
# 8. ‡∏ó‡∏≥‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ö‡∏ô 20 ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏à‡∏≤‡∏Å test set

# ===== Hints =====
# - ‡πÉ‡∏ä‡πâ `mlflow.pytorch.log_model()` ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏•‡∏á‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô PyTorch models
# - ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å training metrics ‡∏î‡πâ‡∏ß‡∏¢ `mlflow.log_metric()`
# - ‡πÉ‡∏ä‡πâ `mlflow.log_param()` ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö hyperparameters
# - ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å champion model ‡πÇ‡∏î‡∏¢‡∏î‡∏π accuracy ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î
# - ‡πÉ‡∏ä‡πâ `mlflow.pytorch.load_model()` ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÇ‡∏´‡∏•‡∏î model ‡∏à‡∏≤‡∏Å Registry

```

### üìã Expected Output - ‡∏Ç‡πâ‡∏≠‡∏ó‡∏µ‡πà 2

```
‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö MNIST PyTorch Models:

| Version | Architecture | Accuracy | Loss | Parameters | Training Time |
|---------|--------------|----------|------|------------|---------------|
| 1 | Shallow NN | 95-96% | XX | ~100K | XX sec |
| 2 | Medium Deep | 97-98% | XX | ~130K | XX sec |
| 3 | Deep + Dropout | 98-99% | XX | ~300K | XX sec |

Champion Model: Version 3 (Deep NN with Dropout)
Alias Assignment:
- baseline ‚Üí Version 1
- staging ‚Üí Version 2
- champion ‚Üí Version 3

Sample Predictions (20 samples):
Predicted: [X, X, X, ...]
Actual:    [X, X, X, ...]
Confidence: [XX%, XX%, XX%, ...]
```

---

# üìã Grading Rubric

## ‡∏Ç‡πâ‡∏≠‡∏ó‡∏µ‡πà 1: Wine Quality (Scikit-learn)

| Criteria | Points | Notes |
|----------|--------|-------|
| **Data Loading & Preparation** | 10 | ‡πÇ‡∏´‡∏•‡∏î wine dataset, split train/test |
| **Model 1 (DecisionTree)** | 10 | Train, log metrics, register model |
| **Model 2 (RandomForest)** | 10 | Train, log metrics, register model |
| **Model 3 (GradientBoosting)** | 10 | Train, log metrics, register model |
| **MLflow Registry Setup** | 10 | Description, Tags, Aliases |
| **Model Loading & Testing** | 10 | Load champion, predict, show results |
| **Code Quality** | 10 | Comments, structure, error handling |
| **Subtotal** | **70** | - |

## ‡∏Ç‡πâ‡∏≠‡∏ó‡∏µ‡πà 2: MNIST (PyTorch)

| Criteria | Points | Notes |
|----------|--------|-------|
| **Data Preparation** | 10 | ‡πÇ‡∏´‡∏•‡∏î MNIST, normalize, DataLoader |
| **Model 1 (Shallow NN)** | 10 | Train, log metrics, register model |
| **Model 2 (Medium Deep)** | 10 | Train, log metrics, register model |
| **Model 3 (Deep + Dropout)** | 10 | Train, log metrics, register model |
| **MLflow Registry Setup** | 10 | Description, Tags, Aliases |
| **Model Loading & Testing** | 10 | Load champion, predict, show results |
| **Code Quality** | 10 | Comments, structure, error handling |
| **Subtotal** | **70** | - |

## Overall Quality

| Criteria | Points | Notes |
|----------|--------|-------|
| **MLflow UI Screenshots** | 10 | Show registered models and versions |
| **Comparison Tables** | 10 | Clear presentation of results |
| **Code Organization** | 10 | Well-structured, easy to read |
| **Subtotal** | **30** | - |

## **TOTAL GRADE: 100 Points**

---

# üí° Tips & Tricks

## ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏ó‡∏µ‡πà 1 (Wine Quality - Scikit-learn)

1. **Model Registration Pattern:**
   ```python
   with mlflow.start_run(run_name="wine-v1"):
       mlflow.log_params({...})
       model = DecisionTreeClassifier(...)
       model.fit(X_train, y_train)
       mlflow.log_metrics({...})
       signature = infer_signature(X_train, model.predict(X_train))
       mlflow.sklearn.log_model(
           model, 
           "model", 
           signature=signature,
           registered_model_name="wine-quality-classifier"
       )
   ```

2. **Tag Management:**
   ```python
   client.set_registered_model_tag(model_name, "task", "classification")
   client.set_model_version_tag(model_name, "1", "model_type", "DecisionTree")
   ```

3. **Alias Management:**
   ```python
   client.set_registered_model_alias(model_name, "champion", "3")
   champion = client.get_model_version_by_alias(model_name, "champion")
   ```

## ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏ó‡∏µ‡πà 2 (MNIST - PyTorch)

1. **Device Management:**
   ```python
   device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
   model = model.to(device)
   images = images.to(device)
   ```

2. **Model Signature for PyTorch:**
   ```python
   sample_input = get_sample_input(train_loader, device)
   signature = infer_signature(
       sample_input.cpu().numpy().reshape(5, -1),
       model(sample_input).detach().cpu().numpy()
   )
   ```

3. **Logging Metrics by Epoch:**
   ```python
   for epoch in range(num_epochs):
       train_loss, train_acc = train_epoch(...)
       test_loss, test_acc = evaluate(...)
       
       mlflow.log_metric("train_loss", train_loss, step=epoch)
       mlflow.log_metric("train_accuracy", train_acc, step=epoch)
       if epoch % 5 == 0:
           mlflow.log_metric("test_loss", test_loss, step=epoch)
           mlflow.log_metric("test_accuracy", test_acc, step=epoch)
   ```

4. **Viewing Results:**
   - Open browser: http://127.0.0.1:5000
   - ‡πÑ‡∏õ‡∏ó‡∏µ‡πà "Models" tab ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏π registered models
   - ‡πÑ‡∏õ‡∏ó‡∏µ‡πà "Experiments" tab ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏π runs ‡πÅ‡∏•‡∏∞ metrics

---

# üîó ‡∏ó‡∏£‡∏±‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå

## Scikit-learn (‡∏Ç‡πâ‡∏≠‡∏ó‡∏µ‡πà 1)
- [Scikit-learn Classification Models](https://scikit-learn.org/stable/supervised_learning.html)
- [Wine Dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_wine.html)

## PyTorch (‡∏Ç‡πâ‡∏≠‡∏ó‡∏µ‡πà 2)
- [PyTorch Neural Networks](https://pytorch.org/docs/stable/nn.html)
- [PyTorch Loss Functions](https://pytorch.org/docs/stable/nn.html#loss-functions)
- [PyTorch Optimizers](https://pytorch.org/docs/stable/optim.html)
- [MNIST Dataset](https://pytorch.org/vision/stable/datasets.html#mnist)

## MLflow
- [MLflow Model Registry](https://mlflow.org/docs/latest/model-registry.html)
- [MLflow Scikit-learn Integration](https://mlflow.org/docs/latest/models.html#scikit-learn)
- [MLflow PyTorch Integration](https://mlflow.org/docs/latest/models.html#pytorch)
- [MLflow Python API](https://mlflow.org/docs/latest/python_api/mlflow.html)

---

# üìä ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏µ‡πà‡∏Ñ‡∏≤‡∏î‡∏´‡∏ß‡∏±‡∏á

## ‡∏Ç‡πâ‡∏≠‡∏ó‡∏µ‡πà 1 - Wine Quality Accuracy
- DecisionTree: ~90-92%
- RandomForest: ~95-97%
- GradientBoosting: ~97-99% ‚úì (Champion)

## ‡∏Ç‡πâ‡∏≠‡∏ó‡∏µ‡πà 2 - MNIST Accuracy
- Shallow NN: ~95-96%
- Medium Deep: ~95-97%
- Deep + Dropout: ~97-99% ‚úì (Champion)

---

# üìù Submission Checklist

- [ ] ‡∏Ç‡πâ‡∏≠‡∏ó‡∏µ‡πà 1: code ‡∏ó‡∏µ‡πà complete
- [ ] ‡∏Ç‡πâ‡∏≠‡∏ó‡∏µ‡πà 1: screenshot MLflow UI
- [ ] ‡∏Ç‡πâ‡∏≠‡∏ó‡∏µ‡πà 1: table comparing 3 models
- [ ] ‡∏Ç‡πâ‡∏≠‡∏ó‡∏µ‡πà 2: code ‡∏ó‡∏µ‡πà complete
- [ ] ‡∏Ç‡πâ‡∏≠‡∏ó‡∏µ‡πà 2: screenshot MLflow UI
- [ ] ‡∏Ç‡πâ‡∏≠‡∏ó‡∏µ‡πà 2: table comparing 3 models
- [ ] ‡∏Ç‡πâ‡∏≠‡∏ó‡∏µ‡πà 2: sample predictions output
- [ ] ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏• (summary) ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á 2 ‡∏Ç‡πâ‡∏≠
- [ ] ‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏™‡πà‡∏á‡∏°‡∏µ‡∏ä‡∏∑‡πà‡∏≠‡πÅ‡∏•‡∏∞‡∏£‡∏´‡∏±‡∏™‡∏ô‡∏±‡∏Å‡∏®‡∏∂‡∏Å‡∏©‡∏≤

---

**‡∏Ç‡∏≠‡πÉ‡∏´‡πâ‡πÇ‡∏ä‡∏Ñ‡∏î‡∏µ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏Å‡∏≤‡∏£‡∏ö‡πâ‡∏≤‡∏ô! üéì**

**MLflow Server URL:** http://127.0.0.1:5000

---
