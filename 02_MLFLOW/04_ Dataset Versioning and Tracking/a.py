# %% [markdown]
# # üöÄ Lab: Prompt Optimization using MLflow
#
# **‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ (Learning Objectives)**
# - ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏´‡∏•‡∏±‡∏Å‡∏Å‡∏≤‡∏£‡∏Ç‡∏≠‡∏á Prompt Engineering ‡πÅ‡∏•‡∏∞ Optimization
# - ‡πÉ‡∏ä‡πâ MLflow ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£ track ‡πÅ‡∏•‡∏∞‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö prompts ‡∏ï‡πà‡∏≤‡∏á‡πÜ
# - ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå metrics ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏•‡∏∑‡∏≠‡∏Å prompt ‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î
# - ‡∏™‡∏£‡πâ‡∏≤‡∏á systematic approach ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö prompts
#
# **‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏°‡∏∑‡∏≠‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ:** Python, MLflow, OpenAI/Ollama API
#
# ---

# %% [markdown]
# ## üì¶ Part 1: Environment Setup
#
# ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á libraries ‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô

# %%
# Install required packages (run once)
# !pip install mlflow openai tiktoken pandas numpy scikit-learn

# %%
# Import libraries
import mlflow
import mlflow.pyfunc
from mlflow.tracking import MlflowClient
import json
import time
import hashlib
from datetime import datetime
from typing import List, Dict, Any, Optional
import pandas as pd
import numpy as np

# For LLM interaction
import os
from openai import OpenAI

# For evaluation metrics
from sklearn.metrics import accuracy_score
import re

print("‚úÖ Libraries imported successfully!")

# %% [markdown]
# ## ‚öôÔ∏è Part 2: Configuration
#
# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ MLflow ‡πÅ‡∏•‡∏∞ LLM API

# %%
# MLflow Configuration
MLFLOW_TRACKING_URI = "http://localhost:5000"  # ‡∏´‡∏£‡∏∑‡∏≠‡πÉ‡∏ä‡πâ local: "mlruns"
EXPERIMENT_NAME = "prompt-optimization-lab"

# Set tracking URI
mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)

# Create or get experiment
experiment = mlflow.get_experiment_by_name(EXPERIMENT_NAME)
if experiment is None:
    experiment_id = mlflow.create_experiment(
        EXPERIMENT_NAME,
        tags={"project": "prompt-engineering", "version": "1.0"}
    )
    print(f"‚úÖ Created new experiment: {EXPERIMENT_NAME} (ID: {experiment_id})")
else:
    experiment_id = experiment.experiment_id
    print(f"‚úÖ Using existing experiment: {EXPERIMENT_NAME} (ID: {experiment_id})")

mlflow.set_experiment(EXPERIMENT_NAME)

# %%
# LLM Configuration
# Option 1: OpenAI
# client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
# MODEL_NAME = "gpt-3.5-turbo"

# Option 2: Ollama (Local)
client = OpenAI(
    base_url="http://localhost:11434/v1",
    api_key="ollama"  # Ollama doesn't need real API key
)
MODEL_NAME = "llama3.2"  # ‡∏´‡∏£‡∏∑‡∏≠ model ‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡∏°‡∏µ

print(f"‚úÖ LLM configured: {MODEL_NAME}")

# %% [markdown]
# ## üìù Part 3: Define Test Dataset
#
# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏î‡∏™‡∏≠‡∏ö prompts
#
# **Task: Sentiment Classification**

# %%
# Test dataset for sentiment classification
test_data = [
    {
        "text": "‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏î‡∏µ‡∏°‡∏≤‡∏Å ‡∏™‡πà‡∏á‡πÄ‡∏£‡πá‡∏ß ‡∏õ‡∏£‡∏∞‡∏ó‡∏±‡∏ö‡πÉ‡∏à‡∏°‡∏≤‡∏Å‡∏Ñ‡∏£‡∏±‡∏ö",
        "expected": "positive"
    },
    {
        "text": "‡πÅ‡∏¢‡πà‡∏°‡∏≤‡∏Å ‡∏£‡∏≠‡∏ô‡∏≤‡∏ô‡∏°‡∏≤‡∏Å ‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏ï‡∏£‡∏á‡∏õ‡∏Å",
        "expected": "negative"
    },
    {
        "text": "‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤‡∏Å‡πá‡πÇ‡∏≠‡πÄ‡∏Ñ‡∏ô‡∏∞ ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏î‡∏µ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÅ‡∏¢‡πà",
        "expected": "neutral"
    },
    {
        "text": "‡∏ä‡∏≠‡∏ö‡∏°‡∏≤‡∏Å‡πÄ‡∏•‡∏¢ ‡∏à‡∏∞‡∏Å‡∏•‡∏±‡∏ö‡∏°‡∏≤‡∏ã‡∏∑‡πâ‡∏≠‡πÉ‡∏´‡∏°‡πà‡πÅ‡∏ô‡πà‡∏ô‡∏≠‡∏ô!",
        "expected": "positive"
    },
    {
        "text": "‡∏ú‡∏¥‡∏î‡∏´‡∏ß‡∏±‡∏á‡∏°‡∏≤‡∏Å ‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡πÑ‡∏°‡πà‡∏Ñ‡∏∏‡πâ‡∏°‡∏£‡∏≤‡∏Ñ‡∏≤",
        "expected": "negative"
    },
    {
        "text": "‡∏ò‡∏£‡∏£‡∏°‡∏î‡∏≤ ‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ",
        "expected": "neutral"
    },
    {
        "text": "‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡∏î‡∏µ‡πÄ‡∏¢‡∏µ‡πà‡∏¢‡∏° ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÄ‡∏•‡∏¢‡∏Ñ‡∏£‡∏±‡∏ö",
        "expected": "positive"
    },
    {
        "text": "‡πÑ‡∏°‡πà‡∏ã‡∏∑‡πâ‡∏≠‡∏≠‡∏µ‡∏Å‡πÅ‡∏•‡πâ‡∏ß ‡πÄ‡∏™‡∏µ‡∏¢‡πÄ‡∏á‡∏¥‡∏ô‡πÄ‡∏õ‡∏•‡πà‡∏≤",
        "expected": "negative"
    },
    {
        "text": "‡∏£‡∏≤‡∏Ñ‡∏≤‡∏û‡∏≠‡∏£‡∏±‡∏ö‡πÑ‡∏î‡πâ ‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏Å‡πá‡∏á‡∏±‡πâ‡∏ô‡πÜ",
        "expected": "neutral"
    },
    {
        "text": "‡∏Ç‡∏≠‡∏á‡πÅ‡∏ó‡πâ 100% ‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡πÄ‡∏Å‡∏¥‡∏ô‡∏£‡∏≤‡∏Ñ‡∏≤ ‡∏™‡∏∏‡∏î‡∏¢‡∏≠‡∏î!",
        "expected": "positive"
    }
]

print(f"‚úÖ Test dataset loaded: {len(test_data)} samples")
print(f"   - Positive: {sum(1 for d in test_data if d['expected'] == 'positive')}")
print(f"   - Negative: {sum(1 for d in test_data if d['expected'] == 'negative')}")
print(f"   - Neutral: {sum(1 for d in test_data if d['expected'] == 'neutral')}")

# %% [markdown]
# ## üéØ Part 4: Define Prompt Templates
#
# ‡∏™‡∏£‡πâ‡∏≤‡∏á prompt templates ‡∏´‡∏•‡∏≤‡∏¢‡πÅ‡∏ö‡∏ö‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û

# %%
# Prompt templates for comparison
prompt_templates = {
    "v1_basic": {
        "name": "Basic Prompt",
        "description": "Simple instruction without examples",
        "template": """Classify the sentiment of the following Thai text as positive, negative, or neutral.

Text: {text}

Respond with only one word: positive, negative, or neutral"""
    },
    
    "v2_detailed": {
        "name": "Detailed Instruction",
        "description": "More detailed instructions with criteria",
        "template": """You are a sentiment analysis expert for Thai language.

Analyze the sentiment of the given text and classify it into one of three categories:
- positive: The text expresses satisfaction, happiness, or recommendation
- negative: The text expresses dissatisfaction, complaint, or disappointment  
- neutral: The text is neither clearly positive nor negative

Text to analyze: {text}

Important: Respond with exactly one word (positive, negative, or neutral)."""
    },
    
    "v3_few_shot": {
        "name": "Few-Shot Learning",
        "description": "Prompt with examples (few-shot)",
        "template": """Classify Thai text sentiment. Here are examples:

Example 1: "‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤‡∏î‡∏µ‡∏°‡∏≤‡∏Å ‡∏ä‡∏≠‡∏ö‡∏°‡∏≤‡∏Å" ‚Üí positive
Example 2: "‡πÅ‡∏¢‡πà‡∏°‡∏≤‡∏Å ‡πÑ‡∏°‡πà‡∏õ‡∏£‡∏∞‡∏ó‡∏±‡∏ö‡πÉ‡∏à‡πÄ‡∏•‡∏¢" ‚Üí negative  
Example 3: "‡∏Å‡πá‡∏û‡∏≠‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ" ‚Üí neutral

Now classify this text: {text}

Answer (one word only):"""
    },
    
    "v4_cot": {
        "name": "Chain-of-Thought",
        "description": "Prompt encouraging step-by-step reasoning",
        "template": """Analyze the sentiment of this Thai text step by step.

Text: {text}

Step 1: Identify key sentiment words or phrases
Step 2: Determine if overall tone is positive, negative, or neutral
Step 3: Give final classification

Final answer (one word: positive/negative/neutral):"""
    },
    
    "v5_role_play": {
        "name": "Role-Play Expert",
        "description": "Persona-based prompt",
        "template": """You are a senior Thai language analyst at a leading e-commerce company. 
You have 10 years of experience analyzing customer reviews.

Your task: Classify the sentiment of this customer review.

Review: {text}

Based on your expertise, what is the sentiment? (Answer: positive, negative, or neutral)"""
    }
}

print(f"‚úÖ Defined {len(prompt_templates)} prompt templates:")
for key, value in prompt_templates.items():
    print(f"   - {key}: {value['name']}")

# %% [markdown]
# ## üîß Part 5: Helper Functions
#
# ‡∏™‡∏£‡πâ‡∏≤‡∏á functions ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡∏ß‡∏±‡∏î‡∏ú‡∏•

# %%
def call_llm(prompt: str, temperature: float = 0.0) -> Dict[str, Any]:
    """
    ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ LLM ‡πÅ‡∏•‡∏∞‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤ response ‡∏û‡∏£‡πâ‡∏≠‡∏° metadata
    """
    start_time = time.time()
    
    try:
        response = client.chat.completions.create(
            model=MODEL_NAME,
            messages=[{"role": "user", "content": prompt}],
            temperature=temperature,
            max_tokens=100
        )
        
        latency = time.time() - start_time
        
        return {
            "success": True,
            "response": response.choices[0].message.content.strip(),
            "latency": latency,
            "tokens_used": response.usage.total_tokens if response.usage else 0,
            "prompt_tokens": response.usage.prompt_tokens if response.usage else 0,
            "completion_tokens": response.usage.completion_tokens if response.usage else 0
        }
    except Exception as e:
        return {
            "success": False,
            "response": str(e),
            "latency": time.time() - start_time,
            "tokens_used": 0,
            "prompt_tokens": 0,
            "completion_tokens": 0
        }

# %%
def extract_sentiment(response: str) -> str:
    """
    ‡∏î‡∏∂‡∏á sentiment ‡∏à‡∏≤‡∏Å LLM response
    """
    response_lower = response.lower()
    
    # ‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç: ‡∏ñ‡πâ‡∏≤‡∏û‡∏ö‡∏Ñ‡∏≥‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡πÉ‡∏ô response
    for sentiment in ["positive", "negative", "neutral"]:
        if sentiment in response_lower:
            # ‡∏´‡∏≤ occurrence ‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢
            last_pos = response_lower.rfind(sentiment)
            if last_pos != -1:
                return sentiment
    
    return "unknown"

# %%
def calculate_metrics(predictions: List[str], ground_truth: List[str]) -> Dict[str, float]:
    """
    ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì metrics ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏•
    """
    # Accuracy
    correct = sum(1 for p, g in zip(predictions, ground_truth) if p == g)
    accuracy = correct / len(ground_truth)
    
    # Per-class metrics
    classes = ["positive", "negative", "neutral"]
    metrics = {"accuracy": accuracy}
    
    for cls in classes:
        # True Positives, False Positives, False Negatives
        tp = sum(1 for p, g in zip(predictions, ground_truth) if p == cls and g == cls)
        fp = sum(1 for p, g in zip(predictions, ground_truth) if p == cls and g != cls)
        fn = sum(1 for p, g in zip(predictions, ground_truth) if p != cls and g == cls)
        
        # Precision, Recall, F1
        precision = tp / (tp + fp) if (tp + fp) > 0 else 0
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0
        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
        
        metrics[f"{cls}_precision"] = precision
        metrics[f"{cls}_recall"] = recall
        metrics[f"{cls}_f1"] = f1
    
    # Macro F1
    metrics["macro_f1"] = np.mean([metrics[f"{cls}_f1"] for cls in classes])
    
    return metrics

# %%
def get_prompt_hash(prompt_template: str) -> str:
    """
    ‡∏™‡∏£‡πâ‡∏≤‡∏á hash ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö prompt template (‡πÉ‡∏ä‡πâ track versions)
    """
    return hashlib.md5(prompt_template.encode()).hexdigest()[:8]

print("‚úÖ Helper functions defined!")

# %% [markdown]
# ## üß™ Part 6: Run Prompt Optimization Experiment
#
# ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞ prompt template ‡πÅ‡∏•‡∏∞ log ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÑ‡∏õ‡∏¢‡∏±‡∏á MLflow

# %%
def run_prompt_experiment(
    prompt_key: str,
    prompt_config: Dict[str, str],
    test_data: List[Dict],
    temperature: float = 0.0
) -> Dict[str, Any]:
    """
    ‡∏£‡∏±‡∏ô experiment ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö prompt template ‡∏´‡∏ô‡∏∂‡πà‡∏á‡∏ï‡∏±‡∏ß
    """
    template = prompt_config["template"]
    results = []
    predictions = []
    ground_truth = []
    total_latency = 0
    total_tokens = 0
    
    print(f"\nüîÑ Testing: {prompt_config['name']}")
    print("-" * 50)
    
    for i, sample in enumerate(test_data):
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á prompt ‡∏à‡∏≤‡∏Å template
        prompt = template.format(text=sample["text"])
        
        # ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å LLM
        llm_result = call_llm(prompt, temperature)
        
        if llm_result["success"]:
            # Extract sentiment
            predicted = extract_sentiment(llm_result["response"])
            predictions.append(predicted)
            ground_truth.append(sample["expected"])
            
            # Collect stats
            total_latency += llm_result["latency"]
            total_tokens += llm_result["tokens_used"]
            
            # Store result
            results.append({
                "text": sample["text"][:50] + "...",
                "expected": sample["expected"],
                "predicted": predicted,
                "correct": predicted == sample["expected"],
                "latency": llm_result["latency"],
                "raw_response": llm_result["response"][:100]
            })
            
            status = "‚úì" if predicted == sample["expected"] else "‚úó"
            print(f"  [{i+1}/{len(test_data)}] {status} Expected: {sample['expected']}, Got: {predicted}")
        else:
            print(f"  [{i+1}/{len(test_data)}] ‚ùå Error: {llm_result['response']}")
            predictions.append("error")
            ground_truth.append(sample["expected"])
    
    # Calculate metrics
    metrics = calculate_metrics(predictions, ground_truth)
    metrics["avg_latency"] = total_latency / len(test_data)
    metrics["total_tokens"] = total_tokens
    metrics["avg_tokens_per_request"] = total_tokens / len(test_data)
    
    return {
        "prompt_key": prompt_key,
        "config": prompt_config,
        "results": results,
        "metrics": metrics,
        "predictions": predictions,
        "ground_truth": ground_truth
    }

# %%
# Run experiments for all prompt templates
all_experiments = {}

print("=" * 60)
print("üöÄ STARTING PROMPT OPTIMIZATION EXPERIMENTS")
print("=" * 60)

for prompt_key, prompt_config in prompt_templates.items():
    
    # Start MLflow run
    with mlflow.start_run(run_name=f"prompt_{prompt_key}") as run:
        
        # Log parameters
        mlflow.log_param("prompt_version", prompt_key)
        mlflow.log_param("prompt_name", prompt_config["name"])
        mlflow.log_param("prompt_description", prompt_config["description"])
        mlflow.log_param("prompt_hash", get_prompt_hash(prompt_config["template"]))
        mlflow.log_param("model_name", MODEL_NAME)
        mlflow.log_param("temperature", 0.0)
        mlflow.log_param("test_samples", len(test_data))
        
        # Log prompt template as artifact
        mlflow.log_text(prompt_config["template"], "prompt_template.txt")
        
        # Run experiment
        experiment_result = run_prompt_experiment(
            prompt_key, 
            prompt_config, 
            test_data
        )
        
        # Log metrics
        for metric_name, metric_value in experiment_result["metrics"].items():
            mlflow.log_metric(metric_name, metric_value)
        
        # Log detailed results as artifact
        results_df = pd.DataFrame(experiment_result["results"])
        results_df.to_csv("/tmp/results.csv", index=False)
        mlflow.log_artifact("/tmp/results.csv", "evaluation")
        
        # Store for comparison
        all_experiments[prompt_key] = {
            "run_id": run.info.run_id,
            "metrics": experiment_result["metrics"],
            "results": experiment_result["results"]
        }
        
        print(f"\nüìä Results for {prompt_config['name']}:")
        print(f"   Accuracy: {experiment_result['metrics']['accuracy']:.2%}")
        print(f"   Macro F1: {experiment_result['metrics']['macro_f1']:.2%}")
        print(f"   Avg Latency: {experiment_result['metrics']['avg_latency']:.3f}s")
        print(f"   Run ID: {run.info.run_id}")

print("\n" + "=" * 60)
print("‚úÖ ALL EXPERIMENTS COMPLETED!")
print("=" * 60)

# %% [markdown]
# ## üìä Part 7: Compare Results
#
# ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Ç‡∏≠‡∏á prompt templates ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î

# %%
# Create comparison dataframe
comparison_data = []

for prompt_key, exp_data in all_experiments.items():
    row = {
        "Prompt Version": prompt_key,
        "Name": prompt_templates[prompt_key]["name"],
        "Accuracy": exp_data["metrics"]["accuracy"],
        "Macro F1": exp_data["metrics"]["macro_f1"],
        "Avg Latency (s)": exp_data["metrics"]["avg_latency"],
        "Total Tokens": exp_data["metrics"]["total_tokens"],
        "Run ID": exp_data["run_id"][:8]
    }
    comparison_data.append(row)

comparison_df = pd.DataFrame(comparison_data)
comparison_df = comparison_df.sort_values("Accuracy", ascending=False)

print("\nüìà PROMPT COMPARISON RESULTS")
print("=" * 80)
print(comparison_df.to_string(index=False))

# %%
# Find best prompt
best_prompt = comparison_df.iloc[0]
print("\n" + "=" * 60)
print("üèÜ BEST PERFORMING PROMPT")
print("=" * 60)
print(f"Version: {best_prompt['Prompt Version']}")
print(f"Name: {best_prompt['Name']}")
print(f"Accuracy: {best_prompt['Accuracy']:.2%}")
print(f"Macro F1: {best_prompt['Macro F1']:.2%}")
print(f"Avg Latency: {best_prompt['Avg Latency (s)']:.3f}s")

# %%
# Detailed per-class performance
print("\nüìä PER-CLASS PERFORMANCE (Best Prompt)")
print("-" * 50)

best_key = best_prompt['Prompt Version']
best_metrics = all_experiments[best_key]["metrics"]

for cls in ["positive", "negative", "neutral"]:
    print(f"\n{cls.upper()}:")
    print(f"  Precision: {best_metrics[f'{cls}_precision']:.2%}")
    print(f"  Recall: {best_metrics[f'{cls}_recall']:.2%}")
    print(f"  F1-Score: {best_metrics[f'{cls}_f1']:.2%}")

# %% [markdown]
# ## üì¶ Part 8: Register Best Model in MLflow
#
# ‡∏•‡∏á‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô prompt ‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡πÄ‡∏õ‡πá‡∏ô model ‡πÉ‡∏ô MLflow Model Registry

# %%
# Create a custom MLflow model for the best prompt
class PromptModel(mlflow.pyfunc.PythonModel):
    """
    Custom MLflow model that wraps a prompt template
    """
    
    def __init__(self, prompt_template: str, model_name: str):
        self.prompt_template = prompt_template
        self.model_name = model_name
    
    def predict(self, context, model_input):
        """
        Run prediction using the prompt template
        """
        # model_input should be a DataFrame with 'text' column
        results = []
        
        for idx, row in model_input.iterrows():
            prompt = self.prompt_template.format(text=row['text'])
            
            # Note: In production, you'd call the actual LLM here
            # For demo, we'll just return the prompt
            results.append({
                "input": row['text'],
                "prompt": prompt
            })
        
        return results

# %%
# Register the best prompt as a model
best_prompt_key = best_prompt['Prompt Version']
best_template = prompt_templates[best_prompt_key]["template"]

with mlflow.start_run(run_name=f"register_best_prompt_{best_prompt_key}") as run:
    
    # Log the prompt model
    prompt_model = PromptModel(
        prompt_template=best_template,
        model_name=MODEL_NAME
    )
    
    # Log model
    mlflow.pyfunc.log_model(
        artifact_path="prompt_model",
        python_model=prompt_model,
        registered_model_name="sentiment_prompt_model"
    )
    
    # Log additional metadata
    mlflow.log_param("prompt_version", best_prompt_key)
    mlflow.log_param("accuracy", best_prompt['Accuracy'])
    mlflow.log_param("macro_f1", best_prompt['Macro F1'])
    
    # Log prompt template
    mlflow.log_text(best_template, "best_prompt_template.txt")
    
    print(f"‚úÖ Registered model: sentiment_prompt_model")
    print(f"   Run ID: {run.info.run_id}")

# %% [markdown]
# ## üî¨ Part 9: Advanced - A/B Testing Configuration
#
# ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° configuration ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö A/B testing ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á prompts

# %%
# Create A/B test configuration
ab_test_config = {
    "experiment_name": "prompt_ab_test",
    "variants": [
        {
            "name": "control",
            "prompt_version": "v1_basic",
            "traffic_percentage": 50
        },
        {
            "name": "treatment",
            "prompt_version": best_prompt_key,
            "traffic_percentage": 50
        }
    ],
    "success_metrics": ["accuracy", "macro_f1"],
    "guardrail_metrics": ["avg_latency"],
    "min_sample_size": 1000,
    "confidence_level": 0.95
}

# Save configuration
with mlflow.start_run(run_name="ab_test_config"):
    mlflow.log_dict(ab_test_config, "ab_test_config.json")
    print("‚úÖ A/B Test configuration logged to MLflow")

print("\nüìã A/B Test Configuration:")
print(json.dumps(ab_test_config, indent=2))

# %% [markdown]
# ## üìà Part 10: Visualize Results
#
# ‡∏™‡∏£‡πâ‡∏≤‡∏á visualization ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö prompts

# %%
# Simple text-based visualization
print("\n" + "=" * 70)
print("üìä ACCURACY COMPARISON CHART")
print("=" * 70)

max_acc = comparison_df['Accuracy'].max()

for _, row in comparison_df.iterrows():
    bar_length = int(row['Accuracy'] / max_acc * 40)
    bar = "‚ñà" * bar_length
    spaces = " " * (40 - bar_length)
    print(f"{row['Prompt Version']:15} |{bar}{spaces}| {row['Accuracy']:.1%}")

# %%
# Latency vs Accuracy trade-off
print("\n" + "=" * 70)
print("‚ö° LATENCY vs ACCURACY TRADE-OFF")
print("=" * 70)

for _, row in comparison_df.iterrows():
    acc = row['Accuracy']
    lat = row['Avg Latency (s)']
    
    # Simple efficiency score (accuracy / latency)
    efficiency = acc / lat if lat > 0 else 0
    
    print(f"{row['Prompt Version']:15} | Acc: {acc:.1%} | Lat: {lat:.2f}s | Efficiency: {efficiency:.2f}")

# %% [markdown]
# ## üéØ Part 11: Exercise - Create Your Own Prompt
#
# **‡πÅ‡∏ö‡∏ö‡∏ù‡∏∂‡∏Å‡∏´‡∏±‡∏î:** ‡∏™‡∏£‡πâ‡∏≤‡∏á prompt template ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏≠‡∏á‡πÅ‡∏•‡∏∞‡∏ó‡∏î‡∏™‡∏≠‡∏ö

# %%
# TODO: ‡∏™‡∏£‡πâ‡∏≤‡∏á prompt template ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏≠‡∏á
my_custom_prompt = {
    "name": "My Custom Prompt",
    "description": "Your custom prompt description",
    "template": """
[‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô prompt template ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà]

Text: {text}

Answer:
"""
}

# Uncomment to test your prompt:
# prompt_templates["v6_custom"] = my_custom_prompt
# 
# with mlflow.start_run(run_name="prompt_v6_custom"):
#     result = run_prompt_experiment("v6_custom", my_custom_prompt, test_data)
#     for metric_name, metric_value in result["metrics"].items():
#         mlflow.log_metric(metric_name, metric_value)

# %% [markdown]
# ## üìù Summary & Key Takeaways
#
# ### ‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÉ‡∏ô‡πÅ‡∏•‡∏ö‡∏ô‡∏µ‡πâ:
#
# 1. **Prompt Engineering Techniques**
#    - Basic prompts
#    - Detailed instructions
#    - Few-shot learning
#    - Chain-of-thought reasoning
#    - Role-playing/Persona
#
# 2. **MLflow for Prompt Tracking**
#    - Log prompt templates as parameters ‡πÅ‡∏•‡∏∞ artifacts
#    - Track performance metrics (accuracy, F1, latency)
#    - Compare multiple prompt versions
#    - Register best prompts as models
#
# 3. **Systematic Evaluation**
#    - ‡∏™‡∏£‡πâ‡∏≤‡∏á test dataset ‡∏ó‡∏µ‡πà‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°
#    - ‡∏ß‡∏±‡∏î‡∏ú‡∏•‡∏î‡πâ‡∏ß‡∏¢ metrics ‡∏´‡∏•‡∏≤‡∏¢‡∏ï‡∏±‡∏ß
#    - ‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤ trade-offs (accuracy vs latency)
#
# 4. **Best Practices**
#    - Version control ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö prompts
#    - Reproducible experiments
#    - Documentation ‡πÅ‡∏•‡∏∞ metadata
#
# ---
#
# ### üîó Useful Resources:
# - [MLflow Documentation](https://mlflow.org/docs/latest/index.html)
# - [Prompt Engineering Guide](https://www.promptingguide.ai/)
# - [OpenAI Best Practices](https://platform.openai.com/docs/guides/prompt-engineering)

# %%
print("\n" + "=" * 60)
print("üéâ LAB COMPLETED!")
print("=" * 60)
print(f"\nMLflow UI: {MLFLOW_TRACKING_URI}")
print(f"Experiment: {EXPERIMENT_NAME}")
print(f"\nTo view results, run: mlflow ui --port 5000")
