# %%
# Complete MLOps Labs: Understanding Data Drift Concepts with sklearn

## LAB 1: Understanding Data Drift Concepts


# %% [markdown]
# # LAB 1: Understanding Data Drift Concepts
# ## ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡πÅ‡∏ô‡∏ß‡∏Ñ‡∏¥‡∏î‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á Data Drift
#
# ### ‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ:
# 1. ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á Covariate Shift ‡πÅ‡∏•‡∏∞ Concept Drift
# 2. ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ Statistical tests ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö drift detection (KS, PSI, Wasserstein)
# 3. ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å drift detection method ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡∏Å‡∏±‡∏ö‡∏™‡∏ñ‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏ì‡πå‡∏ï‡πà‡∏≤‡∏á‡πÜ
#
# ### ‡∏ó‡∏§‡∏©‡∏é‡∏µ‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô:
# **Data Drift** ‡∏Ñ‡∏∑‡∏≠ ‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÄ‡∏ß‡∏•‡∏≤‡∏ú‡πà‡∏≤‡∏ô‡πÑ‡∏õ ‡∏ã‡∏∂‡πà‡∏á‡∏≠‡∏≤‡∏à‡∏™‡πà‡∏á‡∏ú‡∏•‡∏Å‡∏£‡∏∞‡∏ó‡∏ö‡∏ï‡πà‡∏≠‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡∏Ç‡∏≠‡∏á ML Model
#
# ‡∏°‡∏µ 2 ‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏´‡∏•‡∏±‡∏Å:
# - **Covariate Shift**: ‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡∏≠‡∏á input features P(X) ‡πÇ‡∏î‡∏¢‡∏ó‡∏µ‡πà‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå P(Y|X) ‡∏¢‡∏±‡∏á‡∏Ñ‡∏á‡πÄ‡∏î‡∏¥‡∏°
# - **Concept Drift**: ‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á input ‡πÅ‡∏•‡∏∞ output P(Y|X)

# %% [markdown]
# ## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 1: ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° Environment ‡πÅ‡∏•‡∏∞ Import Libraries
#
# ‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô ‡πÄ‡∏£‡∏≤‡∏à‡∏∞ import libraries ‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î

# %%
# Import libraries ‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report
import warnings
warnings.filterwarnings('ignore')

# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ random seed ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ó‡∏≥‡∏ã‡πâ‡∏≥‡πÑ‡∏î‡πâ
np.random.seed(42)

# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ matplotlib ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢
plt.rcParams['figure.figsize'] = (12, 6)
plt.rcParams['font.size'] = 10

print("‚úÖ Libraries imported successfully!")
print(f"NumPy version: {np.__version__}")
print(f"Pandas version: {pd.__version__}")

# %% [markdown]
# ## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 2: ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à Covariate Shift
#
# ### ‡∏ó‡∏§‡∏©‡∏é‡∏µ Covariate Shift:
# - ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡∏∂‡πâ‡∏ô‡πÄ‡∏°‡∏∑‡πà‡∏≠ distribution ‡∏Ç‡∏≠‡∏á input features ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á
# - ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏£‡∏≤‡∏Ñ‡∏≤‡∏ö‡πâ‡∏≤‡∏ô‡∏ó‡∏µ‡πà train ‡∏Å‡∏±‡∏ö‡∏ö‡πâ‡∏≤‡∏ô‡πÉ‡∏ô‡πÄ‡∏°‡∏∑‡∏≠‡∏á ‡πÅ‡∏ï‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ö‡πâ‡∏≤‡∏ô‡πÉ‡∏ô‡∏ä‡∏ô‡∏ö‡∏ó
# - ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á features ‡∏Å‡∏±‡∏ö target ‡∏¢‡∏±‡∏á‡∏Ñ‡∏á‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°
#
# ‡∏™‡∏π‡∏ï‡∏£‡∏ó‡∏≤‡∏á‡∏Ñ‡∏ì‡∏¥‡∏ï‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå:
# - Training: P_train(X) ‚â† P_test(X)
# - ‡πÅ‡∏ï‡πà: P(Y|X) ‡∏Ñ‡∏á‡∏ó‡∏µ‡πà

# %%
def generate_covariate_shift_data():
    """
    ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÅ‡∏™‡∏î‡∏á Covariate Shift
    
    ‡πÉ‡∏ô‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡∏µ‡πâ:
    - Training data: ‡∏≠‡∏≤‡∏¢‡∏∏‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤ 20-40 ‡∏õ‡∏µ
    - Production data: ‡∏≠‡∏≤‡∏¢‡∏∏‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤ 40-60 ‡∏õ‡∏µ
    - ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏≠‡∏≤‡∏¢‡∏∏‡∏Å‡∏±‡∏ö‡∏û‡∏§‡∏ï‡∏¥‡∏Å‡∏£‡∏£‡∏°‡∏Å‡∏≤‡∏£‡∏ã‡∏∑‡πâ‡∏≠‡∏¢‡∏±‡∏á‡∏Ñ‡∏á‡πÄ‡∏î‡∏¥‡∏°
    """
    
    # Training data: ‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤‡∏≠‡∏≤‡∏¢‡∏∏‡∏ô‡πâ‡∏≠‡∏¢ (20-40)
    np.random.seed(42)
    n_train = 1000
    age_train = np.random.normal(30, 5, n_train)
    income_train = age_train * 1500 + np.random.normal(0, 5000, n_train)
    
    # ‡∏Å‡∏é‡∏Å‡∏≤‡∏£‡∏ã‡∏∑‡πâ‡∏≠: ‡∏ã‡∏∑‡πâ‡∏≠‡∏ñ‡πâ‡∏≤ income > 40000 + age * 500
    threshold_train = 40000 + age_train * 500
    purchase_train = (income_train > threshold_train).astype(int)
    
    train_df = pd.DataFrame({
        'age': age_train,
        'income': income_train,
        'purchase': purchase_train
    })
    
    # Production data: ‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤‡∏≠‡∏≤‡∏¢‡∏∏‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô (40-60) - Covariate Shift!
    n_prod = 1000
    age_prod = np.random.normal(50, 5, n_prod)  # ‡∏≠‡∏≤‡∏¢‡∏∏‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô!
    income_prod = age_prod * 1500 + np.random.normal(0, 5000, n_prod)
    
    # ‡∏Å‡∏é‡∏Å‡∏≤‡∏£‡∏ã‡∏∑‡πâ‡∏≠‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏° (‡πÑ‡∏°‡πà‡∏°‡∏µ Concept Drift)
    threshold_prod = 40000 + age_prod * 500
    purchase_prod = (income_prod > threshold_prod).astype(int)
    
    prod_df = pd.DataFrame({
        'age': age_prod,
        'income': income_prod,
        'purchase': purchase_prod
    })
    
    return train_df, prod_df

# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
train_covariate, prod_covariate = generate_covariate_shift_data()

print("üìä Training Data Summary:")
print(train_covariate.describe())
print("\nüìä Production Data Summary:")
print(prod_covariate.describe())

# %%
# Visualize Covariate Shift
fig, axes = plt.subplots(1, 3, figsize=(15, 4))

# Plot 1: Age Distribution
axes[0].hist(train_covariate['age'], bins=30, alpha=0.7, label='Training', color='blue')
axes[0].hist(prod_covariate['age'], bins=30, alpha=0.7, label='Production', color='red')
axes[0].set_xlabel('Age')
axes[0].set_ylabel('Frequency')
axes[0].set_title('Covariate Shift: Age Distribution\n(Training vs Production)')
axes[0].legend()
axes[0].axvline(train_covariate['age'].mean(), color='blue', linestyle='--', label='Train Mean')
axes[0].axvline(prod_covariate['age'].mean(), color='red', linestyle='--', label='Prod Mean')

# Plot 2: Income Distribution
axes[1].hist(train_covariate['income'], bins=30, alpha=0.7, label='Training', color='blue')
axes[1].hist(prod_covariate['income'], bins=30, alpha=0.7, label='Production', color='red')
axes[1].set_xlabel('Income')
axes[1].set_ylabel('Frequency')
axes[1].set_title('Covariate Shift: Income Distribution\n(Training vs Production)')
axes[1].legend()

# Plot 3: Relationship P(Y|X) ‡∏¢‡∏±‡∏á‡∏Ñ‡∏á‡πÄ‡∏î‡∏¥‡∏°
axes[2].scatter(train_covariate['age'], train_covariate['income'], 
                c=train_covariate['purchase'], alpha=0.3, cmap='coolwarm', label='Training')
axes[2].set_xlabel('Age')
axes[2].set_ylabel('Income')
axes[2].set_title('P(Y|X) Relationship\n(Decision boundary ‡∏¢‡∏±‡∏á‡∏Ñ‡∏á‡πÄ‡∏î‡∏¥‡∏°)')

plt.tight_layout()
plt.savefig('covariate_shift_visualization.png', dpi=150, bbox_inches='tight')
plt.show()

print("\nüí° ‡∏™‡∏±‡∏á‡πÄ‡∏Å‡∏ï: Distribution ‡∏Ç‡∏≠‡∏á Age ‡πÅ‡∏•‡∏∞ Income ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÑ‡∏õ ‡πÅ‡∏ï‡πà‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à‡∏ã‡∏∑‡πâ‡∏≠‡∏¢‡∏±‡∏á‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°")

# %% [markdown]
# ## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 3: ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à Concept Drift
#
# ### ‡∏ó‡∏§‡∏©‡∏é‡∏µ Concept Drift:
# - ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡∏∂‡πâ‡∏ô‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á input ‡πÅ‡∏•‡∏∞ output ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á
# - ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: ‡∏û‡∏§‡∏ï‡∏¥‡∏Å‡∏£‡∏£‡∏°‡∏Å‡∏≤‡∏£‡∏ã‡∏∑‡πâ‡∏≠‡∏Ç‡∏≠‡∏á‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏´‡∏•‡∏±‡∏á COVID-19
# - ‡πÅ‡∏°‡πâ input distribution ‡∏à‡∏∞‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏° ‡πÅ‡∏ï‡πà output ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô
#
# ‡∏™‡∏π‡∏ï‡∏£‡∏ó‡∏≤‡∏á‡∏Ñ‡∏ì‡∏¥‡∏ï‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå:
# - P(X) ‡∏≠‡∏≤‡∏à‡∏Ñ‡∏á‡∏ó‡∏µ‡πà‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏Å‡πá‡πÑ‡∏î‡πâ
# - ‡πÅ‡∏ï‡πà: P(Y|X) ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á

# %%
def generate_concept_drift_data():
    """
    ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÅ‡∏™‡∏î‡∏á Concept Drift
    
    ‡πÉ‡∏ô‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡∏µ‡πâ:
    - Training data: ‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤‡∏ã‡∏∑‡πâ‡∏≠‡πÄ‡∏°‡∏∑‡πà‡∏≠ income ‡∏™‡∏π‡∏á‡∏Å‡∏ß‡πà‡∏≤ threshold
    - Production data: threshold ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô (‡∏û‡∏§‡∏ï‡∏¥‡∏Å‡∏£‡∏£‡∏°‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô)
    """
    
    np.random.seed(42)
    n_samples = 1000
    
    # Training data
    age_train = np.random.normal(35, 10, n_samples)
    income_train = np.random.normal(50000, 15000, n_samples)
    
    # ‡∏Å‡∏é‡∏Å‡∏≤‡∏£‡∏ã‡∏∑‡πâ‡∏≠‡πÄ‡∏î‡∏¥‡∏°: ‡∏ã‡∏∑‡πâ‡∏≠‡∏ñ‡πâ‡∏≤ income > 45000
    purchase_train = (income_train > 45000).astype(int)
    
    train_df = pd.DataFrame({
        'age': age_train,
        'income': income_train,
        'purchase': purchase_train
    })
    
    # Production data: distribution ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°
    age_prod = np.random.normal(35, 10, n_samples)
    income_prod = np.random.normal(50000, 15000, n_samples)
    
    # ‡∏Å‡∏é‡∏Å‡∏≤‡∏£‡∏ã‡∏∑‡πâ‡∏≠‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô: ‡∏ã‡∏∑‡πâ‡∏≠‡∏ñ‡πâ‡∏≤ income > 55000 (Concept Drift!)
    # ‡∏≠‡∏≤‡∏à‡πÄ‡∏Å‡∏¥‡∏î‡∏à‡∏≤‡∏Å‡πÄ‡∏®‡∏£‡∏©‡∏ê‡∏Å‡∏¥‡∏à‡∏ï‡∏Å‡∏ï‡πà‡∏≥ ‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏£‡∏≤‡∏¢‡πÑ‡∏î‡πâ‡∏™‡∏π‡∏á‡∏Ç‡∏∂‡πâ‡∏ô‡∏ñ‡∏∂‡∏á‡∏à‡∏∞‡∏ã‡∏∑‡πâ‡∏≠
    purchase_prod = (income_prod > 55000).astype(int)
    
    prod_df = pd.DataFrame({
        'age': age_prod,
        'income': income_prod,
        'purchase': purchase_prod
    })
    
    return train_df, prod_df

# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
train_concept, prod_concept = generate_concept_drift_data()

print("üìä Training Data Summary:")
print(f"Purchase Rate: {train_concept['purchase'].mean():.2%}")
print(train_concept.describe())

print("\nüìä Production Data Summary:")
print(f"Purchase Rate: {prod_concept['purchase'].mean():.2%}")
print(prod_concept.describe())

# %%
# Visualize Concept Drift
fig, axes = plt.subplots(1, 3, figsize=(15, 4))

# Plot 1: Income Distribution (‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Å‡∏±‡∏ô)
axes[0].hist(train_concept['income'], bins=30, alpha=0.7, label='Training', color='blue')
axes[0].hist(prod_concept['income'], bins=30, alpha=0.7, label='Production', color='red')
axes[0].set_xlabel('Income')
axes[0].set_ylabel('Frequency')
axes[0].set_title('No Covariate Shift: Income Distribution\n(Training ‚âà Production)')
axes[0].legend()

# Plot 2: Purchase Rate by Income Bin
income_bins = np.linspace(20000, 80000, 10)

train_purchase_rate = []
prod_purchase_rate = []
bin_centers = []

for i in range(len(income_bins)-1):
    mask_train = (train_concept['income'] >= income_bins[i]) & (train_concept['income'] < income_bins[i+1])
    mask_prod = (prod_concept['income'] >= income_bins[i]) & (prod_concept['income'] < income_bins[i+1])
    
    if mask_train.sum() > 0:
        train_purchase_rate.append(train_concept[mask_train]['purchase'].mean())
    else:
        train_purchase_rate.append(0)
        
    if mask_prod.sum() > 0:
        prod_purchase_rate.append(prod_concept[mask_prod]['purchase'].mean())
    else:
        prod_purchase_rate.append(0)
    
    bin_centers.append((income_bins[i] + income_bins[i+1]) / 2)

axes[1].plot(bin_centers, train_purchase_rate, 'b-o', label='Training P(Y|X)', linewidth=2)
axes[1].plot(bin_centers, prod_purchase_rate, 'r-o', label='Production P(Y|X)', linewidth=2)
axes[1].set_xlabel('Income')
axes[1].set_ylabel('Purchase Probability')
axes[1].set_title('Concept Drift: P(Y|X) Changed!\n(Decision boundary moved)')
axes[1].legend()
axes[1].axhline(0.5, color='gray', linestyle='--', alpha=0.5)

# Plot 3: Decision Boundary Comparison
axes[2].scatter(train_concept['income'], train_concept['purchase'] + np.random.normal(0, 0.05, len(train_concept)), 
                alpha=0.3, color='blue', label='Training')
axes[2].scatter(prod_concept['income'], prod_concept['purchase'] + np.random.normal(0, 0.05, len(prod_concept)), 
                alpha=0.3, color='red', label='Production')
axes[2].axvline(45000, color='blue', linestyle='--', linewidth=2, label='Train Threshold (45k)')
axes[2].axvline(55000, color='red', linestyle='--', linewidth=2, label='Prod Threshold (55k)')
axes[2].set_xlabel('Income')
axes[2].set_ylabel('Purchase (with jitter)')
axes[2].set_title('Concept Drift Visualization\n(Threshold shifted from 45k to 55k)')
axes[2].legend()

plt.tight_layout()
plt.savefig('concept_drift_visualization.png', dpi=150, bbox_inches='tight')
plt.show()

print("\nüí° ‡∏™‡∏±‡∏á‡πÄ‡∏Å‡∏ï: Distribution ‡∏Ç‡∏≠‡∏á Income ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏° ‡πÅ‡∏ï‡πà‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡∏Å‡∏±‡∏ö Purchase ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÑ‡∏õ!")
print("   - Training: ‡∏ã‡∏∑‡πâ‡∏≠‡πÄ‡∏°‡∏∑‡πà‡∏≠ income > 45,000")
print("   - Production: ‡∏ã‡∏∑‡πâ‡∏≠‡πÄ‡∏°‡∏∑‡πà‡∏≠ income > 55,000")

# %% [markdown]
# ## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 4: Statistical Tests ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Drift Detection
#
# ### 4.1 Kolmogorov-Smirnov (KS) Test
#
# **‡∏ó‡∏§‡∏©‡∏é‡∏µ:**
# - ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö cumulative distribution function (CDF) ‡∏Ç‡∏≠‡∏á 2 samples
# - ‡∏ß‡∏±‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á 2 CDFs
# - ‡∏Ç‡πâ‡∏≠‡∏î‡∏µ: ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏™‡∏°‡∏°‡∏ï‡∏¥ distribution, sensitive ‡∏ï‡πà‡∏≠‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á
# - ‡∏Ç‡πâ‡∏≠‡πÄ‡∏™‡∏µ‡∏¢: ‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ‡∏Å‡∏±‡∏ö continuous variables ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
#
# **‡∏Å‡∏≤‡∏£‡∏ï‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°:**
# - KS Statistic: 0-1 (‡∏¢‡∏¥‡πà‡∏á‡∏™‡∏π‡∏á = ‡∏¢‡∏¥‡πà‡∏á‡∏ï‡πà‡∏≤‡∏á)
# - p-value < 0.05: reject null hypothesis ‚Üí ‡∏°‡∏µ drift

# %%
def kolmogorov_smirnov_test(data1, data2, feature_name="feature"):
    """
    ‡∏ó‡∏≥ KS Test ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö drift
    
    Parameters:
    -----------
    data1 : array-like - ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ä‡∏∏‡∏î‡πÅ‡∏£‡∏Å (reference/training)
    data2 : array-like - ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ä‡∏∏‡∏î‡∏ó‡∏µ‡πà‡∏™‡∏≠‡∏á (current/production)
    feature_name : str - ‡∏ä‡∏∑‡πà‡∏≠ feature
    
    Returns:
    --------
    dict : ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Ç‡∏≠‡∏á KS test
    """
    statistic, p_value = stats.ks_2samp(data1, data2)
    
    # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î threshold ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö drift
    drift_detected = p_value < 0.05
    
    result = {
        'feature': feature_name,
        'test': 'Kolmogorov-Smirnov',
        'statistic': statistic,
        'p_value': p_value,
        'drift_detected': drift_detected,
        'interpretation': 'DRIFT DETECTED!' if drift_detected else 'No significant drift'
    }
    
    return result

# ‡∏ó‡∏î‡∏™‡∏≠‡∏ö KS Test ‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Covariate Shift
print("=" * 60)
print("üîç KS Test Results for Covariate Shift Data")
print("=" * 60)

ks_age = kolmogorov_smirnov_test(train_covariate['age'], prod_covariate['age'], 'age')
ks_income = kolmogorov_smirnov_test(train_covariate['income'], prod_covariate['income'], 'income')

for result in [ks_age, ks_income]:
    print(f"\nFeature: {result['feature']}")
    print(f"  KS Statistic: {result['statistic']:.4f}")
    print(f"  P-value: {result['p_value']:.6f}")
    print(f"  Result: {result['interpretation']}")

# %%
# Visualize KS Test
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

for idx, (feature, ax) in enumerate(zip(['age', 'income'], axes)):
    # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì CDF
    sorted_train = np.sort(train_covariate[feature])
    sorted_prod = np.sort(prod_covariate[feature])
    
    cdf_train = np.arange(1, len(sorted_train) + 1) / len(sorted_train)
    cdf_prod = np.arange(1, len(sorted_prod) + 1) / len(sorted_prod)
    
    ax.plot(sorted_train, cdf_train, 'b-', linewidth=2, label='Training CDF')
    ax.plot(sorted_prod, cdf_prod, 'r-', linewidth=2, label='Production CDF')
    
    # ‡πÅ‡∏™‡∏î‡∏á KS statistic (maximum distance)
    ks_result = kolmogorov_smirnov_test(train_covariate[feature], prod_covariate[feature], feature)
    
    ax.set_xlabel(feature.capitalize())
    ax.set_ylabel('Cumulative Probability')
    ax.set_title(f'KS Test: {feature.capitalize()}\nKS Statistic = {ks_result["statistic"]:.4f}, p-value = {ks_result["p_value"]:.4f}')
    ax.legend()
    ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('ks_test_visualization.png', dpi=150, bbox_inches='tight')
plt.show()

# %% [markdown]
# ### 4.2 Population Stability Index (PSI)
#
# **‡∏ó‡∏§‡∏©‡∏é‡∏µ:**
# - ‡∏ß‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡∏≠‡∏á distribution ‡πÇ‡∏î‡∏¢‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö proportions ‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞ bin
# - ‡∏ô‡∏¥‡∏¢‡∏°‡πÉ‡∏ä‡πâ‡πÉ‡∏ô credit scoring ‡πÅ‡∏•‡∏∞ financial models
# - ‡∏™‡∏π‡∏ï‡∏£: PSI = Œ£ (Actual% - Expected%) √ó ln(Actual% / Expected%)
#
# **‡∏Å‡∏≤‡∏£‡∏ï‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏° PSI:**
# - PSI < 0.1: ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏ó‡∏µ‡πà‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç
# - 0.1 ‚â§ PSI < 0.25: ‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á ‡∏Ñ‡∏ß‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö
# - PSI ‚â• 0.25: ‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏°‡∏≤‡∏Å ‡∏ï‡πâ‡∏≠‡∏á‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£

# %%
def calculate_psi(expected, actual, bins=10, eps=1e-6):
    """
    ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Population Stability Index (PSI)
    
    Parameters:
    -----------
    expected : array-like - ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• reference (training)
    actual : array-like - ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• current (production)
    bins : int - ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô bins ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö discretize
    eps : float - ‡∏Ñ‡πà‡∏≤‡πÄ‡∏•‡πá‡∏Å‡πÜ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô division by zero
    
    Returns:
    --------
    float : ‡∏Ñ‡πà‡∏≤ PSI
    dict : ‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì
    """
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á bins ‡∏à‡∏≤‡∏Å expected data
    breakpoints = np.percentile(expected, np.linspace(0, 100, bins + 1))
    breakpoints = np.unique(breakpoints)  # ‡∏•‡∏ö duplicates
    
    # ‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞ bin
    expected_counts, _ = np.histogram(expected, bins=breakpoints)
    actual_counts, _ = np.histogram(actual, bins=breakpoints)
    
    # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì proportions
    expected_props = expected_counts / len(expected) + eps
    actual_props = actual_counts / len(actual) + eps
    
    # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì PSI
    psi_values = (actual_props - expected_props) * np.log(actual_props / expected_props)
    psi = np.sum(psi_values)
    
    # ‡∏ï‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
    if psi < 0.1:
        interpretation = "No significant change (PSI < 0.1)"
        severity = "LOW"
    elif psi < 0.25:
        interpretation = "Moderate change - monitor closely (0.1 ‚â§ PSI < 0.25)"
        severity = "MEDIUM"
    else:
        interpretation = "Significant change - action required (PSI ‚â• 0.25)"
        severity = "HIGH"
    
    return {
        'psi': psi,
        'interpretation': interpretation,
        'severity': severity,
        'bin_psi_values': psi_values,
        'expected_props': expected_props,
        'actual_props': actual_props,
        'breakpoints': breakpoints
    }

# ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì PSI ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞ feature
print("=" * 60)
print("üìä PSI Results for Covariate Shift Data")
print("=" * 60)

for feature in ['age', 'income']:
    psi_result = calculate_psi(train_covariate[feature], prod_covariate[feature])
    print(f"\nFeature: {feature}")
    print(f"  PSI Value: {psi_result['psi']:.4f}")
    print(f"  Severity: {psi_result['severity']}")
    print(f"  Interpretation: {psi_result['interpretation']}")

# %%
# Visualize PSI
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

for idx, feature in enumerate(['age', 'income']):
    psi_result = calculate_psi(train_covariate[feature], prod_covariate[feature])
    
    x = np.arange(len(psi_result['expected_props']))
    width = 0.35
    
    axes[idx].bar(x - width/2, psi_result['expected_props'], width, label='Expected (Train)', color='blue', alpha=0.7)
    axes[idx].bar(x + width/2, psi_result['actual_props'], width, label='Actual (Prod)', color='red', alpha=0.7)
    
    axes[idx].set_xlabel('Bin')
    axes[idx].set_ylabel('Proportion')
    axes[idx].set_title(f'PSI Analysis: {feature.capitalize()}\nPSI = {psi_result["psi"]:.4f} ({psi_result["severity"]})')
    axes[idx].legend()
    axes[idx].set_xticks(x)

plt.tight_layout()
plt.savefig('psi_visualization.png', dpi=150, bbox_inches='tight')
plt.show()

# %% [markdown]
# ### 4.3 Wasserstein Distance (Earth Mover's Distance)
#
# **‡∏ó‡∏§‡∏©‡∏é‡∏µ:**
# - ‡∏ß‡∏±‡∏î "‡∏á‡∏≤‡∏ô" ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô distribution ‡∏´‡∏ô‡∏∂‡πà‡∏á‡πÑ‡∏õ‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏µ‡∏Å distribution
# - ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏ï‡πâ‡∏ô‡∏ó‡∏∏‡∏ô‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏Ç‡∏ô‡∏¢‡πâ‡∏≤‡∏¢‡∏î‡∏¥‡∏ô (Earth Mover)
# - ‡∏Ç‡πâ‡∏≠‡∏î‡∏µ: ‡∏Ñ‡∏≥‡∏ô‡∏∂‡∏á‡∏ñ‡∏∂‡∏á distance ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á bins, sensitive ‡∏ï‡πà‡∏≠ shift ‡πÉ‡∏ô‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á
# - ‡∏Ç‡πâ‡∏≠‡πÄ‡∏™‡∏µ‡∏¢: ‡∏ï‡πâ‡∏≠‡∏á normalize ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡πÑ‡∏î‡πâ

# %%
def wasserstein_distance_test(data1, data2, feature_name="feature"):
    """
    ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Wasserstein Distance ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö drift detection
    
    Parameters:
    -----------
    data1 : array-like - ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ä‡∏∏‡∏î‡πÅ‡∏£‡∏Å (reference)
    data2 : array-like - ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ä‡∏∏‡∏î‡∏ó‡∏µ‡πà‡∏™‡∏≠‡∏á (current)
    feature_name : str - ‡∏ä‡∏∑‡πà‡∏≠ feature
    
    Returns:
    --------
    dict : ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Ç‡∏≠‡∏á Wasserstein distance
    """
    # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Wasserstein distance
    distance = stats.wasserstein_distance(data1, data2)
    
    # Normalize ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ standard deviation ‡∏Ç‡∏≠‡∏á reference data
    std_ref = np.std(data1)
    normalized_distance = distance / std_ref if std_ref > 0 else distance
    
    # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î threshold (‡∏õ‡∏£‡∏±‡∏ö‡πÑ‡∏î‡πâ‡∏ï‡∏≤‡∏° domain)
    if normalized_distance < 0.1:
        severity = "LOW"
        interpretation = "No significant drift"
    elif normalized_distance < 0.5:
        severity = "MEDIUM"
        interpretation = "Moderate drift detected"
    else:
        severity = "HIGH"
        interpretation = "Significant drift detected"
    
    return {
        'feature': feature_name,
        'distance': distance,
        'normalized_distance': normalized_distance,
        'severity': severity,
        'interpretation': interpretation
    }

# ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Wasserstein Distance
print("=" * 60)
print("üìè Wasserstein Distance Results for Covariate Shift Data")
print("=" * 60)

for feature in ['age', 'income']:
    wd_result = wasserstein_distance_test(train_covariate[feature], prod_covariate[feature], feature)
    print(f"\nFeature: {feature}")
    print(f"  Wasserstein Distance: {wd_result['distance']:.4f}")
    print(f"  Normalized Distance: {wd_result['normalized_distance']:.4f}")
    print(f"  Severity: {wd_result['severity']}")
    print(f"  Interpretation: {wd_result['interpretation']}")

# %%
# Visualize Wasserstein Distance
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

for idx, feature in enumerate(['age', 'income']):
    ax = axes[idx]
    
    # ‡πÅ‡∏™‡∏î‡∏á histogram ‡πÅ‡∏•‡∏∞ KDE
    ax.hist(train_covariate[feature], bins=30, alpha=0.5, density=True, label='Training', color='blue')
    ax.hist(prod_covariate[feature], bins=30, alpha=0.5, density=True, label='Production', color='red')
    
    # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Wasserstein distance
    wd_result = wasserstein_distance_test(train_covariate[feature], prod_covariate[feature], feature)
    
    # ‡πÅ‡∏™‡∏î‡∏á mean ‡πÅ‡∏•‡∏∞ "‡∏Å‡∏≤‡∏£‡∏Ç‡∏ô‡∏¢‡πâ‡∏≤‡∏¢"
    mean_train = train_covariate[feature].mean()
    mean_prod = prod_covariate[feature].mean()
    
    ax.axvline(mean_train, color='blue', linestyle='--', linewidth=2, label=f'Train Mean: {mean_train:.1f}')
    ax.axvline(mean_prod, color='red', linestyle='--', linewidth=2, label=f'Prod Mean: {mean_prod:.1f}')
    
    # ‡πÅ‡∏™‡∏î‡∏á arrow ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö "earth moving"
    ax.annotate('', xy=(mean_prod, 0.01), xytext=(mean_train, 0.01),
                arrowprops=dict(arrowstyle='->', color='green', lw=3))
    
    ax.set_xlabel(feature.capitalize())
    ax.set_ylabel('Density')
    ax.set_title(f'Wasserstein Distance: {feature.capitalize()}\nDistance = {wd_result["distance"]:.2f} ({wd_result["severity"]})')
    ax.legend(loc='upper right')

plt.tight_layout()
plt.savefig('wasserstein_visualization.png', dpi=150, bbox_inches='tight')
plt.show()

print("\nüí° ‡∏•‡∏π‡∏Å‡∏®‡∏£‡∏™‡∏µ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ß‡πÅ‡∏™‡∏î‡∏á‡∏ó‡∏¥‡∏®‡∏ó‡∏≤‡∏á‡∏Ç‡∏≠‡∏á '‡∏Å‡∏≤‡∏£‡∏Ç‡∏ô‡∏¢‡πâ‡∏≤‡∏¢' ‡∏à‡∏≤‡∏Å Training ‡πÑ‡∏õ Production")

# %% [markdown]
# ## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 5: ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡πÅ‡∏•‡∏∞‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Drift Detection Method
#
# ### ‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πâ‡∏≠‡∏î‡∏µ-‡∏Ç‡πâ‡∏≠‡πÄ‡∏™‡∏µ‡∏¢‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞ method:
#
# | Method | ‡∏Ç‡πâ‡∏≠‡∏î‡∏µ | ‡∏Ç‡πâ‡∏≠‡πÄ‡∏™‡∏µ‡∏¢ | ‡πÉ‡∏ä‡πâ‡πÄ‡∏°‡∏∑‡πà‡∏≠ |
# |--------|-------|---------|---------|
# | KS Test | ‡πÑ‡∏°‡πà‡∏™‡∏°‡∏°‡∏ï‡∏¥ distribution, sensitive | ‡πÉ‡∏ä‡πâ‡∏Å‡∏±‡∏ö continuous ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô | ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ statistical significance |
# | PSI | ‡∏°‡∏µ threshold ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô, industry standard | ‡∏ï‡πâ‡∏≠‡∏á binning | Credit scoring, Risk models |
# | Wasserstein | ‡∏Ñ‡∏≥‡∏ô‡∏∂‡∏á‡∏ñ‡∏∂‡∏á distance, ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏á‡πà‡∏≤‡∏¢ | ‡∏ï‡πâ‡∏≠‡∏á normalize | ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö shift ‡πÉ‡∏ô‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á |

# %%
def comprehensive_drift_analysis(reference_data, current_data, feature_name):
    """
    ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå drift ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡∏ó‡∏∏‡∏Å methods ‡πÅ‡∏•‡∏∞‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
    """
    results = {
        'feature': feature_name,
        'ks': kolmogorov_smirnov_test(reference_data, current_data, feature_name),
        'psi': calculate_psi(reference_data, current_data),
        'wasserstein': wasserstein_distance_test(reference_data, current_data, feature_name)
    }
    
    # ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•
    drift_votes = 0
    if results['ks']['drift_detected']:
        drift_votes += 1
    if results['psi']['severity'] in ['MEDIUM', 'HIGH']:
        drift_votes += 1
    if results['wasserstein']['severity'] in ['MEDIUM', 'HIGH']:
        drift_votes += 1
    
    results['consensus'] = 'DRIFT' if drift_votes >= 2 else 'NO DRIFT'
    results['drift_votes'] = drift_votes
    
    return results

# ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
print("=" * 80)
print("üìä COMPREHENSIVE DRIFT ANALYSIS COMPARISON")
print("=" * 80)

comparison_results = []

for feature in ['age', 'income']:
    result = comprehensive_drift_analysis(
        train_covariate[feature], 
        prod_covariate[feature], 
        feature
    )
    comparison_results.append(result)
    
    print(f"\nüîç Feature: {feature.upper()}")
    print("-" * 40)
    print(f"  KS Test: stat={result['ks']['statistic']:.4f}, p={result['ks']['p_value']:.4f} ‚Üí {result['ks']['interpretation']}")
    print(f"  PSI: {result['psi']['psi']:.4f} ‚Üí {result['psi']['severity']}")
    print(f"  Wasserstein: {result['wasserstein']['normalized_distance']:.4f} ‚Üí {result['wasserstein']['severity']}")
    print(f"  üìã CONSENSUS ({result['drift_votes']}/3 votes): {result['consensus']}")

# %%
# ‡∏™‡∏£‡πâ‡∏≤‡∏á comparison table
comparison_df = pd.DataFrame([
    {
        'Feature': r['feature'],
        'KS Statistic': f"{r['ks']['statistic']:.4f}",
        'KS p-value': f"{r['ks']['p_value']:.4f}",
        'KS Drift': '‚úì' if r['ks']['drift_detected'] else '‚úó',
        'PSI': f"{r['psi']['psi']:.4f}",
        'PSI Severity': r['psi']['severity'],
        'Wasserstein': f"{r['wasserstein']['normalized_distance']:.4f}",
        'WD Severity': r['wasserstein']['severity'],
        'Consensus': r['consensus']
    }
    for r in comparison_results
])

print("\n" + "=" * 80)
print("üìä SUMMARY TABLE")
print("=" * 80)
print(comparison_df.to_string(index=False))

# %% [markdown]
# ## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 6: ‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Drift Detection Method
#
# ### Decision Tree ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Method:
#
# ```
# 1. Data Type?
#    ‚îú‚îÄ‚îÄ Continuous ‚Üí ‡πÑ‡∏õ‡∏Ç‡πâ‡∏≠ 2
#    ‚îî‚îÄ‚îÄ Categorical ‚Üí ‡πÉ‡∏ä‡πâ Chi-squared test ‡∏´‡∏£‡∏∑‡∏≠ PSI
#
# 2. ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ Statistical Significance?
#    ‚îú‚îÄ‚îÄ ‡πÉ‡∏ä‡πà ‚Üí KS Test ‡∏´‡∏£‡∏∑‡∏≠ Chi-squared
#    ‚îî‚îÄ‚îÄ ‡πÑ‡∏°‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô ‚Üí PSI ‡∏´‡∏£‡∏∑‡∏≠ Wasserstein
#
# 3. Industry Requirement?
#    ‚îú‚îÄ‚îÄ Finance/Credit ‚Üí PSI (‡∏°‡∏µ regulatory standards)
#    ‚îî‚îÄ‚îÄ ‡∏≠‡∏∑‡πà‡∏ô‡πÜ ‚Üí ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ï‡∏≤‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°
#
# 4. ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏Ñ‡∏ß‡∏≤‡∏° sensitive ‡∏™‡∏π‡∏á?
#    ‚îú‚îÄ‚îÄ ‡πÉ‡∏ä‡πà ‚Üí Wasserstein (detect small shifts)
#    ‚îî‚îÄ‚îÄ ‡∏õ‡∏Å‡∏ï‡∏¥ ‚Üí KS ‡∏´‡∏£‡∏∑‡∏≠ PSI
# ```

# %%
def recommend_drift_method(data_type, needs_significance, industry, high_sensitivity):
    """
    ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥ drift detection method ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°
    
    Parameters:
    -----------
    data_type : str - 'continuous' ‡∏´‡∏£‡∏∑‡∏≠ 'categorical'
    needs_significance : bool - ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ statistical significance ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
    industry : str - 'finance', 'healthcare', 'general'
    high_sensitivity : bool - ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ sensitivity ‡∏™‡∏π‡∏á‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
    
    Returns:
    --------
    str : recommended method ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•
    """
    recommendations = []
    
    if data_type == 'categorical':
        recommendations.append({
            'method': 'Chi-squared Test',
            'reason': '‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö categorical data',
            'priority': 1
        })
        recommendations.append({
            'method': 'PSI (binned)',
            'reason': '‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏ä‡πâ‡∏Å‡∏±‡∏ö categorical ‡πÑ‡∏î‡πâ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ category ‡πÄ‡∏õ‡πá‡∏ô bins',
            'priority': 2
        })
    else:  # continuous
        if industry == 'finance':
            recommendations.append({
                'method': 'PSI',
                'reason': 'Industry standard ‡πÉ‡∏ô finance, ‡∏°‡∏µ regulatory requirements',
                'priority': 1
            })
        
        if needs_significance:
            recommendations.append({
                'method': 'KS Test',
                'reason': '‡πÉ‡∏´‡πâ p-value ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö statistical significance',
                'priority': 1 if industry != 'finance' else 2
            })
        
        if high_sensitivity:
            recommendations.append({
                'method': 'Wasserstein Distance',
                'reason': 'Sensitive ‡∏ï‡πà‡∏≠ small shifts ‡πÅ‡∏•‡∏∞ location changes',
                'priority': 2
            })
        
        # Default recommendation
        if not recommendations:
            recommendations.append({
                'method': 'PSI + KS Test',
                'reason': '‡πÉ‡∏ä‡πâ‡∏ó‡∏±‡πâ‡∏á‡∏™‡∏≠‡∏á‡πÄ‡∏û‡∏∑‡πà‡∏≠ cross-validate',
                'priority': 1
            })
    
    # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏ï‡∏≤‡∏° priority
    recommendations.sort(key=lambda x: x['priority'])
    
    return recommendations

# ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô
print("=" * 60)
print("üéØ DRIFT METHOD RECOMMENDATION EXAMPLES")
print("=" * 60)

scenarios = [
    {'data_type': 'continuous', 'needs_significance': True, 'industry': 'finance', 'high_sensitivity': False},
    {'data_type': 'continuous', 'needs_significance': False, 'industry': 'general', 'high_sensitivity': True},
    {'data_type': 'categorical', 'needs_significance': True, 'industry': 'healthcare', 'high_sensitivity': False},
]

for i, scenario in enumerate(scenarios, 1):
    print(f"\nüìã Scenario {i}:")
    print(f"   Data Type: {scenario['data_type']}")
    print(f"   Needs Significance: {scenario['needs_significance']}")
    print(f"   Industry: {scenario['industry']}")
    print(f"   High Sensitivity: {scenario['high_sensitivity']}")
    
    recs = recommend_drift_method(**scenario)
    print("   Recommendations:")
    for j, rec in enumerate(recs, 1):
        print(f"   {j}. {rec['method']} - {rec['reason']}")

# %% [markdown]
# ## ‡∏™‡∏£‡∏∏‡∏õ LAB 1
#
# ### ‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ:
# 1. **Covariate Shift**: P(X) ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô ‡πÅ‡∏ï‡πà P(Y|X) ‡∏Ñ‡∏á‡∏ó‡∏µ‡πà
# 2. **Concept Drift**: P(Y|X) ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô (relationship ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô)
# 3. **KS Test**: ‡∏ß‡∏±‡∏î maximum distance ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á CDFs, ‡πÉ‡∏´‡πâ p-value
# 4. **PSI**: ‡∏ß‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡∏≠‡∏á proportions, ‡∏°‡∏µ threshold ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô
# 5. **Wasserstein**: ‡∏ß‡∏±‡∏î "‡∏á‡∏≤‡∏ô" ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô distribution
#
# ### Best Practices:
# - ‡πÉ‡∏ä‡πâ‡∏´‡∏•‡∏≤‡∏¢ methods ‡πÄ‡∏û‡∏∑‡πà‡∏≠ cross-validate
# - ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å method ‡∏ï‡∏≤‡∏° data type ‡πÅ‡∏•‡∏∞ business requirements
# - ‡∏ï‡∏±‡πâ‡∏á threshold ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡∏Å‡∏±‡∏ö context

# %%
print("=" * 60)
print("‚úÖ LAB 1 COMPLETED!")
print("=" * 60)
print("""
üìö Key Takeaways:
1. Covariate Shift vs Concept Drift - ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á
2. KS Test - ‡πÉ‡∏ä‡πâ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ statistical significance
3. PSI - Industry standard ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö finance
4. Wasserstein - Sensitive ‡∏ï‡πà‡∏≠ location shifts

üîú Next: LAB 2 - Feature Drift Detection
""")

## LAB 2: Feature Drift Detection


# %% [markdown]
# # LAB 2: Feature Drift Detection
# ## ‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö Drift ‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞ Feature
#
# ### ‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ:
# 1. ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö drift ‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞ feature ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏∞‡∏ö‡∏ö
# 2. ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå numerical vs categorical feature drift
# 3. ‡∏™‡∏£‡πâ‡∏≤‡∏á visualization ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö feature distributions over time
#
# ### ‡∏ó‡∏§‡∏©‡∏é‡∏µ:
# ‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö drift ‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞ feature ‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡πÄ‡∏û‡∏£‡∏≤‡∏∞:
# - ‡∏ä‡πà‡∏ß‡∏¢‡∏£‡∏∞‡∏ö‡∏∏ root cause ‡∏Ç‡∏≠‡∏á model performance degradation
# - ‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏ß‡πà‡∏≤ feature ‡πÑ‡∏´‡∏ô‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î
# - ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ prioritize ‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÑ‡∏î‡πâ

# %% [markdown]
# ## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 1: ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° Environment

# %%
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from sklearn.datasets import make_classification
from sklearn.preprocessing import StandardScaler
import warnings
warnings.filterwarnings('ignore')

np.random.seed(42)
plt.rcParams['figure.figsize'] = (14, 8)

print("‚úÖ Libraries imported successfully!")

# %% [markdown]
# ## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 2: ‡∏™‡∏£‡πâ‡∏≤‡∏á Dataset ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏´‡∏•‡∏≤‡∏¢ Features
#
# ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á dataset ‡∏ó‡∏µ‡πà‡∏à‡∏≥‡∏•‡∏≠‡∏á‡∏™‡∏ñ‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏ì‡πå‡∏à‡∏£‡∏¥‡∏á:
# - ‡∏°‡∏µ‡∏ó‡∏±‡πâ‡∏á numerical ‡πÅ‡∏•‡∏∞ categorical features
# - ‡∏ö‡∏≤‡∏á features ‡∏°‡∏µ drift ‡∏ö‡∏≤‡∏á features ‡πÑ‡∏°‡πà‡∏°‡∏µ
# - ‡∏°‡∏µ drift ‡πÉ‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ï‡πà‡∏≤‡∏á‡πÜ

# %%
def create_multi_feature_dataset(n_samples=2000, drift_level='mixed'):
    """
    ‡∏™‡∏£‡πâ‡∏≤‡∏á dataset ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏´‡∏•‡∏≤‡∏¢ features ‡∏û‡∏£‡πâ‡∏≠‡∏° simulated drift
    
    Parameters:
    -----------
    n_samples : int - ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô samples ‡∏ï‡πà‡∏≠ dataset
    drift_level : str - 'none', 'mild', 'severe', 'mixed'
    
    Returns:
    --------
    tuple : (reference_df, current_df, feature_info)
    """
    
    np.random.seed(42)
    
    # === Reference Data (Training) ===
    reference_data = {
        # Numerical Features
        'age': np.random.normal(35, 10, n_samples),
        'income': np.random.normal(50000, 15000, n_samples),
        'credit_score': np.random.normal(650, 80, n_samples),
        'account_balance': np.random.exponential(10000, n_samples),
        'transaction_count': np.random.poisson(20, n_samples),
        
        # Categorical Features (encoded as integers)
        'region': np.random.choice([0, 1, 2, 3], n_samples, p=[0.3, 0.3, 0.25, 0.15]),
        'customer_type': np.random.choice([0, 1, 2], n_samples, p=[0.5, 0.35, 0.15]),
        'product_category': np.random.choice([0, 1, 2, 3, 4], n_samples, p=[0.2, 0.2, 0.2, 0.2, 0.2])
    }
    
    # === Current Data (Production) with various drift levels ===
    if drift_level == 'none':
        current_data = {k: v.copy() for k, v in reference_data.items()}
        # ‡πÄ‡∏û‡∏¥‡πà‡∏° random noise ‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢
        for key in ['age', 'income', 'credit_score']:
            current_data[key] = np.random.normal(
                reference_data[key].mean(),
                reference_data[key].std(),
                n_samples
            )
    
    elif drift_level == 'mixed':
        current_data = {
            # No drift
            'age': np.random.normal(35, 10, n_samples),  # ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°
            'credit_score': np.random.normal(650, 80, n_samples),  # ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°
            
            # Mild drift
            'income': np.random.normal(55000, 15000, n_samples),  # mean shift ‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢
            'transaction_count': np.random.poisson(25, n_samples),  # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡∏∂‡πâ‡∏ô‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢
            
            # Severe drift
            'account_balance': np.random.exponential(20000, n_samples),  # scale ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏°‡∏≤‡∏Å
            
            # Categorical drift
            'region': np.random.choice([0, 1, 2, 3], n_samples, p=[0.15, 0.15, 0.35, 0.35]),  # distribution ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô
            'customer_type': np.random.choice([0, 1, 2], n_samples, p=[0.5, 0.35, 0.15]),  # ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°
            'product_category': np.random.choice([0, 1, 2, 3, 4], n_samples, p=[0.4, 0.1, 0.1, 0.2, 0.2])  # ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏ö‡πâ‡∏≤‡∏á
        }
    
    reference_df = pd.DataFrame(reference_data)
    current_df = pd.DataFrame(current_data)
    
    # Feature info
    feature_info = {
        'numerical': ['age', 'income', 'credit_score', 'account_balance', 'transaction_count'],
        'categorical': ['region', 'customer_type', 'product_category'],
        'expected_drift': {
            'age': 'none',
            'income': 'mild',
            'credit_score': 'none',
            'account_balance': 'severe',
            'transaction_count': 'mild',
            'region': 'severe',
            'customer_type': 'none',
            'product_category': 'mild'
        }
    }
    
    return reference_df, current_df, feature_info

# ‡∏™‡∏£‡πâ‡∏≤‡∏á dataset
reference_df, current_df, feature_info = create_multi_feature_dataset(drift_level='mixed')

print("üìä Reference Data Shape:", reference_df.shape)
print("üìä Current Data Shape:", current_df.shape)
print("\nüìã Feature Types:")
print(f"  Numerical: {feature_info['numerical']}")
print(f"  Categorical: {feature_info['categorical']}")

print("\nüìã Expected Drift Levels:")
for feature, level in feature_info['expected_drift'].items():
    print(f"  {feature}: {level}")

# %%
# ‡πÅ‡∏™‡∏î‡∏á summary statistics
print("\n" + "=" * 60)
print("REFERENCE DATA SUMMARY")
print("=" * 60)
print(reference_df.describe())

print("\n" + "=" * 60)
print("CURRENT DATA SUMMARY")
print("=" * 60)
print(current_df.describe())

# %% [markdown]
# ## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 3: ‡∏™‡∏£‡πâ‡∏≤‡∏á Feature Drift Detector Class
#
# ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á class ‡∏ó‡∏µ‡πà‡∏£‡∏ß‡∏°‡∏ó‡∏∏‡∏Å drift detection methods ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡∏™‡∏∞‡∏î‡∏ß‡∏Å

# %%
class FeatureDriftDetector:
    """
    Class ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö drift ‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞ feature
    
    ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á numerical ‡πÅ‡∏•‡∏∞ categorical features
    ‡πÉ‡∏ä‡πâ‡∏´‡∏•‡∏≤‡∏¢ statistical tests ‡πÄ‡∏û‡∏∑‡πà‡∏≠ comprehensive analysis
    """
    
    def __init__(self, reference_data, current_data, numerical_features=None, categorical_features=None):
        """
        Initialize detector
        
        Parameters:
        -----------
        reference_data : pd.DataFrame - ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• reference (training)
        current_data : pd.DataFrame - ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• current (production)
        numerical_features : list - ‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠ numerical features
        categorical_features : list - ‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠ categorical features
        """
        self.reference = reference_data
        self.current = current_data
        
        # Auto-detect feature types if not provided
        if numerical_features is None:
            self.numerical_features = reference_data.select_dtypes(include=[np.number]).columns.tolist()
        else:
            self.numerical_features = numerical_features
            
        if categorical_features is None:
            self.categorical_features = []
        else:
            self.categorical_features = categorical_features
        
        self.results = {}
    
    def ks_test(self, feature):
        """Kolmogorov-Smirnov test ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö numerical features"""
        stat, p_value = stats.ks_2samp(
            self.reference[feature].dropna(),
            self.current[feature].dropna()
        )
        return {'statistic': stat, 'p_value': p_value}
    
    def calculate_psi(self, feature, bins=10):
        """Population Stability Index"""
        ref_data = self.reference[feature].dropna()
        cur_data = self.current[feature].dropna()
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á bins
        breakpoints = np.percentile(ref_data, np.linspace(0, 100, bins + 1))
        breakpoints = np.unique(breakpoints)
        
        ref_counts, _ = np.histogram(ref_data, bins=breakpoints)
        cur_counts, _ = np.histogram(cur_data, bins=breakpoints)
        
        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì proportions
        eps = 1e-6
        ref_props = ref_counts / len(ref_data) + eps
        cur_props = cur_counts / len(cur_data) + eps
        
        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì PSI
        psi = np.sum((cur_props - ref_props) * np.log(cur_props / ref_props))
        
        return {'psi': psi}
    
    def wasserstein_test(self, feature):
        """Wasserstein Distance"""
        ref_data = self.reference[feature].dropna()
        cur_data = self.current[feature].dropna()
        
        distance = stats.wasserstein_distance(ref_data, cur_data)
        
        # Normalize by std of reference
        std = ref_data.std()
        normalized = distance / std if std > 0 else distance
        
        return {'distance': distance, 'normalized_distance': normalized}
    
    def chi_squared_test(self, feature):
        """Chi-squared test ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö categorical features"""
        # ‡∏ô‡∏±‡∏ö frequency ‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞ category
        ref_counts = self.reference[feature].value_counts()
        cur_counts = self.current[feature].value_counts()
        
        # ‡∏£‡∏ß‡∏° categories ‡πÉ‡∏´‡πâ‡πÄ‡∏ó‡πà‡∏≤‡∏Å‡∏±‡∏ô
        all_categories = set(ref_counts.index) | set(cur_counts.index)
        ref_freq = [ref_counts.get(cat, 0) for cat in all_categories]
        cur_freq = [cur_counts.get(cat, 0) for cat in all_categories]
        
        # Normalize to expected frequencies
        total_ref = sum(ref_freq)
        total_cur = sum(cur_freq)
        expected = [(r / total_ref) * total_cur for r in ref_freq]
        
        # Chi-squared test
        try:
            stat, p_value = stats.chisquare(cur_freq, expected)
        except:
            stat, p_value = 0, 1.0
        
        return {'statistic': stat, 'p_value': p_value}
    
    def analyze_numerical_feature(self, feature):
        """‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå drift ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö numerical feature"""
        results = {
            'feature': feature,
            'type': 'numerical',
            'ks_test': self.ks_test(feature),
            'psi': self.calculate_psi(feature),
            'wasserstein': self.wasserstein_test(feature)
        }
        
        # ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•
        ks_drift = results['ks_test']['p_value'] < 0.05
        psi_value = results['psi']['psi']
        
        if psi_value < 0.1:
            psi_severity = 'none'
        elif psi_value < 0.25:
            psi_severity = 'mild'
        else:
            psi_severity = 'severe'
        
        results['drift_detected'] = ks_drift or psi_severity != 'none'
        results['severity'] = psi_severity
        
        return results
    
    def analyze_categorical_feature(self, feature):
        """‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå drift ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö categorical feature"""
        chi2_result = self.chi_squared_test(feature)
        
        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì PSI ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö categorical
        ref_props = self.reference[feature].value_counts(normalize=True)
        cur_props = self.current[feature].value_counts(normalize=True)
        
        all_cats = set(ref_props.index) | set(cur_props.index)
        eps = 1e-6
        psi = 0
        for cat in all_cats:
            ref_p = ref_props.get(cat, 0) + eps
            cur_p = cur_props.get(cat, 0) + eps
            psi += (cur_p - ref_p) * np.log(cur_p / ref_p)
        
        results = {
            'feature': feature,
            'type': 'categorical',
            'chi_squared': chi2_result,
            'psi': {'psi': psi}
        }
        
        # ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•
        chi2_drift = chi2_result['p_value'] < 0.05
        
        if psi < 0.1:
            psi_severity = 'none'
        elif psi < 0.25:
            psi_severity = 'mild'
        else:
            psi_severity = 'severe'
        
        results['drift_detected'] = chi2_drift or psi_severity != 'none'
        results['severity'] = psi_severity
        
        return results
    
    def analyze_all_features(self):
        """‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå drift ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏∏‡∏Å features"""
        all_results = {}
        
        # Numerical features
        for feature in self.numerical_features:
            all_results[feature] = self.analyze_numerical_feature(feature)
        
        # Categorical features
        for feature in self.categorical_features:
            all_results[feature] = self.analyze_categorical_feature(feature)
        
        self.results = all_results
        return all_results
    
    def get_summary_report(self):
        """‡∏™‡∏£‡πâ‡∏≤‡∏á summary report"""
        if not self.results:
            self.analyze_all_features()
        
        summary = []
        for feature, result in self.results.items():
            row = {
                'Feature': feature,
                'Type': result['type'],
                'Drift Detected': '‚úì' if result['drift_detected'] else '‚úó',
                'Severity': result['severity'].upper(),
                'PSI': f"{result['psi']['psi']:.4f}"
            }
            
            if result['type'] == 'numerical':
                row['KS p-value'] = f"{result['ks_test']['p_value']:.4f}"
            else:
                row['Chi2 p-value'] = f"{result['chi_squared']['p_value']:.4f}"
            
            summary.append(row)
        
        return pd.DataFrame(summary)

# %%
# ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô FeatureDriftDetector
detector = FeatureDriftDetector(
    reference_data=reference_df,
    current_data=current_df,
    numerical_features=feature_info['numerical'],
    categorical_features=feature_info['categorical']
)

# ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ó‡∏∏‡∏Å features
all_results = detector.analyze_all_features()

# ‡πÅ‡∏™‡∏î‡∏á summary report
print("=" * 80)
print("üìä FEATURE DRIFT DETECTION REPORT")
print("=" * 80)
summary_df = detector.get_summary_report()
print(summary_df.to_string(index=False))

# %% [markdown]
# ## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 4: Visualize Feature Distributions
#
# ‡∏Å‡∏≤‡∏£ visualize ‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏´‡πâ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à drift ‡πÑ‡∏î‡πâ‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô

# %%
def plot_numerical_feature_drift(reference, current, feature_name, ax=None):
    """
    ‡∏™‡∏£‡πâ‡∏≤‡∏á visualization ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö numerical feature drift
    """
    if ax is None:
        fig, ax = plt.subplots(figsize=(10, 4))
    
    # Histogram
    ax.hist(reference[feature_name], bins=30, alpha=0.5, label='Reference', color='blue', density=True)
    ax.hist(current[feature_name], bins=30, alpha=0.5, label='Current', color='red', density=True)
    
    # Statistics
    ref_mean = reference[feature_name].mean()
    cur_mean = current[feature_name].mean()
    
    ax.axvline(ref_mean, color='blue', linestyle='--', linewidth=2)
    ax.axvline(cur_mean, color='red', linestyle='--', linewidth=2)
    
    # Calculate PSI for title
    detector_temp = FeatureDriftDetector(reference, current, [feature_name], [])
    result = detector_temp.analyze_numerical_feature(feature_name)
    
    ax.set_title(f'{feature_name}\nPSI: {result["psi"]["psi"]:.4f} | Severity: {result["severity"].upper()}')
    ax.set_xlabel(feature_name)
    ax.set_ylabel('Density')
    ax.legend()
    
    return ax

def plot_categorical_feature_drift(reference, current, feature_name, ax=None):
    """
    ‡∏™‡∏£‡πâ‡∏≤‡∏á visualization ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö categorical feature drift
    """
    if ax is None:
        fig, ax = plt.subplots(figsize=(10, 4))
    
    # Calculate proportions
    ref_props = reference[feature_name].value_counts(normalize=True).sort_index()
    cur_props = current[feature_name].value_counts(normalize=True).sort_index()
    
    # Align categories
    all_cats = sorted(set(ref_props.index) | set(cur_props.index))
    ref_values = [ref_props.get(cat, 0) for cat in all_cats]
    cur_values = [cur_props.get(cat, 0) for cat in all_cats]
    
    x = np.arange(len(all_cats))
    width = 0.35
    
    ax.bar(x - width/2, ref_values, width, label='Reference', color='blue', alpha=0.7)
    ax.bar(x + width/2, cur_values, width, label='Current', color='red', alpha=0.7)
    
    ax.set_xticks(x)
    ax.set_xticklabels([f'Cat {c}' for c in all_cats])
    
    # Calculate PSI for title
    detector_temp = FeatureDriftDetector(reference, current, [], [feature_name])
    result = detector_temp.analyze_categorical_feature(feature_name)
    
    ax.set_title(f'{feature_name}\nPSI: {result["psi"]["psi"]:.4f} | Severity: {result["severity"].upper()}')
    ax.set_xlabel('Category')
    ax.set_ylabel('Proportion')
    ax.legend()
    
    return ax

# %%
# Plot all numerical features
fig, axes = plt.subplots(2, 3, figsize=(15, 10))
axes = axes.flatten()

for idx, feature in enumerate(feature_info['numerical']):
    plot_numerical_feature_drift(reference_df, current_df, feature, axes[idx])

# Remove empty subplot
if len(feature_info['numerical']) < 6:
    axes[-1].set_visible(False)

plt.suptitle('Numerical Features: Distribution Comparison\n(Reference vs Current)', fontsize=14, fontweight='bold')
plt.tight_layout()
plt.savefig('numerical_features_drift.png', dpi=150, bbox_inches='tight')
plt.show()

# %%
# Plot all categorical features
fig, axes = plt.subplots(1, 3, figsize=(15, 4))

for idx, feature in enumerate(feature_info['categorical']):
    plot_categorical_feature_drift(reference_df, current_df, feature, axes[idx])

plt.suptitle('Categorical Features: Distribution Comparison\n(Reference vs Current)', fontsize=14, fontweight='bold')
plt.tight_layout()
plt.savefig('categorical_features_drift.png', dpi=150, bbox_inches='tight')
plt.show()

# %% [markdown]
# ## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 5: Feature Drift Ranking ‡πÅ‡∏•‡∏∞ Prioritization
#
# ‡∏à‡∏±‡∏î‡∏•‡∏≥‡∏î‡∏±‡∏ö features ‡∏ï‡∏≤‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏∏‡∏ô‡πÅ‡∏£‡∏á‡∏Ç‡∏≠‡∏á drift

# %%
def rank_features_by_drift(detector):
    """
    ‡∏à‡∏±‡∏î‡∏•‡∏≥‡∏î‡∏±‡∏ö features ‡∏ï‡∏≤‡∏° drift severity
    """
    if not detector.results:
        detector.analyze_all_features()
    
    rankings = []
    for feature, result in detector.results.items():
        rankings.append({
            'feature': feature,
            'psi': result['psi']['psi'],
            'drift_detected': result['drift_detected'],
            'severity': result['severity'],
            'type': result['type']
        })
    
    # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏ï‡∏≤‡∏° PSI (‡∏°‡∏≤‡∏Å‡πÑ‡∏õ‡∏ô‡πâ‡∏≠‡∏¢)
    rankings.sort(key=lambda x: x['psi'], reverse=True)
    
    return rankings

# ‡∏à‡∏±‡∏î‡∏•‡∏≥‡∏î‡∏±‡∏ö features
rankings = rank_features_by_drift(detector)

print("=" * 60)
print("üìä FEATURE DRIFT RANKING (Sorted by PSI)")
print("=" * 60)

for rank, item in enumerate(rankings, 1):
    severity_emoji = {'none': 'üü¢', 'mild': 'üü°', 'severe': 'üî¥'}[item['severity']]
    print(f"{rank}. {item['feature']:<20} | PSI: {item['psi']:.4f} | {severity_emoji} {item['severity'].upper()}")

# %%
# ‡∏™‡∏£‡πâ‡∏≤‡∏á Drift Ranking Visualization
fig, ax = plt.subplots(figsize=(12, 6))

features = [r['feature'] for r in rankings]
psi_values = [r['psi'] for r in rankings]
colors = ['red' if r['severity'] == 'severe' else 'orange' if r['severity'] == 'mild' else 'green' for r in rankings]

bars = ax.barh(features, psi_values, color=colors, alpha=0.7, edgecolor='black')

# Add threshold lines
ax.axvline(0.1, color='orange', linestyle='--', label='Mild Threshold (0.1)')
ax.axvline(0.25, color='red', linestyle='--', label='Severe Threshold (0.25)')

ax.set_xlabel('PSI Value')
ax.set_ylabel('Feature')
ax.set_title('Feature Drift Ranking by PSI\n(Red=Severe, Orange=Mild, Green=None)')
ax.legend()

# Add value labels
for bar, psi in zip(bars, psi_values):
    ax.text(psi + 0.01, bar.get_y() + bar.get_height()/2, f'{psi:.3f}', va='center')

plt.tight_layout()
plt.savefig('feature_drift_ranking.png', dpi=150, bbox_inches='tight')
plt.show()

# %% [markdown]
# ## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 6: Time-based Distribution Analysis
#
# ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ß‡πà‡∏≤ distribution ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÄ‡∏ß‡∏•‡∏≤‡∏ú‡πà‡∏≤‡∏ô‡πÑ‡∏õ

# %%
def simulate_time_series_data(n_periods=6, samples_per_period=500):
    """
    ‡∏à‡∏≥‡∏•‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏ï‡∏≤‡∏°‡πÄ‡∏ß‡∏•‡∏≤
    """
    all_data = []
    
    for period in range(n_periods):
        np.random.seed(42 + period)
        
        # Gradual drift: mean ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡∏∂‡πâ‡∏ô‡∏ó‡∏µ‡∏•‡∏∞‡∏ô‡πâ‡∏≠‡∏¢
        age_mean = 35 + period * 2  # drift ‡πÉ‡∏ô age
        income_mean = 50000  # ‡πÑ‡∏°‡πà‡∏°‡∏µ drift
        
        data = pd.DataFrame({
            'period': period,
            'age': np.random.normal(age_mean, 10, samples_per_period),
            'income': np.random.normal(income_mean, 15000, samples_per_period),
            'credit_score': np.random.normal(650, 80, samples_per_period)
        })
        all_data.append(data)
    
    return pd.concat(all_data, ignore_index=True)

# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• time series
time_series_data = simulate_time_series_data()
print(f"üìä Time Series Data Shape: {time_series_data.shape}")
print(f"üìã Periods: {time_series_data['period'].unique()}")

# %%
# Visualize distribution over time
fig, axes = plt.subplots(1, 3, figsize=(15, 5))

features_to_plot = ['age', 'income', 'credit_score']
colors = plt.cm.viridis(np.linspace(0, 1, 6))

for idx, feature in enumerate(features_to_plot):
    ax = axes[idx]
    
    for period in range(6):
        period_data = time_series_data[time_series_data['period'] == period][feature]
        ax.hist(period_data, bins=20, alpha=0.3, color=colors[period], label=f'Period {period}')
        ax.axvline(period_data.mean(), color=colors[period], linestyle='--', alpha=0.8)
    
    ax.set_title(f'{feature.capitalize()} Distribution Over Time')
    ax.set_xlabel(feature)
    ax.set_ylabel('Frequency')
    ax.legend(fontsize=8)

plt.suptitle('Feature Distributions Across Time Periods\n(Dashed lines = Mean per period)', fontsize=12, fontweight='bold')
plt.tight_layout()
plt.savefig('time_series_distributions.png', dpi=150, bbox_inches='tight')
plt.show()

# %%
# Calculate PSI over time (comparing each period to Period 0)
def calculate_psi_over_time(data, feature, reference_period=0):
    """
    ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì PSI ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Å‡∏±‡∏ö reference period
    """
    ref_data = data[data['period'] == reference_period][feature]
    
    results = []
    for period in data['period'].unique():
        if period == reference_period:
            results.append({'period': period, 'psi': 0})
        else:
            cur_data = data[data['period'] == period][feature]
            
            # Calculate PSI
            breakpoints = np.percentile(ref_data, np.linspace(0, 100, 11))
            breakpoints = np.unique(breakpoints)
            
            ref_counts, _ = np.histogram(ref_data, bins=breakpoints)
            cur_counts, _ = np.histogram(cur_data, bins=breakpoints)
            
            eps = 1e-6
            ref_props = ref_counts / len(ref_data) + eps
            cur_props = cur_counts / len(cur_data) + eps
            
            psi = np.sum((cur_props - ref_props) * np.log(cur_props / ref_props))
            results.append({'period': period, 'psi': psi})
    
    return pd.DataFrame(results)

# ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì PSI ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞ feature
fig, ax = plt.subplots(figsize=(10, 6))

for feature in ['age', 'income', 'credit_score']:
    psi_over_time = calculate_psi_over_time(time_series_data, feature)
    ax.plot(psi_over_time['period'], psi_over_time['psi'], marker='o', label=feature, linewidth=2)

ax.axhline(0.1, color='orange', linestyle='--', alpha=0.7, label='Mild Threshold')
ax.axhline(0.25, color='red', linestyle='--', alpha=0.7, label='Severe Threshold')

ax.set_xlabel('Time Period')
ax.set_ylabel('PSI (compared to Period 0)')
ax.set_title('PSI Trend Over Time\n(Reference: Period 0)')
ax.legend()
ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('psi_over_time.png', dpi=150, bbox_inches='tight')
plt.show()

print("\nüí° ‡∏™‡∏±‡∏á‡πÄ‡∏Å‡∏ï: Age ‡πÅ‡∏™‡∏î‡∏á gradual drift (PSI ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡∏∂‡πâ‡∏ô‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏¢‡πÜ)")
print("   ‡πÉ‡∏ô‡∏Ç‡∏ì‡∏∞‡∏ó‡∏µ‡πà Income ‡πÅ‡∏•‡∏∞ Credit Score ‡∏Ñ‡πà‡∏≠‡∏ô‡∏Ç‡πâ‡∏≤‡∏á stable")

# %% [markdown]
# ## ‡∏™‡∏£‡∏∏‡∏õ LAB 2
#
# ### ‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ:
# 1. **FeatureDriftDetector Class**: ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏°‡∏∑‡∏≠‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå drift ‡∏ó‡∏∏‡∏Å features
# 2. **Numerical vs Categorical**: ‡πÉ‡∏ä‡πâ methods ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡∏Å‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞ type
# 3. **Visualization**: ‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏´‡πâ‡πÄ‡∏´‡πá‡∏ô drift ‡πÑ‡∏î‡πâ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô
# 4. **Ranking**: ‡∏à‡∏±‡∏î‡∏•‡∏≥‡∏î‡∏±‡∏ö features ‡∏ï‡∏≤‡∏° severity ‡πÄ‡∏û‡∏∑‡πà‡∏≠ prioritization
# 5. **Time-based Analysis**: ‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏° drift ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÄ‡∏ß‡∏•‡∏≤‡∏ú‡πà‡∏≤‡∏ô‡πÑ‡∏õ

# %%
print("=" * 60)
print("‚úÖ LAB 2 COMPLETED!")
print("=" * 60)
print("""
üìö Key Takeaways:
1. Numerical features: ‡πÉ‡∏ä‡πâ KS Test + PSI + Wasserstein
2. Categorical features: ‡πÉ‡∏ä‡πâ Chi-squared + PSI
3. Visualization ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à drift
4. Track PSI over time ‡πÄ‡∏û‡∏∑‡πà‡∏≠ detect gradual drift

üîú Next: LAB 3 - Multivariate Drift Analysis
""")




## LAB 3: Multivariate Drift Analysis


# %% [markdown]
# # LAB 3: Multivariate Drift Analysis
# ## ‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå Drift ‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏¥‡∏î‡∏à‡∏≤‡∏Å‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á Features
#
# ### ‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ:
# 1. ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö drift ‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏¥‡∏î‡∏à‡∏≤‡∏Å‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á features
# 2. ‡πÉ‡∏ä‡πâ Dataset-level drift detection
# 3. ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå Correlation changes ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á features
#
# ### ‡∏ó‡∏§‡∏©‡∏é‡∏µ:
# **Multivariate Drift** ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡∏∂‡πâ‡∏ô‡πÄ‡∏°‡∏∑‡πà‡∏≠:
# - ‡πÅ‡∏ï‡πà‡∏•‡∏∞ feature ‡∏î‡∏π‡∏õ‡∏Å‡∏ï‡∏¥‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÅ‡∏¢‡∏Å
# - ‡πÅ‡∏ï‡πà‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á features ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÑ‡∏õ
# - ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: correlation ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á age ‡πÅ‡∏•‡∏∞ income ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô

# %% [markdown]
# ## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 1: ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° Environment

# %%
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.covariance import EmpiricalCovariance
import warnings
warnings.filterwarnings('ignore')

np.random.seed(42)
plt.rcParams['figure.figsize'] = (12, 8)

print("‚úÖ Libraries imported successfully!")

# %% [markdown]
# ## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 2: ‡∏™‡∏£‡πâ‡∏≤‡∏á Dataset ‡∏ó‡∏µ‡πà‡∏°‡∏µ Multivariate Drift
#
# ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà:
# - Marginal distributions ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Å‡∏±‡∏ô (‡πÑ‡∏°‡πà‡∏°‡∏µ univariate drift)
# - ‡πÅ‡∏ï‡πà correlation structure ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô (multivariate drift)

# %%
def create_multivariate_drift_data(n_samples=2000):
    """
    ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ multivariate drift
    - Marginal distributions ‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢‡∏Å‡∏±‡∏ô
    - Correlation structure ‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô
    """
    np.random.seed(42)
    
    # === Reference Data ===
    # Correlated features: age ‡πÅ‡∏•‡∏∞ income ‡∏°‡∏µ positive correlation ‡∏™‡∏π‡∏á
    mean_ref = [35, 50000, 650]  # age, income, credit_score
    cov_ref = [
        [100, 3000, 50],      # age variance ‡πÅ‡∏•‡∏∞ covariance
        [3000, 225000000, 100000],  # income variance ‡πÅ‡∏•‡∏∞ covariance
        [50, 100000, 6400]    # credit_score variance ‡πÅ‡∏•‡∏∞ covariance
    ]
    
    ref_data = np.random.multivariate_normal(mean_ref, cov_ref, n_samples)
    reference_df = pd.DataFrame(ref_data, columns=['age', 'income', 'credit_score'])
    
    # === Current Data (Multivariate Drift) ===
    # Same marginals but different correlation structure
    mean_cur = [35, 50000, 650]  # means ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°
    
    # Correlation structure ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô: age ‡πÅ‡∏•‡∏∞ income correlation ‡∏•‡∏î‡∏•‡∏á
    cov_cur = [
        [100, 500, 50],       # correlation ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á age-income ‡∏•‡∏î‡∏•‡∏á‡∏°‡∏≤‡∏Å!
        [500, 225000000, 100000],
        [50, 100000, 6400]
    ]
    
    cur_data = np.random.multivariate_normal(mean_cur, cov_cur, n_samples)
    current_df = pd.DataFrame(cur_data, columns=['age', 'income', 'credit_score'])
    
    return reference_df, current_df

# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
ref_multi, cur_multi = create_multivariate_drift_data()

print("üìä Reference Data:")
print(ref_multi.describe())
print("\nüìä Current Data:")
print(cur_multi.describe())

# %%
# ‡πÅ‡∏™‡∏î‡∏á correlation matrix comparison
fig, axes = plt.subplots(1, 3, figsize=(15, 4))

# Reference correlation matrix
corr_ref = ref_multi.corr()
sns.heatmap(corr_ref, annot=True, cmap='coolwarm', center=0, ax=axes[0], 
            vmin=-1, vmax=1, fmt='.3f')
axes[0].set_title('Reference Correlation Matrix')

# Current correlation matrix
corr_cur = cur_multi.corr()
sns.heatmap(corr_cur, annot=True, cmap='coolwarm', center=0, ax=axes[1],
            vmin=-1, vmax=1, fmt='.3f')
axes[1].set_title('Current Correlation Matrix')

# Difference
corr_diff = corr_cur - corr_ref
sns.heatmap(corr_diff, annot=True, cmap='RdBu', center=0, ax=axes[2],
            vmin=-1, vmax=1, fmt='.3f')
axes[2].set_title('Correlation Difference\n(Current - Reference)')

plt.tight_layout()
plt.savefig('correlation_comparison.png', dpi=150, bbox_inches='tight')
plt.show()

print("\nüí° ‡∏™‡∏±‡∏á‡πÄ‡∏Å‡∏ï: Correlation ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á Age-Income ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏à‡∏≤‡∏Å ~0.6 ‡πÄ‡∏õ‡πá‡∏ô ~0.03")

# %% [markdown]
# ## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 3: Univariate vs Multivariate Drift Detection
#
# ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏ß‡πà‡∏≤ univariate methods ‡∏û‡∏•‡∏≤‡∏î‡∏≠‡∏∞‡πÑ‡∏£‡πÑ‡∏õ

# %%
from scipy import stats

def univariate_drift_check(ref_df, cur_df):
    """‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö drift ‡∏î‡πâ‡∏ß‡∏¢ univariate methods"""
    results = []
    
    for col in ref_df.columns:
        # KS Test
        ks_stat, ks_pval = stats.ks_2samp(ref_df[col], cur_df[col])
        
        # Mean comparison
        ref_mean = ref_df[col].mean()
        cur_mean = cur_df[col].mean()
        mean_diff_pct = abs(cur_mean - ref_mean) / ref_mean * 100
        
        # Std comparison
        ref_std = ref_df[col].std()
        cur_std = cur_df[col].std()
        std_diff_pct = abs(cur_std - ref_std) / ref_std * 100
        
        results.append({
            'feature': col,
            'ks_statistic': ks_stat,
            'ks_pvalue': ks_pval,
            'drift_detected': ks_pval < 0.05,
            'mean_diff_%': mean_diff_pct,
            'std_diff_%': std_diff_pct
        })
    
    return pd.DataFrame(results)

# ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö univariate drift
univariate_results = univariate_drift_check(ref_multi, cur_multi)
print("=" * 60)
print("üìä UNIVARIATE DRIFT DETECTION RESULTS")
print("=" * 60)
print(univariate_results.to_string(index=False))

print("\nüí° ‡∏™‡∏±‡∏á‡πÄ‡∏Å‡∏ï: Univariate methods ‡πÑ‡∏°‡πà‡∏û‡∏ö drift ‡∏ó‡∏µ‡πà‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç!")
print("   ‡πÅ‡∏ï‡πà‡πÄ‡∏£‡∏≤‡∏£‡∏π‡πâ‡∏ß‡πà‡∏≤ correlation structure ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÑ‡∏õ")

# %% [markdown]
# ## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 4: Correlation-based Drift Detection
#
# ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡∏≠‡∏á correlation structure

# %%
def correlation_drift_test(ref_df, cur_df, significance_level=0.05):
    """
    ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö drift ‡πÉ‡∏ô correlation structure
    
    ‡πÉ‡∏ä‡πâ Fisher's Z transformation ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö correlations
    """
    results = []
    
    # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì correlation matrices
    ref_corr = ref_df.corr()
    cur_corr = cur_df.corr()
    
    n_ref = len(ref_df)
    n_cur = len(cur_df)
    
    # ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞ pair
    for i, col1 in enumerate(ref_df.columns):
        for j, col2 in enumerate(ref_df.columns):
            if i >= j:  # ‡∏Ç‡πâ‡∏≤‡∏° diagonal ‡πÅ‡∏•‡∏∞ lower triangle
                continue
            
            r_ref = ref_corr.loc[col1, col2]
            r_cur = cur_corr.loc[col1, col2]
            
            # Fisher's Z transformation
            z_ref = np.arctanh(r_ref)
            z_cur = np.arctanh(r_cur)
            
            # Standard error
            se = np.sqrt(1/(n_ref-3) + 1/(n_cur-3))
            
            # Z-test statistic
            z_stat = (z_ref - z_cur) / se
            p_value = 2 * (1 - stats.norm.cdf(abs(z_stat)))
            
            results.append({
                'feature_pair': f'{col1} - {col2}',
                'ref_correlation': r_ref,
                'cur_correlation': r_cur,
                'correlation_change': r_cur - r_ref,
                'z_statistic': z_stat,
                'p_value': p_value,
                'significant_change': p_value < significance_level
            })
    
    return pd.DataFrame(results)

# ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö correlation drift
corr_drift_results = correlation_drift_test(ref_multi, cur_multi)
print("=" * 60)
print("üìä CORRELATION DRIFT DETECTION RESULTS")
print("=" * 60)
print(corr_drift_results.to_string(index=False))

# %%
# Visualize correlation changes
fig, ax = plt.subplots(figsize=(10, 6))

x = range(len(corr_drift_results))
colors = ['red' if sig else 'green' for sig in corr_drift_results['significant_change']]

bars = ax.bar(x, corr_drift_results['correlation_change'], color=colors, alpha=0.7, edgecolor='black')

ax.set_xticks(x)
ax.set_xticklabels(corr_drift_results['feature_pair'], rotation=45, ha='right')
ax.set_ylabel('Correlation Change')
ax.set_title('Correlation Changes Between Reference and Current Data\n(Red = Statistically Significant)')
ax.axhline(0, color='black', linestyle='-', linewidth=0.5)

# Add value labels
for bar, val in zip(bars, corr_drift_results['correlation_change']):
    height = bar.get_height()
    ax.text(bar.get_x() + bar.get_width()/2., height, f'{val:.3f}',
            ha='center', va='bottom' if height > 0 else 'top')

plt.tight_layout()
plt.savefig('correlation_drift.png', dpi=150, bbox_inches='tight')
plt.show()

# %% [markdown]
# ## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 5: PCA-based Multivariate Drift Detection
#
# ‡πÉ‡∏ä‡πâ PCA ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö drift ‡πÉ‡∏ô multivariate structure

# %%
def pca_drift_detection(ref_df, cur_df, n_components=None):
    """
    ‡πÉ‡∏ä‡πâ PCA ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö multivariate drift
    
    ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö:
    1. Explained variance ratios
    2. Principal component directions
    3. Reconstruction errors
    """
    if n_components is None:
        n_components = min(len(ref_df.columns), 3)
    
    # Standardize data
    scaler = StandardScaler()
    ref_scaled = scaler.fit_transform(ref_df)
    cur_scaled = scaler.transform(cur_df)
    
    # Fit PCA on reference data
    pca_ref = PCA(n_components=n_components)
    pca_ref.fit(ref_scaled)
    
    # Transform both datasets using reference PCA
    ref_transformed = pca_ref.transform(ref_scaled)
    cur_transformed = pca_ref.transform(cur_scaled)
    
    # 1. Compare explained variance
    ref_explained_var = pca_ref.explained_variance_ratio_
    
    # Fit PCA on current data for comparison
    pca_cur = PCA(n_components=n_components)
    pca_cur.fit(cur_scaled)
    cur_explained_var = pca_cur.explained_variance_ratio_
    
    # 2. Compare principal components (using cosine similarity)
    component_similarities = []
    for i in range(n_components):
        cos_sim = np.dot(pca_ref.components_[i], pca_cur.components_[i])
        cos_sim = abs(cos_sim)  # Absolute value ‡πÄ‡∏û‡∏£‡∏≤‡∏∞ sign ‡∏≠‡∏≤‡∏à flip
        component_similarities.append(cos_sim)
    
    # 3. Reconstruction error on current data using reference PCA
    ref_reconstructed = pca_ref.inverse_transform(ref_transformed)
    cur_reconstructed = pca_ref.inverse_transform(cur_transformed)
    
    ref_recon_error = np.mean((ref_scaled - ref_reconstructed) ** 2)
    cur_recon_error = np.mean((cur_scaled - cur_reconstructed) ** 2)
    
    results = {
        'ref_explained_variance': ref_explained_var,
        'cur_explained_variance': cur_explained_var,
        'component_similarities': component_similarities,
        'ref_reconstruction_error': ref_recon_error,
        'cur_reconstruction_error': cur_recon_error,
        'reconstruction_error_ratio': cur_recon_error / ref_recon_error if ref_recon_error > 0 else 1,
        'ref_components': pca_ref.components_,
        'cur_components': pca_cur.components_,
        'pca_ref': pca_ref,
        'ref_transformed': ref_transformed,
        'cur_transformed': cur_transformed
    }
    
    return results

# ‡∏ó‡∏≥ PCA drift detection
pca_results = pca_drift_detection(ref_multi, cur_multi)

print("=" * 60)
print("üìä PCA-BASED DRIFT DETECTION RESULTS")
print("=" * 60)

print("\n1. Explained Variance Comparison:")
for i in range(len(pca_results['ref_explained_variance'])):
    ref_ev = pca_results['ref_explained_variance'][i]
    cur_ev = pca_results['cur_explained_variance'][i]
    print(f"   PC{i+1}: Reference = {ref_ev:.4f}, Current = {cur_ev:.4f}, Diff = {cur_ev - ref_ev:.4f}")

print("\n2. Component Similarities (1.0 = identical):")
for i, sim in enumerate(pca_results['component_similarities']):
    status = "‚úì Similar" if sim > 0.9 else "‚ö†Ô∏è Changed!"
    print(f"   PC{i+1}: Cosine Similarity = {sim:.4f} {status}")

print("\n3. Reconstruction Error:")
print(f"   Reference: {pca_results['ref_reconstruction_error']:.6f}")
print(f"   Current: {pca_results['cur_reconstruction_error']:.6f}")
print(f"   Ratio: {pca_results['reconstruction_error_ratio']:.4f}x")

# %%
# Visualize PCA results
fig, axes = plt.subplots(2, 2, figsize=(14, 10))

# Plot 1: Explained variance comparison
ax1 = axes[0, 0]
x = range(1, len(pca_results['ref_explained_variance']) + 1)
width = 0.35
ax1.bar([i - width/2 for i in x], pca_results['ref_explained_variance'], 
        width, label='Reference', color='blue', alpha=0.7)
ax1.bar([i + width/2 for i in x], pca_results['cur_explained_variance'], 
        width, label='Current', color='red', alpha=0.7)
ax1.set_xlabel('Principal Component')
ax1.set_ylabel('Explained Variance Ratio')
ax1.set_title('Explained Variance Comparison')
ax1.legend()
ax1.set_xticks(x)

# Plot 2: Component similarities
ax2 = axes[0, 1]
colors = ['green' if s > 0.9 else 'red' for s in pca_results['component_similarities']]
ax2.bar(x, pca_results['component_similarities'], color=colors, alpha=0.7, edgecolor='black')
ax2.axhline(0.9, color='orange', linestyle='--', label='Similarity Threshold (0.9)')
ax2.set_xlabel('Principal Component')
ax2.set_ylabel('Cosine Similarity')
ax2.set_title('Principal Component Similarity\n(Reference vs Current)')
ax2.set_xticks(x)
ax2.legend()

# Plot 3: Scatter plot in PC space (Reference)
ax3 = axes[1, 0]
ax3.scatter(pca_results['ref_transformed'][:, 0], pca_results['ref_transformed'][:, 1], 
            alpha=0.3, label='Reference', color='blue')
ax3.scatter(pca_results['cur_transformed'][:, 0], pca_results['cur_transformed'][:, 1], 
            alpha=0.3, label='Current', color='red')
ax3.set_xlabel('PC1')
ax3.set_ylabel('PC2')
ax3.set_title('Data in PCA Space\n(Using Reference PCA)')
ax3.legend()

# Plot 4: Component loadings comparison
ax4 = axes[1, 1]
features = ref_multi.columns.tolist()
x = np.arange(len(features))
width = 0.35

for i in range(2):  # ‡πÅ‡∏™‡∏î‡∏á PC1 ‡πÅ‡∏•‡∏∞ PC2
    ref_loadings = pca_results['ref_components'][i]
    cur_loadings = pca_results['cur_components'][i]
    
    ax4.barh([f'PC{i+1} Ref' for f in features] if i == 0 else [f'PC{i+1} Cur' for f in features], 
             ref_loadings if i == 0 else cur_loadings)

ax4.set_xlabel('Loading')
ax4.set_title('PC Loadings (First 2 Components)')

plt.tight_layout()
plt.savefig('pca_drift_analysis.png', dpi=150, bbox_inches='tight')
plt.show()

# %% [markdown]
# ## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 6: Mahalanobis Distance for Dataset-level Drift
#
# ‡πÉ‡∏ä‡πâ Mahalanobis distance ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ß‡∏±‡∏î multivariate drift

# %%
def mahalanobis_drift_detection(ref_df, cur_df, threshold_percentile=95):
    """
    ‡πÉ‡∏ä‡πâ Mahalanobis distance ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö multivariate drift
    
    ‡∏ß‡∏±‡∏î‡∏ß‡πà‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡∏°‡πà‡∏≠‡∏¢‡∏π‡πà‡∏´‡πà‡∏≤‡∏á‡∏à‡∏≤‡∏Å distribution ‡∏Ç‡∏≠‡∏á reference data ‡πÄ‡∏ó‡πà‡∏≤‡πÑ‡∏£
    """
    # Standardize features
    scaler = StandardScaler()
    ref_scaled = scaler.fit_transform(ref_df)
    cur_scaled = scaler.transform(cur_df)
    
    # Fit covariance on reference data
    cov = EmpiricalCovariance().fit(ref_scaled)
    
    # Calculate Mahalanobis distances
    ref_distances = cov.mahalanobis(ref_scaled)
    cur_distances = cov.mahalanobis(cur_scaled)
    
    # Determine threshold from reference data
    threshold = np.percentile(ref_distances, threshold_percentile)
    
    # Count outliers
    ref_outliers = np.sum(ref_distances > threshold) / len(ref_distances) * 100
    cur_outliers = np.sum(cur_distances > threshold) / len(cur_distances) * 100
    
    # Statistical comparison
    ks_stat, ks_pval = stats.ks_2samp(ref_distances, cur_distances)
    
    results = {
        'ref_distances': ref_distances,
        'cur_distances': cur_distances,
        'threshold': threshold,
        'ref_mean_distance': np.mean(ref_distances),
        'cur_mean_distance': np.mean(cur_distances),
        'ref_outlier_pct': ref_outliers,
        'cur_outlier_pct': cur_outliers,
        'ks_statistic': ks_stat,
        'ks_pvalue': ks_pval,
        'drift_detected': ks_pval < 0.05
    }
    
    return results

# ‡∏ó‡∏≥ Mahalanobis drift detection
maha_results = mahalanobis_drift_detection(ref_multi, cur_multi)

print("=" * 60)
print("üìä MAHALANOBIS DISTANCE DRIFT DETECTION")
print("=" * 60)
print(f"\nMean Mahalanobis Distance:")
print(f"  Reference: {maha_results['ref_mean_distance']:.4f}")
print(f"  Current: {maha_results['cur_mean_distance']:.4f}")
print(f"\nOutlier Percentage (beyond {maha_results['threshold']:.2f} threshold):")
print(f"  Reference: {maha_results['ref_outlier_pct']:.2f}%")
print(f"  Current: {maha_results['cur_outlier_pct']:.2f}%")
print(f"\nKS Test on Distances:")
print(f"  Statistic: {maha_results['ks_statistic']:.4f}")
print(f"  P-value: {maha_results['ks_pvalue']:.4f}")
print(f"  Drift Detected: {'Yes ‚úì' if maha_results['drift_detected'] else 'No'}")

# %%
# Visualize Mahalanobis distances
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Plot 1: Distribution of Mahalanobis distances
ax1 = axes[0]
ax1.hist(maha_results['ref_distances'], bins=50, alpha=0.5, label='Reference', color='blue', density=True)
ax1.hist(maha_results['cur_distances'], bins=50, alpha=0.5, label='Current', color='red', density=True)
ax1.axvline(maha_results['threshold'], color='orange', linestyle='--', linewidth=2, 
            label=f'95th percentile threshold ({maha_results["threshold"]:.2f})')
ax1.set_xlabel('Mahalanobis Distance')
ax1.set_ylabel('Density')
ax1.set_title('Distribution of Mahalanobis Distances')
ax1.legend()

# Plot 2: CDF comparison
ax2 = axes[1]
sorted_ref = np.sort(maha_results['ref_distances'])
sorted_cur = np.sort(maha_results['cur_distances'])

ax2.plot(sorted_ref, np.arange(1, len(sorted_ref)+1)/len(sorted_ref), 
         'b-', label='Reference CDF', linewidth=2)
ax2.plot(sorted_cur, np.arange(1, len(sorted_cur)+1)/len(sorted_cur), 
         'r-', label='Current CDF', linewidth=2)
ax2.axvline(maha_results['threshold'], color='orange', linestyle='--', linewidth=2)
ax2.set_xlabel('Mahalanobis Distance')
ax2.set_ylabel('Cumulative Probability')
ax2.set_title(f'CDF Comparison\nKS Statistic = {maha_results["ks_statistic"]:.4f}')
ax2.legend()

plt.tight_layout()
plt.savefig('mahalanobis_drift.png', dpi=150, bbox_inches='tight')
plt.show()

# %% [markdown]
# ## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 7: Comprehensive Multivariate Drift Report

# %%
class MultivariateriftDetector:
    """
    Class ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö comprehensive multivariate drift detection
    """
    
    def __init__(self, reference_df, current_df):
        self.reference = reference_df
        self.current = current_df
        self.results = {}
    
    def analyze(self):
        """‡∏ó‡∏≥ comprehensive analysis"""
        
        # 1. Correlation analysis
        corr_results = correlation_drift_test(self.reference, self.current)
        self.results['correlation'] = corr_results
        
        # 2. PCA analysis
        pca_results = pca_drift_detection(self.reference, self.current)
        self.results['pca'] = pca_results
        
        # 3. Mahalanobis analysis
        maha_results = mahalanobis_drift_detection(self.reference, self.current)
        self.results['mahalanobis'] = maha_results
        
        # 4. Overall assessment
        drift_indicators = {
            'correlation_drift': any(corr_results['significant_change']),
            'pca_structure_change': any(sim < 0.9 for sim in pca_results['component_similarities']),
            'mahalanobis_drift': maha_results['drift_detected']
        }
        
        drift_count = sum(drift_indicators.values())
        
        self.results['summary'] = {
            'drift_indicators': drift_indicators,
            'drift_count': drift_count,
            'overall_assessment': 'HIGH' if drift_count >= 2 else 'MEDIUM' if drift_count == 1 else 'LOW'
        }
        
        return self.results
    
    def print_report(self):
        """‡∏û‡∏¥‡∏°‡∏û‡πå report ‡∏™‡∏£‡∏∏‡∏õ"""
        if not self.results:
            self.analyze()
        
        print("=" * 70)
        print("üìä COMPREHENSIVE MULTIVARIATE DRIFT REPORT")
        print("=" * 70)
        
        print("\n1Ô∏è‚É£ CORRELATION DRIFT:")
        print("-" * 40)
        corr_df = self.results['correlation']
        significant_pairs = corr_df[corr_df['significant_change']]
        if len(significant_pairs) > 0:
            print("   ‚ö†Ô∏è Significant correlation changes detected:")
            for _, row in significant_pairs.iterrows():
                print(f"      {row['feature_pair']}: {row['ref_correlation']:.3f} ‚Üí {row['cur_correlation']:.3f}")
        else:
            print("   ‚úì No significant correlation changes")
        
        print("\n2Ô∏è‚É£ PCA STRUCTURE ANALYSIS:")
        print("-" * 40)
        pca_res = self.results['pca']
        for i, sim in enumerate(pca_res['component_similarities']):
            status = "‚úì" if sim > 0.9 else "‚ö†Ô∏è Changed"
            print(f"   PC{i+1} similarity: {sim:.4f} {status}")
        print(f"   Reconstruction error ratio: {pca_res['reconstruction_error_ratio']:.4f}x")
        
        print("\n3Ô∏è‚É£ MAHALANOBIS DISTANCE ANALYSIS:")
        print("-" * 40)
        maha_res = self.results['mahalanobis']
        print(f"   Mean distance ratio: {maha_res['cur_mean_distance']/maha_res['ref_mean_distance']:.4f}x")
        print(f"   Outlier % (Reference): {maha_res['ref_outlier_pct']:.2f}%")
        print(f"   Outlier % (Current): {maha_res['cur_outlier_pct']:.2f}%")
        print(f"   KS Test p-value: {maha_res['ks_pvalue']:.4f}")
        
        print("\n" + "=" * 70)
        print("üìã OVERALL ASSESSMENT")
        print("=" * 70)
        summary = self.results['summary']
        print(f"   Drift Indicators Triggered: {summary['drift_count']}/3")
        for indicator, triggered in summary['drift_indicators'].items():
            status = "‚ö†Ô∏è" if triggered else "‚úì"
            print(f"      {status} {indicator}: {'Yes' if triggered else 'No'}")
        
        severity_emoji = {'LOW': 'üü¢', 'MEDIUM': 'üü°', 'HIGH': 'üî¥'}
        print(f"\n   üéØ OVERALL MULTIVARIATE DRIFT: {severity_emoji[summary['overall_assessment']]} {summary['overall_assessment']}")

# ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô
detector = MultivariateriftDetector(ref_multi, cur_multi)
detector.analyze()
detector.print_report()

# %% [markdown]
# ## ‡∏™‡∏£‡∏∏‡∏õ LAB 3
#
# ### ‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ:
# 1. **Multivariate Drift**: ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡∏∂‡πâ‡∏ô‡πÄ‡∏°‡∏∑‡πà‡∏≠ relationship ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á features ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô
# 2. **Correlation Analysis**: ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡∏≠‡∏á pairwise correlations
# 3. **PCA Analysis**: ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡∏≠‡∏á multivariate structure
# 4. **Mahalanobis Distance**: ‡∏ß‡∏±‡∏î dataset-level drift
#
# ### Key Insights:
# - Univariate methods ‡∏≠‡∏≤‡∏à‡∏û‡∏•‡∏≤‡∏î multivariate drift
# - ‡πÉ‡∏ä‡πâ‡∏´‡∏•‡∏≤‡∏¢ methods ‡∏£‡πà‡∏ß‡∏°‡∏Å‡∏±‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°
# - Monitor ‡∏ó‡∏±‡πâ‡∏á individual features ‡πÅ‡∏•‡∏∞ relationships

# %%
print("=" * 60)
print("‚úÖ LAB 3 COMPLETED!")
print("=" * 60)
print("""
üìö Key Takeaways:
1. Multivariate drift ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏î‡πâ‡∏ß‡∏¢ univariate methods ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ
2. Correlation drift analysis ‡πÉ‡∏ä‡πâ Fisher's Z transformation
3. PCA structure comparison ‡∏î‡∏π component similarities
4. Mahalanobis distance ‡∏ß‡∏±‡∏î overall distribution shift

üîú Next: LAB 4 - Drift Detection in Production Simulation
""")
```

---

## LAB 4: Drift Detection in Production Simulation

```python
# %% [markdown]
# # LAB 4: Drift Detection in Production Simulation
# ## ‡∏Å‡∏≤‡∏£‡∏à‡∏≥‡∏•‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö Drift ‡πÉ‡∏ô Production Environment
#
# ### ‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ:
# 1. ‡∏™‡∏£‡πâ‡∏≤‡∏á simulated data stream ‡∏ó‡∏µ‡πà‡∏°‡∏µ gradual drift
# 2. ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö sudden vs gradual drift
# 3. Implement sliding window monitoring
#
# ### ‡∏ó‡∏§‡∏©‡∏é‡∏µ:
# ‡πÉ‡∏ô production environment:
# - ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏°‡∏≤‡πÄ‡∏õ‡πá‡∏ô stream ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà batch
# - Drift ‡∏≠‡∏≤‡∏à‡πÄ‡∏Å‡∏¥‡∏î‡πÅ‡∏ö‡∏ö sudden ‡∏´‡∏£‡∏∑‡∏≠ gradual
# - ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ monitoring strategy ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°

# %% [markdown]
# ## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 1: ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° Environment

# %%
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy import stats
from collections import deque
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')

np.random.seed(42)
plt.rcParams['figure.figsize'] = (14, 6)

print("‚úÖ Libraries imported successfully!")

# %% [markdown]
# ## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 2: ‡∏™‡∏£‡πâ‡∏≤‡∏á Data Stream Simulator
#
# ‡∏à‡∏≥‡∏•‡∏≠‡∏á data stream ‡∏ó‡∏µ‡πà‡∏°‡∏µ drift ‡∏´‡∏•‡∏≤‡∏¢‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö

# %%
class DataStreamSimulator:
    """
    Simulator ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡∏£‡πâ‡∏≤‡∏á data stream ‡∏ó‡∏µ‡πà‡∏°‡∏µ drift patterns ‡∏ï‡πà‡∏≤‡∏á‡πÜ
    
    Drift Types:
    - sudden: ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏ó‡∏±‡∏ô‡∏ó‡∏µ‡∏ó‡∏µ‡πà‡∏à‡∏∏‡∏î‡πÉ‡∏î‡∏à‡∏∏‡∏î‡∏´‡∏ô‡∏∂‡πà‡∏á
    - gradual: ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏ä‡πâ‡∏≤‡πÜ ‡∏ï‡∏≤‡∏°‡πÄ‡∏ß‡∏•‡∏≤
    - incremental: ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡∏±‡πâ‡∏ô‡∏ö‡∏±‡∏ô‡πÑ‡∏î
    - seasonal: ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏ï‡∏≤‡∏° pattern ‡∏ã‡πâ‡∏≥
    - no_drift: ‡πÑ‡∏°‡πà‡∏°‡∏µ drift
    """
    
    def __init__(self, base_mean=50, base_std=10, random_seed=42):
        self.base_mean = base_mean
        self.base_std = base_std
        self.random_seed = random_seed
        np.random.seed(random_seed)
    
    def generate_stream(self, n_samples, drift_type='no_drift', drift_params=None):
        """
        ‡∏™‡∏£‡πâ‡∏≤‡∏á data stream
        
        Parameters:
        -----------
        n_samples : int - ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô samples
        drift_type : str - ‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Ç‡∏≠‡∏á drift
        drift_params : dict - parameters ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö drift
        """
        if drift_params is None:
            drift_params = {}
        
        data = np.zeros(n_samples)
        timestamps = [datetime(2024, 1, 1) + timedelta(hours=i) for i in range(n_samples)]
        
        if drift_type == 'no_drift':
            data = np.random.normal(self.base_mean, self.base_std, n_samples)
        
        elif drift_type == 'sudden':
            drift_point = drift_params.get('drift_point', n_samples // 2)
            new_mean = drift_params.get('new_mean', self.base_mean + 2 * self.base_std)
            
            data[:drift_point] = np.random.normal(self.base_mean, self.base_std, drift_point)
            data[drift_point:] = np.random.normal(new_mean, self.base_std, n_samples - drift_point)
        
        elif drift_type == 'gradual':
            drift_start = drift_params.get('drift_start', n_samples // 4)
            drift_end = drift_params.get('drift_end', 3 * n_samples // 4)
            final_mean = drift_params.get('final_mean', self.base_mean + 2 * self.base_std)
            
            for i in range(n_samples):
                if i < drift_start:
                    current_mean = self.base_mean
                elif i > drift_end:
                    current_mean = final_mean
                else:
                    # Linear interpolation
                    progress = (i - drift_start) / (drift_end - drift_start)
                    current_mean = self.base_mean + progress * (final_mean - self.base_mean)
                
                data[i] = np.random.normal(current_mean, self.base_std)
        
        elif drift_type == 'incremental':
            step_size = drift_params.get('step_size', n_samples // 5)
            step_increase = drift_params.get('step_increase', self.base_std * 0.5)
            
            for i in range(n_samples):
                step = i // step_size
                current_mean = self.base_mean + step * step_increase
                data[i] = np.random.normal(current_mean, self.base_std)
        
        elif drift_type == 'seasonal':
            period = drift_params.get('period', 168)  # 1 week in hours
            amplitude = drift_params.get('amplitude', self.base_std)
            
            for i in range(n_samples):
                seasonal_effect = amplitude * np.sin(2 * np.pi * i / period)
                data[i] = np.random.normal(self.base_mean + seasonal_effect, self.base_std)
        
        return pd.DataFrame({
            'timestamp': timestamps,
            'value': data,
            'index': range(n_samples)
        })

# ‡∏™‡∏£‡πâ‡∏≤‡∏á simulator
simulator = DataStreamSimulator(base_mean=50, base_std=10)

print("‚úÖ DataStreamSimulator created")

# %%
# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á drift patterns
fig, axes = plt.subplots(2, 3, figsize=(15, 8))
axes = axes.flatten()

drift_types = ['no_drift', 'sudden', 'gradual', 'incremental', 'seasonal']
drift_params_list = [
    {},
    {'drift_point': 500, 'new_mean': 70},
    {'drift_start': 200, 'drift_end': 800, 'final_mean': 70},
    {'step_size': 200, 'step_increase': 5},
    {'period': 200, 'amplitude': 15}
]

streams = {}
for i, (dtype, params) in enumerate(zip(drift_types, drift_params_list)):
    stream = simulator.generate_stream(1000, drift_type=dtype, drift_params=params)
    streams[dtype] = stream
    
    ax = axes[i]
    ax.plot(stream['index'], stream['value'], alpha=0.7, linewidth=0.5)
    
    # Add rolling mean
    rolling_mean = stream['value'].rolling(window=50).mean()
    ax.plot(stream['index'], rolling_mean, 'r-', linewidth=2, label='Rolling Mean (50)')
    
    ax.axhline(50, color='green', linestyle='--', alpha=0.5, label='Original Mean')
    ax.set_title(f'{dtype.replace("_", " ").title()}')
    ax.set_xlabel('Time Index')
    ax.set_ylabel('Value')
    ax.legend(fontsize=8)

# Hide empty subplot
axes[-1].set_visible(False)

plt.suptitle('Different Types of Data Drift', fontsize=14, fontweight='bold')
plt.tight_layout()
plt.savefig('drift_types.png', dpi=150, bbox_inches='tight')
plt.show()

# %% [markdown]
# ## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 3: Sliding Window Drift Detector
#
# Implement sliding window ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö real-time drift detection

# %%
class SlidingWindowDriftDetector:
    """
    Drift detector ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ sliding window approach
    
    ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö:
    - Real-time monitoring
    - Streaming data
    - Memory-efficient processing
    """
    
    def __init__(self, reference_window_size=200, test_window_size=100, 
                 ks_threshold=0.05, psi_threshold=0.1):
        """
        Parameters:
        -----------
        reference_window_size : int - ‡∏Ç‡∏ô‡∏≤‡∏î‡∏Ç‡∏≠‡∏á reference window
        test_window_size : int - ‡∏Ç‡∏ô‡∏≤‡∏î‡∏Ç‡∏≠‡∏á test window
        ks_threshold : float - threshold ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö KS test p-value
        psi_threshold : float - threshold ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö PSI
        """
        self.reference_window_size = reference_window_size
        self.test_window_size = test_window_size
        self.ks_threshold = ks_threshold
        self.psi_threshold = psi_threshold
        
        # ‡πÉ‡∏ä‡πâ deque ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö efficient sliding window
        self.reference_buffer = deque(maxlen=reference_window_size)
        self.test_buffer = deque(maxlen=test_window_size)
        
        self.history = []
        self.drift_points = []
        self.is_initialized = False
    
    def calculate_psi(self, reference, test, bins=10):
        """‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì PSI"""
        breakpoints = np.percentile(reference, np.linspace(0, 100, bins + 1))
        breakpoints = np.unique(breakpoints)
        
        ref_counts, _ = np.histogram(reference, bins=breakpoints)
        test_counts, _ = np.histogram(test, bins=breakpoints)
        
        eps = 1e-6
        ref_props = ref_counts / len(reference) + eps
        test_props = test_counts / len(test) + eps
        
        psi = np.sum((test_props - ref_props) * np.log(test_props / ref_props))
        return psi
    
    def update(self, value, timestamp=None):
        """
        Update detector ‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡πà‡∏≤‡πÉ‡∏´‡∏°‡πà
        
        Returns:
        --------
        dict : detection result
        """
        # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡πà‡∏≤‡πÉ‡∏ô buffers
        if not self.is_initialized:
            self.reference_buffer.append(value)
            if len(self.reference_buffer) >= self.reference_window_size:
                self.is_initialized = True
            return {'drift_detected': False, 'status': 'initializing'}
        
        self.test_buffer.append(value)
        
        # ‡∏£‡∏≠‡∏à‡∏ô‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏≠‡πÉ‡∏ô test buffer
        if len(self.test_buffer) < self.test_window_size:
            return {'drift_detected': False, 'status': 'collecting'}
        
        # ‡∏ó‡∏≥ drift detection
        ref_array = np.array(self.reference_buffer)
        test_array = np.array(self.test_buffer)
        
        # KS Test
        ks_stat, ks_pval = stats.ks_2samp(ref_array, test_array)
        
        # PSI
        psi = self.calculate_psi(ref_array, test_array)
        
        # Detection logic
        ks_drift = ks_pval < self.ks_threshold
        psi_drift = psi > self.psi_threshold
        drift_detected = ks_drift or psi_drift
        
        result = {
            'timestamp': timestamp,
            'drift_detected': drift_detected,
            'ks_statistic': ks_stat,
            'ks_pvalue': ks_pval,
            'psi': psi,
            'ref_mean': np.mean(ref_array),
            'test_mean': np.mean(test_array),
            'status': 'DRIFT' if drift_detected else 'normal'
        }
        
        self.history.append(result)
        
        if drift_detected:
            self.drift_points.append(len(self.history) - 1)
        
        return result
    
    def reset_reference(self):
        """Reset reference window ‡∏î‡πâ‡∏ß‡∏¢ test window ‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô"""
        self.reference_buffer.clear()
        for val in self.test_buffer:
            self.reference_buffer.append(val)
        self.test_buffer.clear()
        print("üìã Reference window reset with current data")
    
    def get_history_df(self):
        """‡πÅ‡∏õ‡∏•‡∏á history ‡πÄ‡∏õ‡πá‡∏ô DataFrame"""
        return pd.DataFrame(self.history)

# %%
# ‡∏ó‡∏î‡∏™‡∏≠‡∏ö SlidingWindowDriftDetector ‡∏Å‡∏±‡∏ö sudden drift
print("=" * 60)
print("üîç Testing Sliding Window Detector with SUDDEN DRIFT")
print("=" * 60)

detector = SlidingWindowDriftDetector(
    reference_window_size=200,
    test_window_size=100,
    ks_threshold=0.05,
    psi_threshold=0.1
)

sudden_stream = streams['sudden']

for idx, row in sudden_stream.iterrows():
    result = detector.update(row['value'], timestamp=row['timestamp'])
    
    # Print only drift events
    if result.get('drift_detected', False):
        print(f"‚ö†Ô∏è DRIFT at index {idx}: KS p-value={result['ks_pvalue']:.4f}, PSI={result['psi']:.4f}")

print(f"\nüìä Total drift points detected: {len(detector.drift_points)}")

# %%
# Visualize detection results
history_df = detector.get_history_df()

fig, axes = plt.subplots(3, 1, figsize=(14, 10), sharex=True)

# Plot 1: Original data with drift points
ax1 = axes[0]
ax1.plot(sudden_stream['index'], sudden_stream['value'], alpha=0.5, label='Data')
ax1.plot(sudden_stream['index'], sudden_stream['value'].rolling(50).mean(), 
         'b-', linewidth=2, label='Rolling Mean')

# Mark drift points
for dp in detector.drift_points:
    ax1.axvline(dp + 200 + 100, color='red', alpha=0.5, linestyle='--')  # Offset for initialization

ax1.axvline(500, color='green', linestyle='--', linewidth=2, label='Actual Drift Point')
ax1.set_ylabel('Value')
ax1.set_title('Data Stream with Detected Drift Points')
ax1.legend()

# Plot 2: KS p-value over time
ax2 = axes[1]
ax2.plot(history_df.index + 200 + 100, history_df['ks_pvalue'], 'b-', linewidth=1)
ax2.axhline(0.05, color='red', linestyle='--', label='Threshold (0.05)')
ax2.set_ylabel('KS p-value')
ax2.set_title('KS Test p-value Over Time')
ax2.legend()
ax2.set_yscale('log')

# Plot 3: PSI over time
ax3 = axes[2]
ax3.plot(history_df.index + 200 + 100, history_df['psi'], 'g-', linewidth=1)
ax3.axhline(0.1, color='red', linestyle='--', label='Threshold (0.1)')
ax3.set_ylabel('PSI')
ax3.set_xlabel('Time Index')
ax3.set_title('PSI Over Time')
ax3.legend()

plt.tight_layout()
plt.savefig('sliding_window_detection.png', dpi=150, bbox_inches='tight')
plt.show()

# %% [markdown]
# ## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 4: Detecting Gradual Drift
#
# Gradual drift ‡∏¢‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤ sudden drift ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏ä‡πâ‡∏≤‡πÜ

# %%
print("=" * 60)
print("üîç Testing Sliding Window Detector with GRADUAL DRIFT")
print("=" * 60)

detector_gradual = SlidingWindowDriftDetector(
    reference_window_size=200,
    test_window_size=100
)

gradual_stream = streams['gradual']

for idx, row in gradual_stream.iterrows():
    result = detector_gradual.update(row['value'], timestamp=row['timestamp'])

history_gradual = detector_gradual.get_history_df()

print(f"\nüìä Total drift points detected: {len(detector_gradual.drift_points)}")
print(f"   First detection at index: {detector_gradual.drift_points[0] + 300 if detector_gradual.drift_points else 'N/A'}")

# %%
# Compare detection of sudden vs gradual drift
fig, axes = plt.subplots(2, 2, figsize=(14, 10))

# Sudden drift
ax1 = axes[0, 0]
ax1.plot(sudden_stream['index'], sudden_stream['value'], alpha=0.3)
ax1.plot(sudden_stream['index'], sudden_stream['value'].rolling(50).mean(), 'b-', linewidth=2)
for dp in detector.drift_points:
    ax1.axvline(dp + 300, color='red', alpha=0.3)
ax1.axvline(500, color='green', linestyle='--', linewidth=2, label='Actual Drift')
ax1.set_title('Sudden Drift Detection')
ax1.legend()

ax2 = axes[0, 1]
history_df = detector.get_history_df()
ax2.plot(history_df.index + 300, history_df['psi'], 'g-')
ax2.axhline(0.1, color='red', linestyle='--')
ax2.set_title('Sudden Drift: PSI')

# Gradual drift
ax3 = axes[1, 0]
ax3.plot(gradual_stream['index'], gradual_stream['value'], alpha=0.3)
ax3.plot(gradual_stream['index'], gradual_stream['value'].rolling(50).mean(), 'b-', linewidth=2)
for dp in detector_gradual.drift_points:
    ax3.axvline(dp + 300, color='red', alpha=0.3)
ax3.axvspan(200, 800, alpha=0.2, color='green', label='Drift Period')
ax3.set_title('Gradual Drift Detection')
ax3.legend()

ax4 = axes[1, 1]
ax4.plot(history_gradual.index + 300, history_gradual['psi'], 'g-')
ax4.axhline(0.1, color='red', linestyle='--')
ax4.set_title('Gradual Drift: PSI')

plt.tight_layout()
plt.savefig('sudden_vs_gradual_detection.png', dpi=150, bbox_inches='tight')
plt.show()

print("\nüí° Observation:")
print("   - Sudden drift: ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡πÑ‡∏î‡πâ‡πÄ‡∏£‡πá‡∏ß‡πÅ‡∏•‡∏∞‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô")
print("   - Gradual drift: ‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤‡∏ô‡∏≤‡∏ô‡∏Å‡∏ß‡πà‡∏≤‡∏à‡∏∞ detect ‡πÑ‡∏î‡πâ")

# %% [markdown]
# ## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 5: Adaptive Reference Window
#
# ‡∏õ‡∏£‡∏±‡∏ö reference window ‡πÄ‡∏û‡∏∑‡πà‡∏≠ handle gradual drift

# %%
class AdaptiveDriftDetector:
    """
    Drift detector ‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏±‡∏ö reference window ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥
    
    Features:
    - ‡∏õ‡∏£‡∏±‡∏ö reference ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö drift
    - ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô false alarms ‡∏à‡∏≤‡∏Å temporary spikes
    - Track both short-term ‡πÅ‡∏•‡∏∞ long-term drift
    """
    
    def __init__(self, reference_window_size=200, test_window_size=50,
                 confirmation_window=3, psi_threshold=0.1):
        """
        Parameters:
        -----------
        confirmation_window : int - ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ï‡∏¥‡∏î‡∏ï‡πà‡∏≠‡∏Å‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á detect ‡∏Å‡πà‡∏≠‡∏ô‡∏¢‡∏∑‡∏ô‡∏¢‡∏±‡∏ô drift
        """
        self.reference_window_size = reference_window_size
        self.test_window_size = test_window_size
        self.confirmation_window = confirmation_window
        self.psi_threshold = psi_threshold
        
        self.reference_buffer = deque(maxlen=reference_window_size)
        self.test_buffer = deque(maxlen=test_window_size)
        
        self.consecutive_drift_count = 0
        self.confirmed_drifts = []
        self.history = []
        self.adaptation_count = 0
    
    def calculate_psi(self, reference, test, bins=10):
        """‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì PSI"""
        breakpoints = np.percentile(reference, np.linspace(0, 100, bins + 1))
        breakpoints = np.unique(breakpoints)
        
        ref_counts, _ = np.histogram(reference, bins=breakpoints)
        test_counts, _ = np.histogram(test, bins=breakpoints)
        
        eps = 1e-6
        ref_props = ref_counts / len(reference) + eps
        test_props = test_counts / len(test) + eps
        
        psi = np.sum((test_props - ref_props) * np.log(test_props / ref_props))
        return psi
    
    def adapt_reference(self):
        """‡∏õ‡∏£‡∏±‡∏ö reference window"""
        # ‡∏ú‡∏™‡∏° old reference ‡∏Å‡∏±‡∏ö new data
        old_weight = 0.5
        old_ref = list(self.reference_buffer)
        new_data = list(self.test_buffer)
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á new reference
        self.reference_buffer.clear()
        
        # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ö‡∏≤‡∏á‡∏™‡πà‡∏ß‡∏ô‡∏à‡∏≤‡∏Å old reference
        n_old = int(len(old_ref) * old_weight)
        for val in old_ref[-n_old:]:
            self.reference_buffer.append(val)
        
        # ‡πÄ‡∏û‡∏¥‡πà‡∏° new data
        for val in new_data:
            self.reference_buffer.append(val)
        
        self.adaptation_count += 1
        self.consecutive_drift_count = 0
    
    def update(self, value, timestamp=None):
        """Update detector"""
        # Initialize
        if len(self.reference_buffer) < self.reference_window_size:
            self.reference_buffer.append(value)
            return {'status': 'initializing', 'drift_detected': False}
        
        self.test_buffer.append(value)
        
        if len(self.test_buffer) < self.test_window_size:
            return {'status': 'collecting', 'drift_detected': False}
        
        # Drift detection
        ref_array = np.array(self.reference_buffer)
        test_array = np.array(self.test_buffer)
        
        psi = self.calculate_psi(ref_array, test_array)
        potential_drift = psi > self.psi_threshold
        
        if potential_drift:
            self.consecutive_drift_count += 1
        else:
            self.consecutive_drift_count = 0
        
        # Confirmed drift (multiple consecutive detections)
        confirmed = self.consecutive_drift_count >= self.confirmation_window
        
        result = {
            'timestamp': timestamp,
            'psi': psi,
            'potential_drift': potential_drift,
            'confirmed_drift': confirmed,
            'consecutive_count': self.consecutive_drift_count,
            'adaptation_count': self.adaptation_count,
            'ref_mean': np.mean(ref_array),
            'test_mean': np.mean(test_array)
        }
        
        self.history.append(result)
        
        # Adapt if confirmed
        if confirmed:
            self.confirmed_drifts.append(len(self.history) - 1)
            self.adapt_reference()
            result['adapted'] = True
        
        return result
    
    def get_history_df(self):
        return pd.DataFrame(self.history)

# %%
# ‡∏ó‡∏î‡∏™‡∏≠‡∏ö Adaptive Detector
print("=" * 60)
print("üîç Testing ADAPTIVE Drift Detector with GRADUAL DRIFT")
print("=" * 60)

adaptive_detector = AdaptiveDriftDetector(
    reference_window_size=200,
    test_window_size=50,
    confirmation_window=3,
    psi_threshold=0.1
)

for idx, row in gradual_stream.iterrows():
    result = adaptive_detector.update(row['value'], timestamp=row['timestamp'])
    
    if result.get('confirmed_drift', False):
        print(f"‚úÖ CONFIRMED DRIFT at index {idx}: PSI={result['psi']:.4f}, Adapted!")

print(f"\nüìä Total confirmed drifts: {len(adaptive_detector.confirmed_drifts)}")
print(f"üìä Total adaptations: {adaptive_detector.adaptation_count}")

# %%
# Visualize adaptive detection
adaptive_history = adaptive_detector.get_history_df()

fig, axes = plt.subplots(3, 1, figsize=(14, 10), sharex=True)

# Plot 1: Data with adaptations
ax1 = axes[0]
ax1.plot(gradual_stream['index'], gradual_stream['value'], alpha=0.3)
ax1.plot(gradual_stream['index'], gradual_stream['value'].rolling(50).mean(), 'b-', linewidth=2)

offset = 200 + 50  # initialization + test window
for drift_idx in adaptive_detector.confirmed_drifts:
    ax1.axvline(drift_idx + offset, color='red', linestyle='--', alpha=0.7, linewidth=2)

ax1.set_title('Gradual Drift with Adaptive Detection')
ax1.set_ylabel('Value')

# Plot 2: PSI with threshold
ax2 = axes[1]
ax2.plot(adaptive_history.index + offset, adaptive_history['psi'], 'g-')
ax2.axhline(0.1, color='red', linestyle='--', label='Threshold')
ax2.set_ylabel('PSI')
ax2.legend()

# Plot 3: Reference vs Test mean
ax3 = axes[2]
ax3.plot(adaptive_history.index + offset, adaptive_history['ref_mean'], 'b-', label='Reference Mean')
ax3.plot(adaptive_history.index + offset, adaptive_history['test_mean'], 'r-', label='Test Mean')
ax3.set_xlabel('Time Index')
ax3.set_ylabel('Mean')
ax3.legend()

plt.tight_layout()
plt.savefig('adaptive_detection.png', dpi=150, bbox_inches='tight')
plt.show()

print("\nüí° Adaptive detector ‡∏õ‡∏£‡∏±‡∏ö reference window ‡πÄ‡∏°‡∏∑‡πà‡∏≠ detect drift")
print("   ‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏° gradual drift ‡πÑ‡∏î‡πâ‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á")

# %% [markdown]
# ## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 6: Page-Hinkley Test for Change Detection
#
# Algorithm ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö detect mean shift ‡πÉ‡∏ô streaming data

# %%
class PageHinkleyDetector:
    """
    Page-Hinkley test ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö detect mean shift
    
    ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö:
    - Streaming data
    - Detect upward ‡∏´‡∏£‡∏∑‡∏≠ downward shifts
    - Low memory footprint
    """
    
    def __init__(self, delta=0.005, lambda_=50, alpha=0.9999):
        """
        Parameters:
        -----------
        delta : float - magnitude ‡∏ó‡∏µ‡πà‡∏¢‡∏≠‡∏°‡∏£‡∏±‡∏ö‡πÑ‡∏î‡πâ‡∏Ç‡∏≠‡∏á change
        lambda_ : float - detection threshold
        alpha : float - forgetting factor ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö mean estimation
        """
        self.delta = delta
        self.lambda_ = lambda_
        self.alpha = alpha
        
        self.mean = 0
        self.sum = 0
        self.min_sum = float('inf')
        self.max_sum = float('-inf')
        
        self.n_samples = 0
        self.history = []
        self.drift_points = []
    
    def update(self, value, timestamp=None):
        """
        Update detector ‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡πà‡∏≤‡πÉ‡∏´‡∏°‡πà
        """
        self.n_samples += 1
        
        # Update mean
        if self.n_samples == 1:
            self.mean = value
        else:
            self.mean = self.alpha * self.mean + (1 - self.alpha) * value
        
        # Update cumulative sum
        self.sum += value - self.mean - self.delta
        
        # Update min/max
        self.min_sum = min(self.min_sum, self.sum)
        self.max_sum = max(self.max_sum, self.sum)
        
        # Calculate test statistics
        ph_positive = self.sum - self.min_sum  # Detect upward shift
        ph_negative = self.max_sum - self.sum  # Detect downward shift
        
        # Detection
        drift_up = ph_positive > self.lambda_
        drift_down = ph_negative > self.lambda_
        drift_detected = drift_up or drift_down
        
        result = {
            'timestamp': timestamp,
            'value': value,
            'mean': self.mean,
            'sum': self.sum,
            'ph_positive': ph_positive,
            'ph_negative': ph_negative,
            'drift_detected': drift_detected,
            'drift_direction': 'up' if drift_up else ('down' if drift_down else None)
        }
        
        self.history.append(result)
        
        if drift_detected:
            self.drift_points.append(self.n_samples - 1)
            self.reset()
        
        return result
    
    def reset(self):
        """Reset statistics ‡∏´‡∏•‡∏±‡∏á detect drift"""
        self.sum = 0
        self.min_sum = float('inf')
        self.max_sum = float('-inf')
    
    def get_history_df(self):
        return pd.DataFrame(self.history)

# %%
# ‡∏ó‡∏î‡∏™‡∏≠‡∏ö Page-Hinkley
print("=" * 60)
print("üîç Testing PAGE-HINKLEY Detector")
print("=" * 60)

ph_detector = PageHinkleyDetector(delta=0.01, lambda_=30)

# ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏±‡∏ö sudden drift
for idx, row in sudden_stream.iterrows():
    result = ph_detector.update(row['value'], timestamp=row['timestamp'])
    
    if result['drift_detected']:
        print(f"‚ö†Ô∏è DRIFT at index {idx}: Direction = {result['drift_direction']}")

print(f"\nüìä Total drifts detected: {len(ph_detector.drift_points)}")

# %%
# Visualize Page-Hinkley results
ph_history = ph_detector.get_history_df()

fig, axes = plt.subplots(3, 1, figsize=(14, 10), sharex=True)

# Plot 1: Data
ax1 = axes[0]
ax1.plot(sudden_stream['index'], sudden_stream['value'], alpha=0.3)
ax1.plot(ph_history.index, ph_history['mean'], 'r-', linewidth=2, label='Estimated Mean')
for dp in ph_detector.drift_points:
    ax1.axvline(dp, color='red', linestyle='--', alpha=0.7)
ax1.set_title('Page-Hinkley Detection')
ax1.set_ylabel('Value')
ax1.legend()

# Plot 2: PH Statistics
ax2 = axes[1]
ax2.plot(ph_history.index, ph_history['ph_positive'], 'g-', label='PH+ (upward)')
ax2.plot(ph_history.index, ph_history['ph_negative'], 'b-', label='PH- (downward)')
ax2.axhline(30, color='red', linestyle='--', label='Threshold')
ax2.set_ylabel('PH Statistics')
ax2.legend()

# Plot 3: Cumulative Sum
ax3 = axes[2]
ax3.plot(ph_history.index, ph_history['sum'], 'purple')
ax3.set_xlabel('Time Index')
ax3.set_ylabel('Cumulative Sum')

plt.tight_layout()
plt.savefig('page_hinkley_detection.png', dpi=150, bbox_inches='tight')
plt.show()

# %% [markdown]
# ## ‡∏™‡∏£‡∏∏‡∏õ LAB 4
#
# ### ‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ:
# 1. **Data Stream Simulation**: ‡∏™‡∏£‡πâ‡∏≤‡∏á streams ‡∏ó‡∏µ‡πà‡∏°‡∏µ drift patterns ‡∏ï‡πà‡∏≤‡∏á‡πÜ
# 2. **Sliding Window**: ‡∏ß‡∏¥‡∏ò‡∏µ‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö streaming drift detection
# 3. **Adaptive Detection**: ‡∏õ‡∏£‡∏±‡∏ö reference window ‡πÄ‡∏°‡∏∑‡πà‡∏≠ detect drift
# 4. **Page-Hinkley**: Algorithm ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö mean shift detection
#
# ### Comparison:
# | Method | Pros | Cons | Best For |
# |--------|------|------|----------|
# | Sliding Window | Simple, intuitive | Fixed reference | Sudden drift |
# | Adaptive | Handles gradual drift | More complex | Production |
# | Page-Hinkley | Low memory, fast | Mean shift only | Real-time |

# %%
print("=" * 60)
print("‚úÖ LAB 4 COMPLETED!")
print("=" * 60)
print("""
üìö Key Takeaways:
1. Different drift types require different detection strategies
2. Sliding window is fundamental for streaming detection
3. Adaptive reference helps with gradual drift
4. Page-Hinkley is efficient for mean shift detection

üîú Next: LAB 5 - Custom Metrics & Drift Thresholds
""")
```

---

## LAB 5: Custom Metrics & Drift Thresholds

```python
# %% [markdown]
# # LAB 5: Custom Metrics & Drift Thresholds
# ## ‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á Custom Drift Metrics ‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏±‡∏ö Thresholds
#
# ### ‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ:
# 1. ‡∏™‡∏£‡πâ‡∏≤‡∏á custom drift metrics ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö domain
# 2. ‡∏õ‡∏£‡∏±‡∏ö threshold ‡∏ï‡∏≤‡∏° business requirements
# 3. Handle false positives/negatives ‡πÉ‡∏ô drift detection
#
# ### ‡∏ó‡∏§‡∏©‡∏é‡∏µ:
# Default thresholds ‡∏≠‡∏≤‡∏à‡πÑ‡∏°‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö‡∏ó‡∏∏‡∏Å use case:
# - ‡∏ö‡∏≤‡∏á domain ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ sensitivity ‡∏™‡∏π‡∏á
# - ‡∏ö‡∏≤‡∏á domain ‡∏¢‡∏≠‡∏°‡∏£‡∏±‡∏ö drift ‡πÑ‡∏î‡πâ‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏´‡∏ô‡∏∂‡πà‡∏á
# - Cost of false positive vs false negative ‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô

# %% [markdown]
# ## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 1: ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° Environment

# %%
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy import stats
from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score
import warnings
warnings.filterwarnings('ignore')

np.random.seed(42)
plt.rcParams['figure.figsize'] = (12, 6)

print("‚úÖ Libraries imported successfully!")

# %% [markdown]
# ## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 2: ‡∏™‡∏£‡πâ‡∏≤‡∏á Dataset ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Threshold Tuning
#
# ‡∏™‡∏£‡πâ‡∏≤‡∏á labeled dataset ‡∏ó‡∏µ‡πà‡∏£‡∏π‡πâ‡∏ß‡πà‡∏≤‡∏°‡∏µ drift ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà

# %%
def create_labeled_drift_dataset(n_scenarios=100):
    """
    ‡∏™‡∏£‡πâ‡∏≤‡∏á dataset ‡∏û‡∏£‡πâ‡∏≠‡∏° ground truth labels
    
    Returns:
    --------
    list of dicts: ‡πÅ‡∏ï‡πà‡∏•‡∏∞ scenario ‡∏°‡∏µ reference, current, ‡πÅ‡∏•‡∏∞ label
    """
    scenarios = []
    
    for i in range(n_scenarios):
        np.random.seed(i)
        n_samples = 500
        
        # Reference data
        ref_mean = 50
        ref_std = 10
        reference = np.random.normal(ref_mean, ref_std, n_samples)
        
        # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏ß‡πà‡∏≤‡∏°‡∏µ drift ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        has_drift = i < n_scenarios // 2  # 50% ‡∏°‡∏µ drift
        
        if has_drift:
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á drift ‡πÉ‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ï‡πà‡∏≤‡∏á‡πÜ
            drift_level = (i % 5 + 1) * 0.5  # 0.5, 1.0, 1.5, 2.0, 2.5 std
            current_mean = ref_mean + drift_level * ref_std
            drift_magnitude = drift_level
        else:
            # ‡πÑ‡∏°‡πà‡∏°‡∏µ drift ‡πÅ‡∏ï‡πà‡∏°‡∏µ noise ‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢
            current_mean = ref_mean + np.random.uniform(-0.1, 0.1) * ref_std
            drift_magnitude = 0
        
        current = np.random.normal(current_mean, ref_std, n_samples)
        
        scenarios.append({
            'id': i,
            'reference': reference,
            'current': current,
            'has_drift': has_drift,
            'drift_magnitude': drift_magnitude,
            'ref_mean': ref_mean,
            'current_mean': current_mean
        })
    
    return scenarios

# ‡∏™‡∏£‡πâ‡∏≤‡∏á dataset
scenarios = create_labeled_drift_dataset(100)

print(f"üìä Created {len(scenarios)} scenarios")
print(f"   With drift: {sum(1 for s in scenarios if s['has_drift'])}")
print(f"   Without drift: {sum(1 for s in scenarios if not s['has_drift'])}")

# %%
# Visualize drift magnitude distribution
drift_mags = [s['drift_magnitude'] for s in scenarios if s['has_drift']]
plt.figure(figsize=(10, 4))
plt.hist(drift_mags, bins=10, edgecolor='black', alpha=0.7)
plt.xlabel('Drift Magnitude (in std)')
plt.ylabel('Count')
plt.title('Distribution of Drift Magnitudes in Scenarios with Drift')
plt.show()

# %% [markdown]
# ## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 3: ‡∏™‡∏£‡πâ‡∏≤‡∏á Custom Drift Metrics

# %%
class CustomDriftMetrics:
    """
    Class ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì custom drift metrics
    """
    
    @staticmethod
    def psi(reference, current, bins=10):
        """Population Stability Index"""
        breakpoints = np.percentile(reference, np.linspace(0, 100, bins + 1))
        breakpoints = np.unique(breakpoints)
        
        ref_counts, _ = np.histogram(reference, bins=breakpoints)
        cur_counts, _ = np.histogram(current, bins=breakpoints)
        
        eps = 1e-6
        ref_props = ref_counts / len(reference) + eps
        cur_props = cur_counts / len(current) + eps
        
        return np.sum((cur_props - ref_props) * np.log(cur_props / ref_props))
    
    @staticmethod
    def normalized_wasserstein(reference, current):
        """Wasserstein distance normalized by reference std"""
        distance = stats.wasserstein_distance(reference, current)
        return distance / np.std(reference)
    
    @staticmethod
    def mean_shift_ratio(reference, current):
        """Mean shift as ratio of reference std"""
        mean_diff = abs(np.mean(current) - np.mean(reference))
        return mean_diff / np.std(reference)
    
    @staticmethod
    def std_ratio(reference, current):
        """Ratio of standard deviations"""
        return np.std(current) / np.std(reference)
    
    @staticmethod
    def percentile_shift(reference, current, percentiles=[25, 50, 75]):
        """Average shift in percentiles"""
        shifts = []
        ref_std = np.std(reference)
        
        for p in percentiles:
            ref_p = np.percentile(reference, p)
            cur_p = np.percentile(current, p)
            shifts.append(abs(cur_p - ref_p) / ref_std)
        
        return np.mean(shifts)
    
    @staticmethod
    def jensen_shannon_divergence(reference, current, bins=10):
        """Jensen-Shannon divergence"""
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á histograms
        all_data = np.concatenate([reference, current])
        bins = np.histogram_bin_edges(all_data, bins=bins)
        
        ref_hist, _ = np.histogram(reference, bins=bins, density=True)
        cur_hist, _ = np.histogram(current, bins=bins, density=True)
        
        # Normalize
        ref_hist = ref_hist / (ref_hist.sum() + 1e-10)
        cur_hist = cur_hist / (cur_hist.sum() + 1e-10)
        
        # Average distribution
        m = 0.5 * (ref_hist + cur_hist)
        
        # KL divergences
        kl_pm = np.sum(ref_hist * np.log((ref_hist + 1e-10) / (m + 1e-10)))
        kl_qm = np.sum(cur_hist * np.log((cur_hist + 1e-10) / (m + 1e-10)))
        
        return 0.5 * (kl_pm + kl_qm)
    
    @staticmethod
    def combined_score(reference, current, weights=None):
        """
        Combined drift score ‡∏à‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢ metrics
        
        Default weights: PSI=0.3, Wasserstein=0.3, Mean Shift=0.2, Percentile=0.2
        """
        if weights is None:
            weights = {
                'psi': 0.3,
                'wasserstein': 0.3,
                'mean_shift': 0.2,
                'percentile': 0.2
            }
        
        metrics = CustomDriftMetrics
        
        # Normalize each metric to 0-1 range
        psi = min(metrics.psi(reference, current), 1.0)  # Cap at 1
        wasserstein = min(metrics.normalized_wasserstein(reference, current) / 3, 1.0)  # 3 std = max
        mean_shift = min(metrics.mean_shift_ratio(reference, current) / 3, 1.0)
        percentile = min(metrics.percentile_shift(reference, current) / 3, 1.0)
        
        score = (
            weights['psi'] * psi +
            weights['wasserstein'] * wasserstein +
            weights['mean_shift'] * mean_shift +
            weights['percentile'] * percentile
        )
        
        return score

# %%
# ‡∏ó‡∏î‡∏™‡∏≠‡∏ö custom metrics
print("=" * 60)
print("üìä Testing Custom Drift Metrics")
print("=" * 60)

# ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å scenarios ‡∏ó‡∏µ‡πà‡∏°‡∏µ drift levels ‡∏ï‡πà‡∏≤‡∏á‡πÜ
test_scenarios = [s for s in scenarios if s['drift_magnitude'] in [0, 0.5, 1.0, 2.0]][:8]

metrics_results = []
for s in test_scenarios:
    result = {
        'id': s['id'],
        'has_drift': s['has_drift'],
        'magnitude': s['drift_magnitude'],
        'psi': CustomDriftMetrics.psi(s['reference'], s['current']),
        'wasserstein': CustomDriftMetrics.normalized_wasserstein(s['reference'], s['current']),
        'mean_shift': CustomDriftMetrics.mean_shift_ratio(s['reference'], s['current']),
        'percentile': CustomDriftMetrics.percentile_shift(s['reference'], s['current']),
        'js_div': CustomDriftMetrics.jensen_shannon_divergence(s['reference'], s['current']),
        'combined': CustomDriftMetrics.combined_score(s['reference'], s['current'])
    }
    metrics_results.append(result)

metrics_df = pd.DataFrame(metrics_results)
print(metrics_df.to_string(index=False))

# %% [markdown]
# ## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 4: Threshold Optimization
#
# ‡∏´‡∏≤ optimal threshold ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ labeled data

# %%
def calculate_metrics_for_threshold(scenarios, metric_func, threshold):
    """
    ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì precision, recall, f1 ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö threshold ‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î
    """
    y_true = []
    y_pred = []
    
    for s in scenarios:
        y_true.append(1 if s['has_drift'] else 0)
        
        score = metric_func(s['reference'], s['current'])
        y_pred.append(1 if score > threshold else 0)
    
    # Handle edge cases
    if sum(y_pred) == 0:
        return {'precision': 0, 'recall': 0, 'f1': 0}
    
    precision = precision_score(y_true, y_pred, zero_division=0)
    recall = recall_score(y_true, y_pred, zero_division=0)
    f1 = f1_score(y_true, y_pred, zero_division=0)
    
    return {'precision': precision, 'recall': recall, 'f1': f1}

def find_optimal_threshold(scenarios, metric_func, thresholds, optimize_for='f1'):
    """
    ‡∏´‡∏≤ optimal threshold
    """
    results = []
    
    for t in thresholds:
        metrics = calculate_metrics_for_threshold(scenarios, metric_func, t)
        metrics['threshold'] = t
        results.append(metrics)
    
    results_df = pd.DataFrame(results)
    
    # ‡∏´‡∏≤ optimal
    optimal_idx = results_df[optimize_for].idxmax()
    optimal = results_df.iloc[optimal_idx]
    
    return results_df, optimal

# %%
# ‡∏´‡∏≤ optimal threshold ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö PSI
thresholds = np.linspace(0.01, 0.5, 50)

psi_results, psi_optimal = find_optimal_threshold(
    scenarios, 
    CustomDriftMetrics.psi, 
    thresholds
)

print("=" * 60)
print("üìä PSI Threshold Optimization")
print("=" * 60)
print(f"Optimal threshold: {psi_optimal['threshold']:.3f}")
print(f"Precision: {psi_optimal['precision']:.3f}")
print(f"Recall: {psi_optimal['recall']:.3f}")
print(f"F1 Score: {psi_optimal['f1']:.3f}")

# %%
# Visualize threshold optimization
fig, axes = plt.subplots(1, 3, figsize=(15, 4))

metrics_to_plot = ['precision', 'recall', 'f1']
colors = ['blue', 'green', 'red']

for ax, metric, color in zip(axes, metrics_to_plot, colors):
    ax.plot(psi_results['threshold'], psi_results[metric], f'{color}-', linewidth=2)
    ax.axvline(psi_optimal['threshold'], color='black', linestyle='--', 
               label=f'Optimal ({psi_optimal["threshold"]:.3f})')
    ax.set_xlabel('PSI Threshold')
    ax.set_ylabel(metric.capitalize())
    ax.set_title(f'{metric.capitalize()} vs Threshold')
    ax.legend()
    ax.grid(True, alpha=0.3)

plt.suptitle('PSI Threshold Optimization', fontsize=14, fontweight='bold')
plt.tight_layout()
plt.savefig('threshold_optimization_psi.png', dpi=150, bbox_inches='tight')
plt.show()

# %%
# ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö optimal thresholds ‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞ metric
print("=" * 60)
print("üìä Comparing Optimal Thresholds for Different Metrics")
print("=" * 60)

metrics_funcs = {
    'PSI': CustomDriftMetrics.psi,
    'Normalized Wasserstein': CustomDriftMetrics.normalized_wasserstein,
    'Mean Shift Ratio': CustomDriftMetrics.mean_shift_ratio,
    'Combined Score': CustomDriftMetrics.combined_score
}

threshold_ranges = {
    'PSI': np.linspace(0.01, 0.5, 50),
    'Normalized Wasserstein': np.linspace(0.01, 2.0, 50),
    'Mean Shift Ratio': np.linspace(0.01, 2.0, 50),
    'Combined Score': np.linspace(0.01, 0.5, 50)
}

optimal_thresholds = {}
for name, func in metrics_funcs.items():
    results, optimal = find_optimal_threshold(scenarios, func, threshold_ranges[name])
    optimal_thresholds[name] = optimal
    print(f"\n{name}:")
    print(f"  Optimal threshold: {optimal['threshold']:.3f}")
    print(f"  F1 Score: {optimal['f1']:.3f}")

# %% [markdown]
# ## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 5: Business-driven Threshold Setting
#
# ‡∏õ‡∏£‡∏±‡∏ö threshold ‡∏ï‡∏≤‡∏° business requirements

# %%
class BusinessDriftThreshold:
    """
    Class ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≥‡∏´‡∏ô‡∏î threshold ‡∏ï‡∏≤‡∏° business context
    """
    
    def __init__(self, false_positive_cost=1, false_negative_cost=10):
        """
        Parameters:
        -----------
        false_positive_cost : float - cost ‡∏Ç‡∏≠‡∏á false alarm
        false_negative_cost : float - cost ‡∏Ç‡∏≠‡∏á missing drift
        """
        self.fp_cost = false_positive_cost
        self.fn_cost = false_negative_cost
    
    def calculate_total_cost(self, y_true, y_pred):
        """‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì total cost"""
        tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()
        return fp * self.fp_cost + fn * self.fn_cost
    
    def find_cost_optimal_threshold(self, scenarios, metric_func, thresholds):
        """‡∏´‡∏≤ threshold ‡∏ó‡∏µ‡πà minimize total cost"""
        costs = []
        
        for t in thresholds:
            y_true = [1 if s['has_drift'] else 0 for s in scenarios]
            y_pred = [1 if metric_func(s['reference'], s['current']) > t else 0 for s in scenarios]
            
            cost = self.calculate_total_cost(y_true, y_pred)
            costs.append({'threshold': t, 'cost': cost})
        
        cost_df = pd.DataFrame(costs)
        optimal_idx = cost_df['cost'].idxmin()
        return cost_df, cost_df.iloc[optimal_idx]

# %%
# ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö threshold ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö scenarios ‡∏ï‡πà‡∏≤‡∏á‡πÜ
print("=" * 60)
print("üìä Business-driven Threshold Optimization")
print("=" * 60)

# Scenario 1: High cost of missing drift (e.g., fraud detection)
print("\nüî¥ Scenario 1: High cost of missing drift (FN cost = 10x)")
high_fn_cost = BusinessDriftThreshold(false_positive_cost=1, false_negative_cost=10)
cost_df_1, optimal_1 = high_fn_cost.find_cost_optimal_threshold(
    scenarios, CustomDriftMetrics.psi, thresholds
)
print(f"   Optimal threshold: {optimal_1['threshold']:.3f}")
print(f"   Total cost: {optimal_1['cost']:.0f}")

# Scenario 2: High cost of false alarms (e.g., model retraining is expensive)
print("\nüü° Scenario 2: High cost of false alarms (FP cost = 10x)")
high_fp_cost = BusinessDriftThreshold(false_positive_cost=10, false_negative_cost=1)
cost_df_2, optimal_2 = high_fp_cost.find_cost_optimal_threshold(
    scenarios, CustomDriftMetrics.psi, thresholds
)
print(f"   Optimal threshold: {optimal_2['threshold']:.3f}")
print(f"   Total cost: {optimal_2['cost']:.0f}")

# Scenario 3: Balanced costs
print("\nüü¢ Scenario 3: Balanced costs (FP = FN)")
balanced_cost = BusinessDriftThreshold(false_positive_cost=1, false_negative_cost=1)
cost_df_3, optimal_3 = balanced_cost.find_cost_optimal_threshold(
    scenarios, CustomDriftMetrics.psi, thresholds
)
print(f"   Optimal threshold: {optimal_3['threshold']:.3f}")
print(f"   Total cost: {optimal_3['cost']:.0f}")

# %%
# Visualize cost-based optimization
fig, axes = plt.subplots(1, 3, figsize=(15, 4))

scenarios_data = [
    (cost_df_1, optimal_1, 'High FN Cost (10x)', 'red'),
    (cost_df_2, optimal_2, 'High FP Cost (10x)', 'orange'),
    (cost_df_3, optimal_3, 'Balanced Cost', 'green')
]

for ax, (cost_df, optimal, title, color) in zip(axes, scenarios_data):
    ax.plot(cost_df['threshold'], cost_df['cost'], f'{color[0]}-', linewidth=2)
    ax.axvline(optimal['threshold'], color='black', linestyle='--',
               label=f'Optimal ({optimal["threshold"]:.3f})')
    ax.scatter([optimal['threshold']], [optimal['cost']], color='black', s=100, zorder=5)
    ax.set_xlabel('PSI Threshold')
    ax.set_ylabel('Total Cost')
    ax.set_title(title)
    ax.legend()
    ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('cost_based_optimization.png', dpi=150, bbox_inches='tight')
plt.show()

print("\nüí° Insight:")
print("   - High FN cost ‚Üí Lower threshold (detect more, accept false alarms)")
print("   - High FP cost ‚Üí Higher threshold (be more conservative)")
print("   - Balanced ‚Üí Somewhere in between")

# %% [markdown]
# ## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 6: Handling False Positives/Negatives

# %%
class RobustDriftDetector:
    """
    Drift detector ‡∏ó‡∏µ‡πà‡∏°‡∏µ mechanisms ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö handle FP/FN
    """
    
    def __init__(self, primary_metric='psi', primary_threshold=0.1,
                 confirmation_window=3, use_ensemble=True):
        """
        Parameters:
        -----------
        primary_metric : str - metric ‡∏´‡∏•‡∏±‡∏Å
        primary_threshold : float - threshold ‡∏´‡∏•‡∏±‡∏Å
        confirmation_window : int - ‡∏ï‡πâ‡∏≠‡∏á detect ‡∏Å‡∏µ‡πà‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ï‡∏¥‡∏î‡∏Å‡∏±‡∏ô
        use_ensemble : bool - ‡πÉ‡∏ä‡πâ‡∏´‡∏•‡∏≤‡∏¢ metrics ‡∏£‡πà‡∏ß‡∏°‡∏Å‡∏±‡∏ô
        """
        self.primary_metric = primary_metric
        self.primary_threshold = primary_threshold
        self.confirmation_window = confirmation_window
        self.use_ensemble = use_ensemble
        
        self.metrics = CustomDriftMetrics()
        
        # Thresholds for ensemble
        self.thresholds = {
            'psi': 0.1,
            'wasserstein': 0.5,
            'mean_shift': 0.5,
            'percentile': 0.3
        }
        
        self.consecutive_count = 0
        self.history = []
    
    def _calculate_all_metrics(self, reference, current):
        """‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏ó‡∏∏‡∏Å metrics"""
        return {
            'psi': self.metrics.psi(reference, current),
            'wasserstein': self.metrics.normalized_wasserstein(reference, current),
            'mean_shift': self.metrics.mean_shift_ratio(reference, current),
            'percentile': self.metrics.percentile_shift(reference, current)
        }
    
    def detect(self, reference, current):
        """
        Detect drift with robustness mechanisms
        """
        all_metrics = self._calculate_all_metrics(reference, current)
        
        # Single metric detection
        primary_value = all_metrics[self.primary_metric]
        primary_drift = primary_value > self.primary_threshold
        
        if self.use_ensemble:
            # Ensemble: majority voting
            drift_votes = sum(
                1 for m, v in all_metrics.items() 
                if v > self.thresholds.get(m, 0.5)
            )
            ensemble_drift = drift_votes >= 3  # ‡∏ï‡πâ‡∏≠‡∏á 3/4 metrics agree
        else:
            ensemble_drift = primary_drift
        
        # Confirmation mechanism
        if ensemble_drift:
            self.consecutive_count += 1
        else:
            self.consecutive_count = 0
        
        confirmed_drift = self.consecutive_count >= self.confirmation_window
        
        result = {
            'metrics': all_metrics,
            'primary_drift': primary_drift,
            'ensemble_drift': ensemble_drift,
            'consecutive_count': self.consecutive_count,
            'confirmed_drift': confirmed_drift
        }
        
        self.history.append(result)
        
        return result
    
    def evaluate(self, scenarios):
        """
        Evaluate detector performance on labeled scenarios
        """
        y_true = []
        y_pred_primary = []
        y_pred_ensemble = []
        y_pred_confirmed = []
        
        for s in scenarios:
            y_true.append(1 if s['has_drift'] else 0)
            
            result = self.detect(s['reference'], s['current'])
            y_pred_primary.append(1 if result['primary_drift'] else 0)
            y_pred_ensemble.append(1 if result['ensemble_drift'] else 0)
            y_pred_confirmed.append(1 if result['confirmed_drift'] else 0)
            
            # Reset for next scenario
            self.consecutive_count = 0
        
        evaluations = {}
        for name, y_pred in [
            ('primary', y_pred_primary),
            ('ensemble', y_pred_ensemble),
            ('confirmed', y_pred_confirmed)
        ]:
            evaluations[name] = {
                'precision': precision_score(y_true, y_pred, zero_division=0),
                'recall': recall_score(y_true, y_pred, zero_division=0),
                'f1': f1_score(y_true, y_pred, zero_division=0)
            }
        
        return evaluations

# %%
# ‡∏ó‡∏î‡∏™‡∏≠‡∏ö RobustDriftDetector
print("=" * 60)
print("üìä Evaluating Robust Drift Detector")
print("=" * 60)

detector = RobustDriftDetector(
    primary_metric='psi',
    primary_threshold=0.1,
    confirmation_window=1,  # ‡∏•‡∏î‡πÄ‡∏´‡∏•‡∏∑‡∏≠ 1 ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö single scenario testing
    use_ensemble=True
)

evaluations = detector.evaluate(scenarios)

for method, metrics in evaluations.items():
    print(f"\n{method.upper()} Method:")
    print(f"  Precision: {metrics['precision']:.3f}")
    print(f"  Recall: {metrics['recall']:.3f}")
    print(f"  F1 Score: {metrics['f1']:.3f}")

# %%
# Visualize comparison
fig, ax = plt.subplots(figsize=(10, 6))

methods = list(evaluations.keys())
x = np.arange(len(methods))
width = 0.25

metrics_names = ['precision', 'recall', 'f1']
colors = ['blue', 'green', 'red']

for i, (metric, color) in enumerate(zip(metrics_names, colors)):
    values = [evaluations[m][metric] for m in methods]
    ax.bar(x + i * width, values, width, label=metric.capitalize(), color=color, alpha=0.7)

ax.set_ylabel('Score')
ax.set_xlabel('Detection Method')
ax.set_title('Comparison of Detection Methods')
ax.set_xticks(x + width)
ax.set_xticklabels([m.capitalize() for m in methods])
ax.legend()
ax.set_ylim(0, 1)

for i, (metric, color) in enumerate(zip(metrics_names, colors)):
    values = [evaluations[m][metric] for m in methods]
    for j, v in enumerate(values):
        ax.text(x[j] + i * width, v + 0.02, f'{v:.2f}', ha='center', fontsize=9)

plt.tight_layout()
plt.savefig('detection_methods_comparison.png', dpi=150, bbox_inches='tight')
plt.show()

# %% [markdown]
# ## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 7: ‡∏™‡∏£‡∏∏‡∏õ‡πÅ‡∏•‡∏∞ Best Practices

# %%
print("=" * 70)
print("üìã THRESHOLD SETTING BEST PRACTICES")
print("=" * 70)

best_practices = """
1Ô∏è‚É£ DOMAIN-SPECIFIC THRESHOLDS
   - ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ default thresholds ‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà validate
   - ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏±‡∏ö labeled data ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ
   - ‡∏õ‡∏£‡∏∂‡∏Å‡∏©‡∏≤‡∏Å‡∏±‡∏ö domain experts

2Ô∏è‚É£ COST-BASED OPTIMIZATION
   - ‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤ cost ‡∏Ç‡∏≠‡∏á FP vs FN
   - FN ‡πÅ‡∏û‡∏á ‚Üí Lower threshold (more sensitive)
   - FP ‡πÅ‡∏û‡∏á ‚Üí Higher threshold (more conservative)

3Ô∏è‚É£ ENSEMBLE APPROACH
   - ‡πÉ‡∏ä‡πâ‡∏´‡∏•‡∏≤‡∏¢ metrics ‡∏£‡πà‡∏ß‡∏°‡∏Å‡∏±‡∏ô
   - Voting mechanism ‡∏•‡∏î false positives
   - ‡∏ñ‡πâ‡∏≤ metrics ‡πÑ‡∏°‡πà agree ‚Üí investigate further

4Ô∏è‚É£ CONFIRMATION MECHANISM
   - Require consecutive detections
   - ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô temporary spikes
   - Trade-off: delay detection

5Ô∏è‚É£ PERIODIC REVIEW
   - Review thresholds ‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏∞‡∏¢‡∏∞
   - Data patterns ‡∏≠‡∏≤‡∏à‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô
   - Business requirements ‡∏≠‡∏≤‡∏à‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô

6Ô∏è‚É£ MONITORING & ALERTING
   - Different severity levels
   - Different thresholds for warning vs critical
   - Escalation procedures
"""

print(best_practices)

# %% [markdown]
# ## ‡∏™‡∏£‡∏∏‡∏õ LAB 5
#
# ### ‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ:
# 1. **Custom Metrics**: PSI, Wasserstein, Mean Shift, Combined Score
# 2. **Threshold Optimization**: ‡πÉ‡∏ä‡πâ labeled data ‡∏´‡∏≤ optimal threshold
# 3. **Cost-based Approach**: ‡∏õ‡∏£‡∏±‡∏ö threshold ‡∏ï‡∏≤‡∏° FP/FN costs
# 4. **Robust Detection**: Ensemble + Confirmation mechanisms

# %%
print("=" * 60)
print("‚úÖ LAB 5 COMPLETED!")
print("=" * 60)
print("""
üìö Key Takeaways:
1. Default thresholds rarely optimal for your use case
2. Use labeled data to optimize thresholds
3. Consider business costs of FP vs FN
4. Ensemble methods reduce false alarms
5. Confirmation window adds robustness

üîú Next: LAB 6 - End-to-End Monitoring Pipeline
""")
```

---

## LAB 6: End-to-End Monitoring Pipeline

```python
# %% [markdown]
# # LAB 6: End-to-End Monitoring Pipeline
# ## ‡∏™‡∏£‡πâ‡∏≤‡∏á Pipeline ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Drift Monitoring ‡πÅ‡∏ö‡∏ö‡∏Ñ‡∏£‡∏ö‡∏ß‡∏á‡∏à‡∏£
#
# ### ‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ:
# 1. ‡∏£‡∏ß‡∏°‡∏ó‡∏∏‡∏Å components ‡πÄ‡∏Ç‡πâ‡∏≤‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏±‡∏ô
# 2. ‡∏™‡∏£‡πâ‡∏≤‡∏á automated monitoring workflow
# 3. Integrate ‡∏Å‡∏±‡∏ö MLflow ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö experiment tracking
#
# ### ‡∏ó‡∏§‡∏©‡∏é‡∏µ:
# Production ML monitoring ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£:
# - Automated data ingestion
# - Real-time drift detection
# - Alerting mechanisms
# - Experiment tracking
# - Dashboard ‡πÅ‡∏•‡∏∞ reporting

# %% [markdown]
# ## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 1: ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° Environment

# %%
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from datetime import datetime, timedelta
from scipy import stats
import json
import os
import logging
from collections import deque
import warnings
warnings.filterwarnings('ignore')

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

np.random.seed(42)
plt.rcParams['figure.figsize'] = (14, 6)

print("‚úÖ Libraries imported successfully!")

# ‡∏™‡∏£‡πâ‡∏≤‡∏á directory ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö output
os.makedirs('monitoring_output', exist_ok=True)
os.makedirs('monitoring_output/reports', exist_ok=True)
os.makedirs('monitoring_output/alerts', exist_ok=True)

# %% [markdown]
# ## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 2: ‡∏™‡∏£‡πâ‡∏≤‡∏á Data Classes ‡πÅ‡∏•‡∏∞ Utilities

# %%
from dataclasses import dataclass, field
from typing import List, Dict, Optional, Callable
from enum import Enum

class AlertSeverity(Enum):
    INFO = "info"
    WARNING = "warning"
    CRITICAL = "critical"

class DriftType(Enum):
    NONE = "none"
    MILD = "mild"
    MODERATE = "moderate"
    SEVERE = "severe"

@dataclass
class DriftResult:
    """‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö drift"""
    timestamp: datetime
    feature: str
    drift_detected: bool
    drift_type: DriftType
    psi: float
    ks_statistic: float
    ks_pvalue: float
    reference_mean: float
    current_mean: float
    reference_std: float
    current_std: float
    
    def to_dict(self):
        return {
            'timestamp': self.timestamp.isoformat(),
            'feature': self.feature,
            'drift_detected': self.drift_detected,
            'drift_type': self.drift_type.value,
            'psi': self.psi,
            'ks_statistic': self.ks_statistic,
            'ks_pvalue': self.ks_pvalue,
            'reference_mean': self.reference_mean,
            'current_mean': self.current_mean,
            'reference_std': self.reference_std,
            'current_std': self.current_std
        }

@dataclass
class Alert:
    """Alert object"""
    timestamp: datetime
    severity: AlertSeverity
    message: str
    details: Dict
    acknowledged: bool = False
    
    def to_dict(self):
        return {
            'timestamp': self.timestamp.isoformat(),
            'severity': self.severity.value,
            'message': self.message,
            'details': self.details,
            'acknowledged': self.acknowledged
        }

@dataclass
class MonitoringConfig:
    """Configuration ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö monitoring"""
    reference_window_size: int = 1000
    current_window_size: int = 200
    psi_mild_threshold: float = 0.1
    psi_moderate_threshold: float = 0.2
    psi_severe_threshold: float = 0.25
    ks_significance: float = 0.05
    check_interval_seconds: int = 60
    alert_cooldown_minutes: int = 30
    features_to_monitor: List[str] = field(default_factory=list)

# %% [markdown]
# ## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 3: ‡∏™‡∏£‡πâ‡∏≤‡∏á Core Monitoring Components

# %%
class DriftCalculator:
    """
    Component ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì drift metrics
    """
    
    @staticmethod
    def calculate_psi(reference: np.ndarray, current: np.ndarray, bins: int = 10) -> float:
        """Calculate Population Stability Index"""
        breakpoints = np.percentile(reference, np.linspace(0, 100, bins + 1))
        breakpoints = np.unique(breakpoints)
        
        if len(breakpoints) < 2:
            return 0.0
        
        ref_counts, _ = np.histogram(reference, bins=breakpoints)
        cur_counts, _ = np.histogram(current, bins=breakpoints)
        
        eps = 1e-6
        ref_props = ref_counts / len(reference) + eps
        cur_props = cur_counts / len(current) + eps
        
        psi = np.sum((cur_props - ref_props) * np.log(cur_props / ref_props))
        return float(psi)
    
    @staticmethod
    def calculate_ks_test(reference: np.ndarray, current: np.ndarray) -> tuple:
        """Calculate Kolmogorov-Smirnov test"""
        statistic, pvalue = stats.ks_2samp(reference, current)
        return float(statistic), float(pvalue)
    
    @staticmethod
    def determine_drift_type(psi: float, config: MonitoringConfig) -> DriftType:
        """Determine drift severity based on PSI"""
        if psi >= config.psi_severe_threshold:
            return DriftType.SEVERE
        elif psi >= config.psi_moderate_threshold:
            return DriftType.MODERATE
        elif psi >= config.psi_mild_threshold:
            return DriftType.MILD
        return DriftType.NONE

# %%
class DataBuffer:
    """
    Buffer ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏Å‡πá‡∏ö reference ‡πÅ‡∏•‡∏∞ current data
    """
    
    def __init__(self, config: MonitoringConfig):
        self.config = config
        self.reference_data: Dict[str, deque] = {}
        self.current_data: Dict[str, deque] = {}
        self.is_initialized = False
        
    def initialize(self, reference_df: pd.DataFrame):
        """Initialize with reference data"""
        for feature in self.config.features_to_monitor:
            if feature in reference_df.columns:
                self.reference_data[feature] = deque(
                    reference_df[feature].values[-self.config.reference_window_size:],
                    maxlen=self.config.reference_window_size
                )
                self.current_data[feature] = deque(
                    maxlen=self.config.current_window_size
                )
        self.is_initialized = True
        logger.info(f"DataBuffer initialized with {len(self.reference_data)} features")
    
    def add_data(self, data: Dict[str, float]):
        """Add new data point"""
        for feature, value in data.items():
            if feature in self.current_data:
                self.current_data[feature].append(value)
    
    def get_reference(self, feature: str) -> Optional[np.ndarray]:
        """Get reference data for a feature"""
        if feature in self.reference_data:
            return np.array(self.reference_data[feature])
        return None
    
    def get_current(self, feature: str) -> Optional[np.ndarray]:
        """Get current data for a feature"""
        if feature in self.current_data:
            return np.array(self.current_data[feature])
        return None
    
    def is_current_ready(self) -> bool:
        """Check if current buffer has enough data"""
        for feature in self.config.features_to_monitor:
            if feature in self.current_data:
                if len(self.current_data[feature]) < self.config.current_window_size:
                    return False
        return True
    
    def update_reference(self):
        """Update reference with current data"""
        for feature in self.config.features_to_monitor:
            if feature in self.current_data and len(self.current_data[feature]) > 0:
                # Add current data to reference
                for val in self.current_data[feature]:
                    self.reference_data[feature].append(val)
                self.current_data[feature].clear()
        logger.info("Reference data updated with current window")

# %%
class AlertManager:
    """
    Component ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ alerts
    """
    
    def __init__(self, config: MonitoringConfig):
        self.config = config
        self.alerts: List[Alert] = []
        self.last_alert_time: Dict[str, datetime] = {}
    
    def should_alert(self, feature: str) -> bool:
        """Check if we should send alert (cooldown)"""
        if feature not in self.last_alert_time:
            return True
        
        elapsed = datetime.now() - self.last_alert_time[feature]
        return elapsed > timedelta(minutes=self.config.alert_cooldown_minutes)
    
    def create_alert(self, drift_result: DriftResult) -> Optional[Alert]:
        """Create alert based on drift result"""
        if not drift_result.drift_detected:
            return None
        
        if not self.should_alert(drift_result.feature):
            return None
        
        # Determine severity
        if drift_result.drift_type == DriftType.SEVERE:
            severity = AlertSeverity.CRITICAL
        elif drift_result.drift_type == DriftType.MODERATE:
            severity = AlertSeverity.WARNING
        else:
            severity = AlertSeverity.INFO
        
        message = (
            f"Drift detected in feature '{drift_result.feature}': "
            f"PSI={drift_result.psi:.4f}, Type={drift_result.drift_type.value}"
        )
        
        alert = Alert(
            timestamp=drift_result.timestamp,
            severity=severity,
            message=message,
            details=drift_result.to_dict()
        )
        
        self.alerts.append(alert)
        self.last_alert_time[drift_result.feature] = datetime.now()
        
        # Log based on severity
        if severity == AlertSeverity.CRITICAL:
            logger.critical(message)
        elif severity == AlertSeverity.WARNING:
            logger.warning(message)
        else:
            logger.info(message)
        
        return alert
    
    def get_active_alerts(self) -> List[Alert]:
        """Get all unacknowledged alerts"""
        return [a for a in self.alerts if not a.acknowledged]
    
    def acknowledge_alert(self, index: int):
        """Acknowledge an alert"""
        if 0 <= index < len(self.alerts):
            self.alerts[index].acknowledged = True
    
    def save_alerts(self, filepath: str):
        """Save alerts to file"""
        alerts_data = [a.to_dict() for a in self.alerts]
        with open(filepath, 'w') as f:
            json.dump(alerts_data, f, indent=2)

# %% [markdown]
# ## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 4: ‡∏™‡∏£‡πâ‡∏≤‡∏á Main Monitoring Pipeline

# %%
class DriftMonitoringPipeline:
    """
    Main pipeline ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö drift monitoring
    """
    
    def __init__(self, config: MonitoringConfig):
        self.config = config
        self.data_buffer = DataBuffer(config)
        self.alert_manager = AlertManager(config)
        self.drift_calculator = DriftCalculator()
        self.results_history: List[DriftResult] = []
        self.is_running = False
        
    def initialize(self, reference_data: pd.DataFrame):
        """Initialize pipeline with reference data"""
        self.data_buffer.initialize(reference_data)
        logger.info("DriftMonitoringPipeline initialized")
    
    def process_batch(self, batch_data: pd.DataFrame) -> List[DriftResult]:
        """Process a batch of new data"""
        results = []
        
        # Add data to buffer
        for idx, row in batch_data.iterrows():
            data_point = {f: row[f] for f in self.config.features_to_monitor if f in row}
            self.data_buffer.add_data(data_point)
        
        # Check if ready for drift detection
        if not self.data_buffer.is_current_ready():
            logger.debug("Current buffer not ready yet")
            return results
        
        # Perform drift detection for each feature
        for feature in self.config.features_to_monitor:
            result = self._detect_drift_for_feature(feature)
            if result:
                results.append(result)
                self.results_history.append(result)
                
                # Create alert if needed
                if result.drift_detected:
                    self.alert_manager.create_alert(result)
        
        return results
    
    def _detect_drift_for_feature(self, feature: str) -> Optional[DriftResult]:
        """Detect drift for a single feature"""
        reference = self.data_buffer.get_reference(feature)
        current = self.data_buffer.get_current(feature)
        
        if reference is None or current is None:
            return None
        
        if len(current) < 10:  # Need minimum samples
            return None
        
        # Calculate metrics
        psi = self.drift_calculator.calculate_psi(reference, current)
        ks_stat, ks_pval = self.drift_calculator.calculate_ks_test(reference, current)
        
        # Determine drift type
        drift_type = self.drift_calculator.determine_drift_type(psi, self.config)
        drift_detected = drift_type != DriftType.NONE or ks_pval < self.config.ks_significance
        
        return DriftResult(
            timestamp=datetime.now(),
            feature=feature,
            drift_detected=drift_detected,
            drift_type=drift_type,
            psi=psi,
            ks_statistic=ks_stat,
            ks_pvalue=ks_pval,
            reference_mean=float(np.mean(reference)),
            current_mean=float(np.mean(current)),
            reference_std=float(np.std(reference)),
            current_std=float(np.std(current))
        )
    
    def get_summary_report(self) -> Dict:
        """Generate summary report"""
        if not self.results_history:
            return {'status': 'no_data'}
        
        # Group by feature
        feature_summary = {}
        for feature in self.config.features_to_monitor:
            feature_results = [r for r in self.results_history if r.feature == feature]
            if feature_results:
                latest = feature_results[-1]
                drift_count = sum(1 for r in feature_results if r.drift_detected)
                feature_summary[feature] = {
                    'latest_psi': latest.psi,
                    'latest_drift_type': latest.drift_type.value,
                    'drift_count': drift_count,
                    'total_checks': len(feature_results),
                    'drift_rate': drift_count / len(feature_results)
                }
        
        return {
            'timestamp': datetime.now().isoformat(),
            'total_checks': len(self.results_history),
            'total_drifts_detected': sum(1 for r in self.results_history if r.drift_detected),
            'active_alerts': len(self.alert_manager.get_active_alerts()),
            'feature_summary': feature_summary
        }
    
    def save_results(self, output_dir: str = 'monitoring_output'):
        """Save all results to files"""
        # Save drift results
        results_data = [r.to_dict() for r in self.results_history]
        with open(f'{output_dir}/drift_results.json', 'w') as f:
            json.dump(results_data, f, indent=2)
        
        # Save alerts
        self.alert_manager.save_alerts(f'{output_dir}/alerts/alerts.json')
        
        # Save summary
        summary = self.get_summary_report()
        with open(f'{output_dir}/reports/summary.json', 'w') as f:
            json.dump(summary, f, indent=2)
        
        logger.info(f"Results saved to {output_dir}")

# %% [markdown]
# ## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 5: ‡∏™‡∏£‡πâ‡∏≤‡∏á Report Generator

# %%
class ReportGenerator:
    """
    Component ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡∏£‡πâ‡∏≤‡∏á reports ‡πÅ‡∏•‡∏∞ visualizations
    """
    
    def __init__(self, pipeline: DriftMonitoringPipeline):
        self.pipeline = pipeline
    
    def generate_dashboard_data(self) -> Dict:
        """Generate data for dashboard"""
        summary = self.pipeline.get_summary_report()
        
        # Time series data for each feature
        time_series = {}
        for feature in self.pipeline.config.features_to_monitor:
            feature_results = [
                r for r in self.pipeline.results_history 
                if r.feature == feature
            ]
            time_series[feature] = {
                'timestamps': [r.timestamp.isoformat() for r in feature_results],
                'psi_values': [r.psi for r in feature_results],
                'drift_types': [r.drift_type.value for r in feature_results]
            }
        
        return {
            'summary': summary,
            'time_series': time_series,
            'alerts': [a.to_dict() for a in self.pipeline.alert_manager.get_active_alerts()]
        }
    
    def plot_drift_trends(self, save_path: str = None):
        """Plot drift trends for all features"""
        n_features = len(self.pipeline.config.features_to_monitor)
        
        if n_features == 0:
            logger.warning("No features to plot")
            return
        
        fig, axes = plt.subplots(n_features, 1, figsize=(14, 4*n_features), sharex=True)
        
        if n_features == 1:
            axes = [axes]
        
        colors = {'none': 'green', 'mild': 'yellow', 'moderate': 'orange', 'severe': 'red'}
        
        for ax, feature in zip(axes, self.pipeline.config.features_to_monitor):
            feature_results = [
                r for r in self.pipeline.results_history 
                if r.feature == feature
            ]
            
            if not feature_results:
                continue
            
            timestamps = [r.timestamp for r in feature_results]
            psi_values = [r.psi for r in feature_results]
            drift_types = [r.drift_type.value for r in feature_results]
            
            # Plot PSI
            scatter_colors = [colors.get(dt, 'gray') for dt in drift_types]
            ax.scatter(timestamps, psi_values, c=scatter_colors, s=30, alpha=0.7)
            ax.plot(timestamps, psi_values, 'b-', alpha=0.3)
            
            # Add thresholds
            ax.axhline(self.pipeline.config.psi_mild_threshold, 
                      color='yellow', linestyle='--', alpha=0.7, label='Mild')
            ax.axhline(self.pipeline.config.psi_moderate_threshold, 
                      color='orange', linestyle='--', alpha=0.7, label='Moderate')
            ax.axhline(self.pipeline.config.psi_severe_threshold, 
                      color='red', linestyle='--', alpha=0.7, label='Severe')
            
            ax.set_ylabel(f'{feature}\nPSI')
            ax.legend(loc='upper right')
            ax.grid(True, alpha=0.3)
        
        axes[-1].set_xlabel('Time')
        plt.suptitle('Drift Monitoring Dashboard', fontsize=14, fontweight='bold')
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=150, bbox_inches='tight')
            logger.info(f"Plot saved to {save_path}")
        
        plt.show()
    
    def generate_html_report(self, output_path: str):
        """Generate HTML report"""
        summary = self.pipeline.get_summary_report()
        
        html_content = f"""
        <!DOCTYPE html>
        <html>
        <head>
            <title>Drift Monitoring Report</title>
            <style>
                body {{ font-family: Arial, sans-serif; margin: 20px; }}
                .header {{ background-color: #2196F3; color: white; padding: 20px; }}
                .summary {{ background-color: #f0f0f0; padding: 15px; margin: 10px 0; }}
                .alert-critical {{ background-color: #ffebee; border-left: 4px solid #f44336; padding: 10px; margin: 5px 0; }}
                .alert-warning {{ background-color: #fff3e0; border-left: 4px solid #ff9800; padding: 10px; margin: 5px 0; }}
                .alert-info {{ background-color: #e3f2fd; border-left: 4px solid #2196F3; padding: 10px; margin: 5px 0; }}
                table {{ border-collapse: collapse; width: 100%; }}
                th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}
                th {{ background-color: #2196F3; color: white; }}
            </style>
        </head>
        <body>
            <div class="header">
                <h1>üîç Drift Monitoring Report</h1>
                <p>Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
            </div>
            
            <div class="summary">
                <h2>üìä Summary</h2>
                <p><strong>Total Checks:</strong> {summary.get('total_checks', 0)}</p>
                <p><strong>Total Drifts Detected:</strong> {summary.get('total_drifts_detected', 0)}</p>
                <p><strong>Active Alerts:</strong> {summary.get('active_alerts', 0)}</p>
            </div>
            
            <h2>üìà Feature Summary</h2>
            <table>
                <tr>
                    <th>Feature</th>
                    <th>Latest PSI</th>
                    <th>Drift Type</th>
                    <th>Drift Count</th>
                    <th>Drift Rate</th>
                </tr>
        """
        
        for feature, data in summary.get('feature_summary', {}).items():
            html_content += f"""
                <tr>
                    <td>{feature}</td>
                    <td>{data['latest_psi']:.4f}</td>
                    <td>{data['latest_drift_type']}</td>
                    <td>{data['drift_count']}</td>
                    <td>{data['drift_rate']:.1%}</td>
                </tr>
            """
        
        html_content += """
            </table>
            
            <h2>‚ö†Ô∏è Active Alerts</h2>
        """
        
        for alert in self.pipeline.alert_manager.get_active_alerts():
            alert_class = f"alert-{alert.severity.value}"
            html_content += f"""
            <div class="{alert_class}">
                <strong>{alert.severity.value.upper()}</strong>: {alert.message}
                <br><small>{alert.timestamp.strftime('%Y-%m-%d %H:%M:%S')}</small>
            </div>
            """
        
        html_content += """
        </body>
        </html>
        """
        
        with open(output_path, 'w') as f:
            f.write(html_content)
        
        logger.info(f"HTML report saved to {output_path}")

# %% [markdown]
# ## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 6: ‡∏ó‡∏î‡∏™‡∏≠‡∏ö Full Pipeline

# %%
# ‡∏™‡∏£‡πâ‡∏≤‡∏á test data
def generate_test_data(n_samples=5000):
    """‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡∏°‡∏µ drift"""
    np.random.seed(42)
    
    data = []
    base_means = {'feature_a': 50, 'feature_b': 100, 'feature_c': 75}
    
    for i in range(n_samples):
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á drift ‡∏ó‡∏µ‡πà feature_a ‡∏´‡∏•‡∏±‡∏á sample 3000
        if i < 3000:
            feature_a_mean = base_means['feature_a']
        else:
            # Gradual drift
            progress = (i - 3000) / 2000
            feature_a_mean = base_means['feature_a'] + progress * 20
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á sudden drift ‡∏ó‡∏µ‡πà feature_b ‡∏ó‡∏µ‡πà sample 2000
        if i < 2000:
            feature_b_mean = base_means['feature_b']
        else:
            feature_b_mean = base_means['feature_b'] + 30
        
        # feature_c ‡πÑ‡∏°‡πà‡∏°‡∏µ drift
        feature_c_mean = base_means['feature_c']
        
        data.append({
            'timestamp': datetime(2024, 1, 1) + timedelta(hours=i),
            'feature_a': np.random.normal(feature_a_mean, 10),
            'feature_b': np.random.normal(feature_b_mean, 15),
            'feature_c': np.random.normal(feature_c_mean, 12)
        })
    
    return pd.DataFrame(data)

# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏î‡∏™‡∏≠‡∏ö
test_data = generate_test_data(5000)
print(f"üìä Test data shape: {test_data.shape}")
print(test_data.head())

# %%
# Visualize test data
fig, axes = plt.subplots(3, 1, figsize=(14, 8), sharex=True)

for ax, feature in zip(axes, ['feature_a', 'feature_b', 'feature_c']):
    ax.plot(test_data['timestamp'], test_data[feature], alpha=0.3)
    rolling_mean = test_data[feature].rolling(100).mean()
    ax.plot(test_data['timestamp'], rolling_mean, 'r-', linewidth=2, label='Rolling Mean')
    ax.set_ylabel(feature)
    ax.legend()

axes[0].set_title('Test Data with Drift Patterns')
axes[-1].set_xlabel('Time')
plt.tight_layout()
plt.show()

# %%
# Configure ‡πÅ‡∏•‡∏∞ run pipeline
config = MonitoringConfig(
    reference_window_size=1000,
    current_window_size=200,
    psi_mild_threshold=0.1,
    psi_moderate_threshold=0.2,
    psi_severe_threshold=0.25,
    ks_significance=0.05,
    features_to_monitor=['feature_a', 'feature_b', 'feature_c']
)

# ‡∏™‡∏£‡πâ‡∏≤‡∏á pipeline
pipeline = DriftMonitoringPipeline(config)

# ‡πÉ‡∏ä‡πâ 1000 samples ‡πÅ‡∏£‡∏Å‡πÄ‡∏õ‡πá‡∏ô reference
reference_data = test_data.iloc[:1000]
pipeline.initialize(reference_data)

print("=" * 60)
print("üöÄ Starting Drift Monitoring Pipeline")
print("=" * 60)

# Process data ‡πÄ‡∏õ‡πá‡∏ô batches
batch_size = 200
remaining_data = test_data.iloc[1000:]

for i in range(0, len(remaining_data), batch_size):
    batch = remaining_data.iloc[i:i+batch_size]
    results = pipeline.process_batch(batch)
    
    if results and any(r.drift_detected for r in results):
        print(f"\nüìç Batch {i//batch_size + 1} results:")
        for r in results:
            if r.drift_detected:
                print(f"   ‚ö†Ô∏è {r.feature}: PSI={r.psi:.4f}, Type={r.drift_type.value}")

# %%
# Generate summary
print("\n" + "=" * 60)
print("üìä FINAL SUMMARY REPORT")
print("=" * 60)

summary = pipeline.get_summary_report()
print(f"\nTotal Checks: {summary['total_checks']}")
print(f"Total Drifts Detected: {summary['total_drifts_detected']}")
print(f"Active Alerts: {summary['active_alerts']}")

print("\nFeature Summary:")
for feature, data in summary['feature_summary'].items():
    print(f"\n  {feature}:")
    print(f"    Latest PSI: {data['latest_psi']:.4f}")
    print(f"    Drift Type: {data['latest_drift_type']}")
    print(f"    Drift Count: {data['drift_count']}/{data['total_checks']}")
    print(f"    Drift Rate: {data['drift_rate']:.1%}")

# %%
# Generate reports
report_generator = ReportGenerator(pipeline)

# Plot trends
report_generator.plot_drift_trends(save_path='monitoring_output/drift_trends.png')

# Generate HTML report
report_generator.generate_html_report('monitoring_output/reports/drift_report.html')

# Save all results
pipeline.save_results()

print("\n‚úÖ All reports generated and saved to monitoring_output/")

# %% [markdown]
# ## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 7: Integration ‡∏Å‡∏±‡∏ö MLflow (Optional)

# %%
# Note: ‡∏™‡πà‡∏ß‡∏ô‡∏ô‡∏µ‡πâ‡∏ï‡πâ‡∏≠‡∏á install mlflow ‡∏Å‡πà‡∏≠‡∏ô: pip install mlflow

try:
    import mlflow
    MLFLOW_AVAILABLE = True
except ImportError:
    MLFLOW_AVAILABLE = False
    print("‚ö†Ô∏è MLflow not installed. Skipping MLflow integration.")
    print("   Install with: pip install mlflow")

if MLFLOW_AVAILABLE:
    class MLflowDriftTracker:
        """
        Integration ‡∏Å‡∏±‡∏ö MLflow ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö track drift experiments
        """
        
        def __init__(self, experiment_name: str = "drift_monitoring"):
            mlflow.set_experiment(experiment_name)
            self.active_run = None
        
        def start_run(self, run_name: str = None):
            """Start new MLflow run"""
            self.active_run = mlflow.start_run(run_name=run_name)
            return self.active_run
        
        def log_drift_result(self, result: DriftResult):
            """Log drift result to MLflow"""
            if self.active_run is None:
                return
            
            # Log metrics
            mlflow.log_metric(f"{result.feature}_psi", result.psi)
            mlflow.log_metric(f"{result.feature}_ks_stat", result.ks_statistic)
            mlflow.log_metric(f"{result.feature}_drift", 1 if result.drift_detected else 0)
            
            # Log params
            mlflow.log_param(f"{result.feature}_drift_type", result.drift_type.value)
        
        def log_summary(self, summary: Dict):
            """Log summary to MLflow"""
            if self.active_run is None:
                return
            
            mlflow.log_metric("total_drifts", summary.get('total_drifts_detected', 0))
            mlflow.log_metric("total_checks", summary.get('total_checks', 0))
            
            for feature, data in summary.get('feature_summary', {}).items():
                mlflow.log_metric(f"{feature}_drift_rate", data['drift_rate'])
        
        def log_artifact(self, artifact_path: str):
            """Log artifact to MLflow"""
            if self.active_run is None:
                return
            mlflow.log_artifact(artifact_path)
        
        def end_run(self):
            """End MLflow run"""
            if self.active_run:
                mlflow.end_run()
                self.active_run = None
    
    # Example usage
    print("\n" + "=" * 60)
    print("üìä Logging to MLflow")
    print("=" * 60)
    
    tracker = MLflowDriftTracker("drift_monitoring_lab")
    tracker.start_run(run_name="pipeline_run_1")
    
    # Log summary
    tracker.log_summary(summary)
    
    # Log artifacts
    tracker.log_artifact('monitoring_output/drift_trends.png')
    tracker.log_artifact('monitoring_output/reports/drift_report.html')
    
    tracker.end_run()
    print("‚úÖ Results logged to MLflow")

# %% [markdown]
# ## ‡∏™‡∏£‡∏∏‡∏õ LAB 6
#
# ### ‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ:
# 1. **Pipeline Architecture**: ‡πÅ‡∏¢‡∏Å components ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô
# 2. **Data Classes**: ‡πÉ‡∏ä‡πâ dataclasses ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö type safety
# 3. **Alert Management**: ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ alerts ‡∏û‡∏£‡πâ‡∏≠‡∏° cooldown
# 4. **Report Generation**: HTML reports ‡πÅ‡∏•‡∏∞ visualizations
# 5. **MLflow Integration**: Track experiments

# %%
print("=" * 60)
print("‚úÖ LAB 6 COMPLETED!")
print("=" * 60)
print("""
üìö Key Takeaways:
1. Modular architecture ‡∏ó‡∏≥‡πÉ‡∏´‡πâ maintain ‡∏á‡πà‡∏≤‡∏¢
2. Data buffering ‡∏ä‡πà‡∏ß‡∏¢ handle streaming data
3. Alert management ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô alert fatigue
4. Automated reporting saves time
5. MLflow integration enables experiment tracking

üéâ CONGRATULATIONS! You have completed all 6 labs!

üìã What you've learned:
- LAB 1: Data Drift Concepts (Covariate/Concept Shift)
- LAB 2: Feature Drift Detection (per-feature analysis)
- LAB 3: Multivariate Drift Analysis (correlation/PCA)
- LAB 4: Production Simulation (streaming detection)
- LAB 5: Custom Metrics & Thresholds (optimization)
- LAB 6: End-to-End Pipeline (production-ready)

üöÄ Next steps:
- Deploy pipeline to production
- Add more sophisticated alerting (email, Slack)
- Integrate with model retraining triggers
- Add A/B testing capabilities
""")

# %%
# Final cleanup and summary
print("\nüìÅ Output files created:")
for root, dirs, files in os.walk('monitoring_output'):
    for file in files:
        filepath = os.path.join(root, file)
        print(f"   {filepath}")
```

---

## ‡∏™‡∏£‡∏∏‡∏õ‡∏£‡∏ß‡∏°

‡∏ó‡∏±‡πâ‡∏á 6 Labs ‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏î‡∏±‡∏á‡∏ô‡∏µ‡πâ:

| Lab | ‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠ | ‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ |
|-----|--------|----------------|
| 1 | Understanding Data Drift | Covariate/Concept Shift, KS/PSI/Wasserstein |
| 2 | Feature Drift Detection | Per-feature analysis, Numerical vs Categorical |
| 3 | Multivariate Drift | Correlation, PCA, Mahalanobis Distance |
| 4 | Production Simulation | Streaming, Sliding Window, Page-Hinkley |
| 5 | Custom Metrics & Thresholds | Optimization, Cost-based, Ensemble |
| 6 | End-to-End Pipeline | Architecture, Alerting, Reporting, MLflow |
